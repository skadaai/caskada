

================================================
File: docs/getting_started.md
================================================
# Getting Started with BrainyFlow

Welcome to BrainyFlow! This framework helps you build powerful, modular AI applications using a simple yet expressive abstraction based on nested directed graphs.

## 1. Installation

First, ensure you have BrainyFlow installed:

```bash
# For Python
pip install brainyflow

# For TypeScript
npm install brainyflow
```

For detailed installation options, see the [Installation Guide](./installation.md).

## 2. Core Concepts

BrainyFlow is built around a minimalist yet powerful abstraction that separates data flow from computation:

- **[Node](./core_abstraction/node.md)**: The fundamental building block that performs a single task with a clear lifecycle (`prep → exec → post`).
- **[Flow](./core_abstraction/flow.md)**: Orchestrates nodes in a directed graph, supporting branching, looping, and nesting.
- **[Shared Store](./core_abstraction/communication.md)**: A global data structure that enables communication between nodes.
- **[Batch](./core_abstraction/batch.md)**: Processes multiple items either sequentially or in parallel.

## 3. Your First Flow

Let's build a simple Question-Answering flow to demonstrate BrainyFlow's core concepts:

### Step 1: Design Your Flow

Our flow will have two nodes:

1. `GetQuestionNode`: Captures the user's question
2. `AnswerNode`: Generates an answer using an LLM

```mermaid
graph LR
    A[GetQuestionNode] --> B[AnswerNode]
```

### Step 2: Implement the Nodes

{% tabs %}
{% tab title="Python" %}

```python
from brainyflow import Node
from utils import call_llm  # Your LLM implementation

class GetQuestionNode(Node):
    async def prep(self, shared):
        """Get text input from user."""
        shared["question"] = input("Enter your question: ")

class AnswerNode(Node):
    async def prep(self, shared):
        """Extract the question from shared store."""
        return shared["question"]

    async def exec(self, question):
        """Call LLM to generate an answer."""
        return await call_llm(question)

    async def post(self, shared, prep_res, exec_res):
        """Store the answer in shared store."""
        shared["answer"] = exec_res
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
import { Node } from 'brainyflow'
import { input } from '@inquirer/prompts'
import { callLLM } from './utils/callLLM' // Your LLM implementation

class GetQuestionNode extends Node {
  async prep(shared: Record): Promise {
    shared.question = await input({ message: 'Enter your question: ' })
  }
}

class AnswerNode extends Node {
  async prep(shared: Record): Promise {
    return shared.question
  }

  async exec(question: string): Promise {
    return await callLLM(question)
  }

  async post(shared: Record, prepRes: string, execRes: string): Promise {
    shared.answer = execRes
  }
}
```

{% endtab %}
{% endtabs %}

{% hint style="info" %}

**Review:** What was achieved here?

- `GetQuestionNode` writes the user's question to the `shared` store.
- `AnswerNode` reads the question from the `shared` store, calling an LLM utility, and writing the answer back to the `shared` store.

{% endhint %}

### Step 3: Connect the Nodes into a Flow

{% tabs %}
{% tab title="Python" %}

```python
from brainyflow import Flow

def create_qa_flow():
    get_question_node = GetQuestionNode()
    answer_node = AnswerNode()

    # Connect nodes: get_question_node → answer_node
    get_question_node >> answer_node

    return Flow(start=get_question_node)
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
import { Flow } from 'brainyflow'

function createQaFlow(): Flow {
  const getQuestionNode = new GetQuestionNode()
  const answerNode = new AnswerNode()

  // Connect nodes: getQuestionNode → answerNode
  getQuestionNode.next(answerNode)

  return new Flow(getQuestionNode)
}
```

{% endtab %}
{% endtabs %}

### Step 4: Run the Flow

{% tabs %}
{% tab title="Python" %}

```python
import asyncio

async def main():
    shared = {}  # Initialize empty shared store
    qa_flow = create_qa_flow()
    await qa_flow.run(shared)

    print(f"Question: {shared['question']}")
    print(f"Answer: {shared['answer']}")

if __name__ == "__main__":
    asyncio.run(main())
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
async function main() {
  const shared: Record = {} // Initialize empty shared store
  const qaFlow = createQaFlow()
  await qaFlow.run(shared)

  console.log(`Question: ${shared.question}`)
  console.log(`Answer: ${shared.answer}`)
}

main().catch(console.error)
```

{% hint style="info" %}

**Review:** What was achieved here?

- `qaFlow` has connected the nodes, letting the user's question flow from `GetQuestionNode` to `AnswerNode` to generate an answer.

{% endhint %}

{% endtab %}
{% endtabs %}

## 4. Key Design Principles

BrainyFlow follows these core design principles:

1. **Separation of Concerns**: Data storage (shared store) is separate from computation logic (nodes)
2. **Explicit Data Flow**: Data dependencies between steps are clear and traceable
3. **Composability**: Complex systems are built from simple, reusable components
4. **Minimalism**: The framework provides only essential abstractions, avoiding vendor-specific implementations

## 5. Next Steps

Now that you understand the basics, explore these resources to build sophisticated applications:

- [Core Abstractions](./core_abstraction/index.md): Dive deeper into nodes, flows, and communication
- [Design Patterns](./design_pattern/index.md): Learn more complex patterns like Agents, RAG, and MapReduce
- [Agentic Coding Guide](./guides/agentic_coding.md): Best practices for human-AI collaborative development
- [Best Practices](./guides/best_practices.md): Tips for building robust, maintainable applications




================================================
File: docs/core_abstraction/index.md
================================================
# Understanding BrainyFlow's Core Abstractions

BrainyFlow is built around a simple yet powerful abstraction: the **nested directed graph with shared store**. This mental model separates _data flow_ from _computation_, making complex LLM applications more maintainable and easier to reason about.

## Core Philosophy

BrainyFlow follows these fundamental principles:

1. **Modularity & Composability**: Build complex systems from simple, reusable components that are easy to build, test, and maintain
2. **Explicitness**: Make data dependencies between steps clear and traceable
3. **Separation of Concerns**: Data storage (shared store) remains separate from computation logic (nodes)
4. **Minimalism**: The framework provides only essential abstractions, avoiding vendor-specific implementations while supporting various high-level AI design paradigms (agents, workflows, map-reduce, etc.)
5. **Resilience**: Handle failures gracefully with retries and fallbacks

## The Graph + Shared Store Pattern

The fundamental pattern in BrainyFlow combines two key elements:

- **Computation Graph**: A directed graph where nodes represent discrete units of work and edges represent the flow of control
- **Shared Store**: A global data structure that enables communication between nodes

This pattern offers several advantages:

- **Clear visualization** of application logic
- **Easy identification** of bottlenecks
- **Simple debugging** of individual components
- **Natural parallelization** opportunities

## Key Components

BrainyFlow's architecture is based on these fundamental building blocks:

| Component                           | Description             | Key Features                                                                |
| ----------------------------------- | ----------------------- | --------------------------------------------------------------------------- |
| [Node](./node.md)                   | The basic unit of work  | Clear lifecycle (`prep → exec → post`), fault tolerance, graceful fallbacks |
| [Flow](./flow.md)                   | Connects nodes together | Action-based transitions, branching, looping, nesting                       |
| [Communication](./communication.md) | Enables data sharing    | Shared Store (global), Params (node-specific)                               |
| [Batch](./batch.md)                 | Handles multiple items  | Sequential or parallel processing, nested batching                          |
| [Throttling](./throttling.md)       | Manages concurrency     | Rate limiting, concurrency control                                          |

## How They Work Together

1. **Nodes** perform individual tasks with a clear lifecycle:

   - `prep`: Read from shared store and prepare data
   - `exec`: Execute computation (often LLM calls)
   - `post`: Process results and write to shared store

2. **Flows** orchestrate nodes by:

   - Starting with a designated node
   - Following action-based transitions between nodes
   - Supporting branching, looping, and nested flows

3. **Communication** happens through:

   - **Shared Store**: A global dictionary accessible to all nodes
   - **Params**: Node-specific configuration passed down from parent flows

4. **Batch Processing** enables:
   - Processing multiple items sequentially or in parallel
   - Handling large datasets efficiently
   - Supporting nested batch operations

## Getting Started

If you're new to BrainyFlow, we recommend exploring these core abstractions in the following order:

1. [Node](./node.md) - Understand the basic building block
2. [Flow](./flow.md) - Learn how to connect nodes together
3. [Communication](./communication.md) - See how nodes share data
4. [Batch](./batch.md) - Explore handling multiple items
5. [Throttling](./throttling.md) - Learn about managing concurrency

Once you understand these core abstractions, you'll be ready to implement various [Design Patterns](../design_pattern/index.md) to solve real-world problems.




================================================
File: docs/core_abstraction/node.md
================================================
# Node: The Fundamental Building Block

A **Node** is the smallest reusable unit in BrainyFlow. Each Node follows a 3-step lifecycle that enforces the principle of separation of concerns.

## Node Lifecycle

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/node.png?raw=true" width="400"/>
</div>

### 1. `async prep(shared)`

**Purpose**: Read and preprocess data from the shared store.

- Extracts necessary data from the `shared` store
- Performs any required preprocessing or validation
- Returns `prep_res`, which becomes input for `exec()` and `post()`

### 2. `async exec(prep_res)`

**Purpose**: Execute the core computation logic.

- Performs the main computation (often an LLM call or API request)
- ⚠️ Must **NOT** access the `shared` store directly
- ⚠️ Should be designed for idempotence when retries are enabled
- Returns `exec_res`, which is passed to `post()`

### 3. `async post(shared, prep_res, exec_res)`

**Purpose**: Process results and update the shared store.

- Writes computation results back to the `shared` store
- Has access to both the original input (`prep_res`) and result (`exec_res`)
- Returns an action string that determines the next node in the flow
  - If no value is returned, defaults to `"default"`

{% hint style="info" %}
**Why 3 steps?** This design enforces separation of concerns:

- `prep`: Data access and preparation
- `exec`: Pure computation (no side effects)
- `post`: Result processing and state updates

All steps are **optional**. For example, you can implement only `prep` and `post` if you just need to process data without external computation.
{% endhint %}

```mermaid
sequenceDiagram
    participant S as Shared Store
    participant N as Node

    N->>S: 1. prep(): Read from shared store
    Note right of N: Return prep_res

    N->>N: 2. exec(prep_res): Compute result
    Note right of N: Return exec_res

    N->>S: 3. post(shared, prep_res, exec_res): Write to shared store
    Note right of N: Return action string
```

## Fault Tolerance & Retries

Nodes support automatic retries for handling transient failures in `exec()` calls:

```python
# Python
my_node = MyNode(max_retries=3, wait=10)  # Retry up to 3 times with 10s delay

# TypeScript
const myNode = new MyNode({ maxRetries: 3, wait: 10 }) // Retry up to 3 times with 10s delay
```

Key retry parameters:

- `max_retries` (int): Maximum number of execution attempts (default: 1, meaning no retry)
- `wait` (int): Seconds to wait between retries (default: 0)

`wait` is specially helpful when you encounter rate-limits or quota errors from your LLM provider and need to back off.
During retries, you can access the current retry count (0-based) via `self.cur_retry` (Python) or `this.curRetry` (TypeScript).

{% tabs %}
{% tab title="Python" %}

```python
class RetryNode(Node):
    async def exec(self, prep_res):
        print(f"Retried {self.cur_retry} times")
        raise Exception("Failed")
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class RetryNode extends Node {
  async exec(prepRes: any): Promise {
    console.log(`Retried ${this.curRetry} times`)
    throw new Error('Failed')
  }
}
```

## Graceful Fallbacks

To handle failures gracefully after all retries are exhausted, override the `exec_fallback` method:

{% tabs %}
{% tab title="Python" %}

```python
async def exec_fallback(self, prep_res, exc):
    raise exc  # Default behavior is to re-raise
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
async execFallback(prepRes: any, exc: Error): Promise {
  throw exc;  // Default behavior is to re-raise
}
```

{% endtab %}
{% endtabs %}

By default, this method just re-raises the exception. You can override it to return a fallback result instead, which becomes the `exec_res` passed to `post()`.

## Example: Summarize File

{% tabs %}
{% tab title="Python" %}

```python
class SummarizeFile(Node):
    async def prep(self, shared):
        return shared["data"]

    async def exec(self, prep_res):
        if not prep_res:
            return "Empty file content"
        prompt = f"Summarize this text in 10 words: {prep_res}"
        summary = call_llm(prompt)  # might fail
        return summary

    async def exec_fallback(self, prep_res, exc):
        # Provide a simple fallback instead of crashing
        return "There was an error processing your request."

    async def post(self, shared, prep_res, exec_res):
        shared["summary"] = exec_res
        # Return "default" by not returning

summarize_node = SummarizeFile(max_retries=3)

async def main():
    # node.run() calls prep->exec->post
    # If exec() fails, it retries up to 3 times before calling exec_fallback()
    action_result = await summarize_node.run(shared)
    print("Action returned:", action_result)  # "default"
    print("Summary stored:", shared["summary"])

asyncio.run(main())
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class SummarizeFile extends Node {
  async prep(shared: any): Promise<any> {
    return shared['data']
  }

  async exec(prepRes: any): Promise<string> {
    if (!prepRes) {
      return 'Empty file content'
    }
    const prompt = `Summarize this text in 10 words: ${prepRes}`
    const summary = await callLLM(prompt) // might fail
    return summary
  }

  async execFallback(prepRes: any, exc: Error): Promise<string> {
    // Provide a simple fallback instead of crashing
    return 'There was an error processing your request.'
  }

  async post(shared: any, prepRes: any, execRes: any): Promise<string> {
    shared['summary'] = execRes
    // Return "default" by not returning
  }
}

const summarizeNode = new SummarizeFile({ maxRetries: 3 })

// node.run() calls prep->exec->post
// If exec() fails, it retries up to 3 times before calling execFallback()
const actionResult = await summarizeNode.run(shared)

console.log('Action returned:', actionResult) // "default"
console.log('Summary stored:', shared['summary'])
```

{% endtab %}
{% endtabs %}

## Running Individual Nodes

Nodes have an extra method `run(shared)`, which calls `prep->exec->post` and returns the action.

{% hint style="warning" %}
`Node.run` **does not** proceed to a successor!

This method is useful for debugging or testing a single node, but not for running a [flow](./flow.md)!

Always use `Flow.run` instead to ensure the full pipeline runs correctly.
{% endhint %}

Compare it with `Flow.run`:

- `node.run(shared)`: Just runs that node alone (calls `prep->exec->post()`), returns an Action.
- `flow.run(shared)`: Executes from the start node, follows Actions to the next node, and so on until the flow can't continue.




================================================
File: docs/core_abstraction/flow.md
================================================
# Flow: Orchestrating Nodes in a Directed Graph

A **Flow** orchestrates a graph of Nodes, connecting them through action-based transitions. Flows enable you to create complex application logic including sequences, branches, loops, and nested workflows.

## Action-based Transitions

Each Node's `post()` method returns an **Action** string that determines which node to execute next. If `post()` doesn't return anything, the default action `"default"` is used.

### Defining Transitions

{% tabs %}
{% tab title="Python" %}

You can define transitions primarily with syntax sugar:

1. **Basic default transition**: `node_a >> node_b`
   This means if `node_a.post()` returns `"default"`, go to `node_b`.

2. **Named action transition**: `node_a - "action_name" >> node_b`
   This means if `node_a.post()` returns `"action_name"`, go to `node_b`.

Note that `node_a >> node_b` is equivalent to `node_a - "default" >> node_b`

```python
# Basic default transition
node_a >> node_b  # If node_a returns "default", go to node_b

# Named action transition
node_a - "success" >> node_b  # If node_a returns "success", go to node_b
node_a - "error" >> node_c    # If node_a returns "error", go to node_c
```

{% endtab %}

{% tab title="TypeScript" %}

1. **Basic default transition**: `node_a.next(node_b)`
   This means if `node_a.post()` returns `"default"`, go to `node_b`.

2. **Named action transition**: `node_a.on('action_name', node_b)` or `node_a.next(node_b, 'action_name')`
   This means if `node_a.post()` returns `"action_name"`, go to `node_b`.

Note that `node_a.next(node_b)` is equivalent to both `node_a.next(node_b, 'default')` and `node_a.on('default', node_b)`

```typescript
// Basic default transition
node_a.next(node_b) // If node_a returns "default", go to node_b

// Named action transition
node_a.on('success', node_b) // If node_a returns "success", go to node_b
node_a.on('error', node_c) // If node_a returns "error", go to node_c

// Alternative syntax
node_a.next(node_b, 'success') // Same as node_a.on('success', node_b)
```

{% endtab %}
{% endtabs %}

## Creating a Flow

A Flow begins with a **start node** and follows the action-based transitions until it reaches a node with no matching transition for its returned action.

{% tabs %}
{% tab title="Python" %}

```python
from brainyflow import Flow

# Define nodes and transitions
node_a >> node_b
node_b - "success" >> node_c
node_b - "error" >> node_d

# Create flow starting with node_a
flow = Flow(start=node_a)

# Run the flow with a shared store
await flow.run(shared)
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
import { Flow } from 'brainyflow'

// Define nodes and transitions
node_a.next(node_b)
node_b.on('success', node_c)
node_b.on('error', node_d)

// Create flow starting with node_a
const flow = new Flow(node_a)

// Run the flow with a shared store
await flow.run(shared)
```

{% endtab %}
{% endtabs %}

## Flow Execution Process

When you call `flow.run(shared)`:

1. The flow executes the start node
2. It examines the action returned by the node's `post()` method
3. It follows the corresponding transition to the next node
4. This process repeats until it reaches a node with no matching transition for its action

```mermaid
sequenceDiagram
    participant S as Shared Store
    participant F as Flow
    participant N1 as Node A
    participant N2 as Node B

    F->>N1: Execute Node A
    N1->>S: Read from shared store
    N1->>N1: Perform computation
    N1->>S: Write to shared store
    N1-->>F: Return action "default"

    F->>F: Find next node for action "default"
    F->>N2: Execute Node B
    N2->>S: Read from shared store
    N2->>N2: Perform computation
    N2->>S: Write to shared store
    N2-->>F: Return action "success"

    F->>F: No transition defined for "success"
    F-->>F: Flow execution complete
```

## Branching and Looping

Flows support complex patterns like branching (conditionally following different paths) and looping (returning to previous nodes).

### Example: Expense Approval Flow

Here's a simple expense approval flow that demonstrates branching and looping:

{% tabs %}
{% tab title="Python" %}

```python
# Define the nodes first
# review = ReviewExpenseNode()
# revise = ReviseReportNode()
# payment = ProcessPaymentNode()
# finish = FinishProcessNode()
# ...

# Define the flow connections
review - "approved" >> payment        # If approved, process payment
review - "needs_revision" >> revise   # If needs changes, go to revision
review - "rejected" >> finish         # If rejected, finish the process

revise >> review   # After revision, go back for another review
payment >> finish  # After payment, finish the process

# Create the flow
expense_flow = Flow(start=review)
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
// Define the nodes first
// const review = new ReviewExpenseNode()
// const revise = new ReviseReportNode()
// const payment = new ProcessPaymentNode()
// const finish = new FinishProcessNode()
// ..

// Define the flow connections
review.on('approved', payment) // If approved, process payment
review.on('needs_revision', revise) // If needs changes, go to revision
review.on('rejected', finish) // If rejected, finish the process

revise.next(review) // After revision, go back for another review
payment.next(finish) // After payment, finish the process

// Create the flow
const expenseFlow = new Flow(review)
```

{% endtab %}
{% endtabs %}

This flow creates the following execution paths:

1. If `review.post()` returns `"approved"`, the expense moves to the `payment` node
2. If `review.post()` returns `"needs_revision"`, it goes to the `revise` node, which then loops back to `review`
3. If `review.post()` returns `"rejected"`, it moves to the `finish` node and stops

```mermaid
flowchart TD
    review[Review Expense] -->|approved| payment[Process Payment]
    review -->|needs_revision| revise[Revise Report]
    review -->|rejected| finish[Finish Process]

    revise --> review
    payment --> finish
```

## Nested Flows

A Flow can be used as a Node within another Flow, enabling powerful composition patterns. This allows you to:

1. Break down complex applications into manageable sub-flows
2. Reuse flows across different applications
3. Create hierarchical workflows with clear separation of concerns

### Flow as a Node

When a Flow is used as a Node:

- It inherits the Node lifecycle (`prep → exec → post`)
- Its `prep()` and `post()` methods can be overridden
- `post()` always receives `None` for `exec_res` and should instead get the flow execution results from the shared store.
- It won't allow for a custom `exec()` method since its main logic is to orchestrate its internal nodes
- When run, it executes its internal nodes according to their transitions

### Example: Order Processing Pipeline

Here's a practical example that breaks down order processing into nested flows:

{% tabs %}
{% tab title="Python" %}

```python
# Payment processing sub-flow
validate_payment >> process_payment >> payment_confirmation
payment_flow = Flow(start=validate_payment)

# Inventory sub-flow
check_stock >> reserve_items >> update_inventory
inventory_flow = Flow(start=check_stock)

# Shipping sub-flow
create_label >> assign_carrier >> schedule_pickup
shipping_flow = Flow(start=create_label)

# Connect the flows into a main order pipeline
payment_flow >> inventory_flow >> shipping_flow

# Create the master flow
order_pipeline = Flow(start=payment_flow)

# Run the entire pipeline
await order_pipeline.run(shared_data)
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
// Payment processing sub-flow
validatePayment.next(processPayment).next(paymentConfirmation)
const paymentFlow = new Flow(validatePayment)

// Inventory sub-flow
checkStock.next(reserveItems).next(updateInventory)
const inventoryFlow = new Flow(checkStock)

// Shipping sub-flow
createLabel.next(assignCarrier).next(schedulePickup)
const shippingFlow = new Flow(createLabel)

// Connect the flows into a main order pipeline
paymentFlow.next(inventoryFlow).next(shippingFlow)

// Create the master flow
const orderPipeline = new Flow(paymentFlow)

// Run the entire pipeline
await orderPipeline.run(sharedData)
```

{% endtab %}
{% endtabs %}

This creates a clean separation of concerns while maintaining a clear execution path:

```mermaid
flowchart LR
    subgraph order_pipeline[Order Pipeline]
        subgraph paymentFlow["Payment Flow"]
            A[Validate Payment] --> B[Process Payment] --> C[Payment Confirmation]
        end

        subgraph inventoryFlow["Inventory Flow"]
            D[Check Stock] --> E[Reserve Items] --> F[Update Inventory]
        end

        subgraph shippingFlow["Shipping Flow"]
            G[Create Label] --> H[Assign Carrier] --> I[Schedule Pickup]
        end

        paymentFlow --> inventoryFlow
        inventoryFlow --> shippingFlow
    end
```

## Flow Parameters

When a Flow is used as a Node, you can pass parameters to it using `set_params()`. These parameters are then accessible within the Flow's nodes:

{% tabs %}
{% tab title="Python" %}

```python
# Create a flow
process_flow = Flow(start=some_node)

# Set parameters
process_flow.set_params({"mode": "fast", "max_items": 10})

# Use the flow in another flow
main_flow = Flow(start=process_flow)

# Run the main flow
await main_flow.run(shared)
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
// Create a flow
const processFlow = new Flow(someNode)

// Set parameters
processFlow.setParams({ mode: 'fast', maxItems: 10 })

// Use the flow in another flow
const mainFlow = new Flow(processFlow)

// Run the main flow
await mainFlow.run(shared)
```

{% endtab %}
{% endtabs %}

## Best Practices

1. **Start Simple**: Begin with a linear flow and add branching/looping as needed
2. **Visualize First**: Sketch your flow diagram before coding
3. **Name Actions Clearly**: Use descriptive action names that indicate the decision made
4. **Test Incrementally**: Build and test one section of your flow at a time
5. **Document Transitions**: Add comments explaining the conditions for each transition
6. **Error Handling**: Always include paths for handling errors
7. **Avoid Deep Nesting**: Keep nesting to 2-3 levels for maintainability

By following these principles, you can create complex, maintainable AI applications that are easy to reason about and extend.




================================================
File: docs/core_abstraction/communication.md
================================================
# Communication

Nodes and Flows in BrainyFlow communicate through two primary mechanisms:

1. **Shared Store (recommended for most cases)**

   - A global data structure (typically an in-memory dictionary) that all nodes can read from (`prep()`) and write to (`post()`)
   - Ideal for sharing data results, large content, or information needed by multiple nodes
   - Follows the principle of separation of concerns by keeping data separate from computation logic

2. **Params (primarily for [Batch](./batch.md) operations)**
   - Node-specific configuration passed down from parent flows
   - Best for identifiers like filenames or IDs, especially in Batch processing
   - Parameter keys and values should be **immutable** during execution

If you know memory management, think of the **Shared Store** like a **heap** (shared by all function calls), and **Params** like a **stack** (assigned by the caller).

## Shared Store

The Shared Store is the primary communication mechanism in BrainyFlow, embodying the principle of separation between data storage and computation logic.

### Overview

A shared store is typically an in-memory dictionary that serves as a global data repository:

{% tabs %}
{% tab title="Python" %}

```python
shared = {
    "data": {...},
    "results": {...},
    "config": {...},
    # Any other data you need to share
}
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
const shared = {
  data: {...},
  results: {...},
  config: {...},
  // Any other data you need to share
}
```

{% endtab %}
{% endtabs %}

The shared store can also contain file handlers, database connections, or other resources that need to be accessible across nodes.

### Best Practices

1. **Define a Clear Schema**: Plan your shared store structure before implementation
2. **Use Namespaces**: Group related data under descriptive keys
3. **Document Structure**: Comment on expected data types and formats
4. **Avoid Deep Nesting**: Keep the structure reasonably flat for readability

### Example Usage

{% tabs %}
{% tab title="Python" %}

```python
class LoadData(Node):
    async def post(self, shared, prep_res, exec_res):
        # Write data to shared store
        shared["data"] = "Some text content"

class Summarize(Node):
    async def prep(self, shared):
        # Read data from shared store
        return shared["data"]

    async def exec(self, prep_res):
        # Call LLM to summarize
        prompt = f"Summarize: {prep_res}"
        summary = call_llm(prompt)
        return summary

    async def post(self, shared, prep_res, exec_res):
        # Write summary to shared store
        shared["summary"] = exec_res

load_data = LoadData()
summarize = Summarize()
load_data >> summarize
flow = Flow(start=load_data)

shared = {}
await flow.run(shared)
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class LoadData extends Node {
  async post(shared: Record): Promise {
    // Write data to shared store
    shared.data = 'Some text content'
  }
}

class Summarize extends Node {
  async prep(shared: Record): Promise {
    // Read data from shared store
    return shared.data
  }

  async exec(prepRes: string): Promise {
    // Call LLM to summarize
    const prompt = `Summarize: ${prepRes}`
    const summary = await callLLM(prompt)
    return summary
  }

  async post(shared: Record, prepRes: string, execRes: string): Promise {
    // Write summary to shared store
    shared.summary = execRes
  }
}

const loadData = new LoadData()
const summarize = new Summarize()
loadData.next(summarize)
const flow = new Flow(loadData)

const shared = {}
await flow.run(shared)
```

{% endtab %}
{% endtabs %}

## Params

While the Shared Store is the primary communication mechanism, Params provide a way to configure individual nodes with specific settings.

### Key Characteristics

- **Immutable**: Params don't change during a node's execution cycle
- **Hierarchical**: Params are passed down from parent flows to child nodes
- **Local**: Each node or flow has its own params that don't affect other nodes

### When to Use Params

- **Batch Processing**: To identify which item is being processed
- **Configuration**: For node-specific settings that don't need to be shared
- **Identification**: For tracking the source or purpose of a computation

### Example Usage

{% tabs %}
{% tab title="Python" %}

```python
# 1) Create a Node that uses params
class SummarizeFile(Node):
    async def prep(self, shared):
        # Access the node's param
        filename = self.params["filename"]
        return shared["data"].get(filename, "")

    async def exec(self, prep_res):
        prompt = f"Summarize: {prep_res}"
        return call_llm(prompt)

    async def post(self, shared, prep_res, exec_res):
        filename = self.params["filename"]
        shared["summary"][filename] = exec_res

# 2) Set params directly on a node (for testing)
node = SummarizeFile()
node.set_params({"filename": "doc1.txt"})
await node.run(shared)

# 3) Set params on a flow (overrides node params)
flow = Flow(start=node)
flow.set_params({"filename": "doc2.txt"})
await flow.run(shared)  # The node summarizes doc2.txt, not doc1.txt
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
// 1) Create a Node that uses params
class SummarizeFile extends Node {
  async prep(shared: Record): Promise {
    // Access the node's param
    const filename = this.params.filename
    return shared.data[filename] || ''
  }

  async exec(prepRes: string): Promise {
    const prompt = `Summarize: ${prepRes}`
    return await callLLM(prompt)
  }

  async post(shared: Record, prepRes: string, execRes: string): Promise {
    const filename = this.params.filename
    shared.summary[filename] = execRes
  }
}

// 2) Set params directly on a node (for testing)
const node = new SummarizeFile()
node.setParams({ filename: 'doc1.txt' })
await node.run(shared)

// 3) Set params on a flow (overrides node params)
const flow = new Flow(node)
flow.setParams({ filename: 'doc2.txt' })
await flow.run(shared) // The node summarizes doc2.txt, not doc1.txt
```

{% endtab %}
{% endtabs %}

## Choosing Between Shared Store and Params

| Use Shared Store when...                    | Use Params when...                           |
| ------------------------------------------- | -------------------------------------------- |
| Data needs to be accessed by multiple nodes | Configuration is specific to a single node   |
| Information persists across the entire flow | Working with Batch processing                |
| Storing large amounts of data               | Passing identifiers or simple values         |
| Maintaining state throughout execution      | Configuring behavior without affecting state |

{% hint style="success" %}
**Best Practice**: Use Shared Store for almost all cases to maintain separation of concerns. Params are primarily useful for Batch processing and node-specific configuration.
{% endhint %}




================================================
File: docs/core_abstraction/batch.md
================================================
# Batch Processing

Batch processing in BrainyFlow enables efficient handling of multiple items, whether sequentially or in parallel. This is particularly useful for:

- Processing large datasets or lists (e.g., multiple files, database records)
- Applying the same operation to multiple inputs
- Dividing large tasks into manageable chunks

## Node-Level Batch Processing

BrainyFlow provides two specialized node types for batch processing:

### SequentialBatchNode

A `SequentialBatchNode` processes items one after another, which is useful when:

- Order of processing matters
- Operations have dependencies between items
- You need to conserve resources or manage rate limits

It extends `Node`, with changes to:

- **`async prep(shared)`**: returns an **iterable** (e.g., list, generator).
- **`async exec(item)`**: called **once** per item in that iterable.
- **`async post(shared, prep_res, exec_res_list)`**: after all items are processed, receives a **list** of results (`exec_res_list`) and returns an **Action**.

#### Example: Sequential Summarize File

{% tabs %}
{% tab title="Python" %}
{% hint style="info" %}
**Python GIL Note**: Due to Python's GIL, parallel nodes can't truly parallelize CPU-bound tasks but excel at I/O-bound work like API calls.
{% endhint %}

```python
class SequentialSummaries(SequentialBatchNode):
    async def prep(self, shared):
        """Return an iterable of items to process."""
        content = shared["data"]
        chunk_size = 10000
        # Suppose we have a big file; chunk it!
        return [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]

    async def exec(self, chunk):
        """Process a single chunk. Called once per item."""
        prompt = f"Summarize this chunk in 10 words: {chunk}"
        return call_llm(prompt)

    async def post(self, shared, prep_res, exec_res_list):
        """Process all results after all items are processed."""
        shared["summary"] = "\n".join(exec_res_list)
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class SequentialSummaries extends SequentialBatchNode {
  async prep(shared: Record): Promise {
    const content = shared.data
    const chunkSize = 10000
    const chunks: string[] = []

    // Suppose we have a big file; chunk it!
    for (let i = 0; i < content.length; i += chunkSize) {
      chunks.push(content.slice(i, i + chunkSize))
    }
    return chunks
  }

  async exec(chunk: string): Promise<string> {
    const prompt = `Summarize this chunk in 10 words: ${chunk}`
    return await callLLM(prompt)
  }

  async post(shared: Record, prepRes: string[], execResList: string[]): Promise {
    shared.summary = execResList.join('\n')
  }
}
```

{% endtab %}
{% endtabs %}

### ParallelBatchNode

A `ParallelBatchNode` processes items concurrently, which is useful when:

- Operations are independent of each other
- You want to maximize throughput
- Tasks are primarily I/O-bound (like API calls)

{% hint style="warning" %}
**Concurrency Considerations**:

- Ensure operations are truly independent before using parallel processing
- Be mindful of rate limits when making API calls
- Consider using [Throttling](./throttling.md) to control concurrency

{% endhint %}

It extends `Node`, with changes to:

- **`async prep(shared)`**: returns an **iterable** (e.g., list, generator).
- **`async exec(item)`**: called **concurrently** for each item.
- **`async post(shared, prep_res, exec_res_list)`**: after all items are processed, receives a **list** of results (`exec_res_list`) and returns an **Action**.

#### Example: Parallel Summarize of a Large File

{% tabs %}
{% tab title="Python" %}

```python
class ParallelSummaries(ParallelBatchNode):
    async def prep(self, shared):
        """Return an iterable of items to process in parallel."""
        content = shared["data"]
        chunk_size = 10000
        # Suppose we have a big file; chunk it!
        return [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]

    async def exec(self, chunk):
        """Process a single chunk. Called concurrently for all items."""
        prompt = f"Summarize this chunk in 10 words: {chunk}"
        return call_llm(prompt)

    async def post(self, shared, prep_res, exec_res_list):
        """Process all results after all items are processed."""
        shared["summary"] = "\n".join(exec_res_list)
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class ParallelSummaries extends ParallelBatchNode {
  async prep(shared: Record): Promise {
    const content = shared.data
    const chunkSize = 10000
    const chunks: string[] = []

    // Suppose we have a big file; chunk it!
    for (let i = 0; i < content.length; i += chunkSize) {
      chunks.push(content.slice(i, i + chunkSize))
    }
    return chunks
  }

  async exec(chunk: string): Promise<string> {
    const prompt = `Summarize this chunk in 10 words: ${chunk}`
    return await callLLM(prompt)
  }

  async post(shared: Record, prepRes: string[], execResList: string[]): Promise {
    shared.summary = execResList.join('\n')
  }
}
```

{% endtab %}
{% endtabs %}

## Flow-Level Batch Processing

BrainyFlow also supports batch processing at the flow level, allowing you to run an entire flow multiple times with different parameters:

### SequentialBatchFlow

A `SequentialBatchFlow` runs a flow multiple times in sequence, with different `params` each time. Think of it as a loop that replays the Flow for each parameter set.


{% hint style="info" %}
**When to use**: Choose sequential processing when order matters or when working with APIs that have strict rate limits. See [Throttling](./throttling.md) for managing rate limits.
{% endhint %}

#### Example: Summarize Many Files

{% tabs %}
{% tab title="Python" %}

```python
class SummarizeAllFiles(SequentialBatchFlow):
    async def prep(self, shared):
        """Return a list of parameter dictionaries, one per file."""
        filenames = list(shared["data"].keys()) # e.g., ["file1.txt", "file2.txt", ...]
        return [{"filename": fn} for fn in filenames]

summarize_file = Flow(start=load_file)

# Create a batch flow that processes all files sequentially
summarize_all_files = SummarizeAllFiles(start=summarize_file)
await summarize_all_files.run(shared)
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class SummarizeAllFiles extends SequentialBatchFlow {
  async prep(shared: Record): Promise<Array<Record<string, string>>> {
    const filenames = Object.keys(shared.data) // e.g., ["file1.txt", "file2.txt", ...]
    return filenames.map(fn => ({ filename: fn }))
  }
}

const summarizeFile = new Flow(loadFile)

// Create a batch flow that processes all files sequentially
const summarizeAllFiles = new SummarizeAllFiles(summarizeFile)
await summarizeAllFiles.run(shared)
```

{% endtab %}
{% endtabs %}

#### Under the Hood

1. `prep(shared)` returns a list of param dicts—e.g., `[{filename: "file1.txt"}, {filename: "file2.txt"}, ...]`.
2. The **BatchFlow** loops through each dict. For each one:
   - It merges the dict with the BatchFlow’s own `params`.
   - It calls `flow.run(shared)` using the merged result.
3. This means the sub-Flow is run **repeatedly**, once for every param dict.


### ParallelBatchFlow

A `ParallelBatchFlow` runs a flow multiple times concurrently, with different `params` each time:

{% tabs %}
{% tab title="Python" %}

```python
class SummarizeAllFiles(ParallelBatchFlow):
    async def prep(self, shared):
        """Return a list of parameter dictionaries, one per file."""
        filenames = list(shared["data"].keys())
        return [{"filename": fn} for fn in filenames]

# Create a flow for processing a single file
summarize_file = Flow(start=load_file)

# Create a batch flow that processes all files in parallel
summarize_all_files = ParallelBatchFlow(start=summarize_file)
await summarize_all_files.run(shared)
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class SummarizeAllFiles extends ParallelBatchFlow {
  async prep(shared: Record): Promise>> {
    const filenames = Object.keys(shared.data)
    return filenames.map(fn => ({ filename: fn }))
  }
}

// Create a flow for processing a single file
const summarizeFile = new Flow(loadFile)

// Create a batch flow that processes all files in parallel
const summarizeAllFiles = new ParallelBatchFlow(summarizeFile)
await summarizeAllFiles.run(shared)
```

{% endtab %}
{% endtabs %}

## Nested Batch Processing

You can nest a **SequentialBatchFlow** or **ParallelBatchFlow** in another batch flow. For instance:

- **Outer** batch: returns a list of diretory param dicts (e.g., `{"directory": "/pathA"}`, `{"directory": "/pathB"}`, ...).
- **Inner** batch: returning a list of per-file param dicts.

At each level, **BatchFlow** merges its own param dict with the parent’s. By the time you reach the **innermost** node, the final `params` is the merged result of **all** parents in the chain. This way, a nested structure can keep track of the entire context (e.g., directory + file name) at once.

{% tabs %}
{% tab title="Python" %}

```python
class FileBatchFlow(SequentialBatchFlow):
    async def prep(self, shared):
        directory = self.params["directory"]
        # e.g., files = ["file1.txt", "file2.txt", ...]
        files = [f for f in os.listdir(directory) if f.endswith(".txt")]
        return [{"filename": f} for f in files]

class DirectoryBatchFlow(SequentialBatchFlow):
    async def prep(self, shared):
        directories = ["/path/to/dirA", "/path/to/dirB"]
        return [{"directory": d} for d in directories]

# MapSummaries have params like {"directory": "/path/to/dirA", "filename": "file1.txt"}
inner_flow = FileBatchFlow(start=MapSummaries())
outer_flow = DirectoryBatchFlow(start=inner_flow)
await outer_flow.run(shared)
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class FileBatchFlow extends SequentialBatchFlow {
  async prep(shared: Record): Promise<Array<Record<string, string>>> {
    const directory = this.params.directory
    // In a real implementation, use fs.readdirSync or similar
    const files = ['file1.txt', 'file2.txt']
    return files.map(f => ({ filename: f }))
  }
}

class DirectoryBatchFlow extends SequentialBatchFlow {
  async prep(shared: Record): Promise<Array<Record<string, string>>> {
    const directories = ['/path/to/dirA', '/path/to/dirB']
    return directories.map(d => ({ directory: d }))
  }
}

// MapSummaries have params like {"directory": "/path/to/dirA", "filename": "file1.txt"}
const innerFlow = new FileBatchFlow(new MapSummaries())
const outerFlow = new DirectoryBatchFlow(innerFlow)
await outerFlow.run(shared)
```

{% endtab %}
{% endtabs %}

In this nested batch example:

1. The outer flow iterates through directories
2. For each directory, the inner flow processes all files
3. Parameters are merged, so the innermost node receives both directory and filename

## Best Practices

1. **Choose the Right Type**: Use sequential processing when order matters or when managing rate limits; use parallel processing for independent operations
2. **Manage Chunk Size**: Balance between too many small chunks (overhead) and too few large chunks (memory issues)
3. **Error Handling**: Implement proper error handling to prevent one failure from stopping the entire batch
4. **Progress Tracking**: Add logging or progress indicators for long-running batch operations
5. **Resource Management**: Be mindful of memory usage, especially with large datasets




================================================
File: docs/core_abstraction/throttling.md
================================================
---
title: '(Advanced) Throttling'
---

# (Advanced) Throttling

**Throttling** helps manage concurrency and avoid rate limits when making API calls. This is particularly important when:

1. Calling external APIs with rate limits
2. Managing expensive operations (like LLM calls)
3. Preventing system overload from too many parallel requests

## Concurrency Control Patterns

### 1. Using Semaphores (Python)

```python
import asyncio

class LimitedParallelNode(Node):
    def __init__(self, concurrency=3):
        self.semaphore = asyncio.Semaphore(concurrency)

    async def exec(self, items):
        async def limited_process(item):
            async with self.semaphore:
                return await self.process_item(item)

        tasks = [limited_process(item) for item in items]
        return await asyncio.gather(*tasks)

    async def process_item(self, item):
        # Process a single item
        pass
```

### 2. Using p-limit (TypeScript)

```typescript
import pLimit from 'p-limit'

class LimitedParallelNode extends Node {
  constructor(private concurrency = 3) {
    super()
  }

  async exec(items) {
    const limit = pLimit(this.concurrency)
    return Promise.all(items.map((item) => limit(() => this.processItem(item))))
  }

  async processItem(item) {
    // Process a single item
  }
}
```

## Rate Limiting with Window Limits

{% tabs %}
{% tab title="Python" %}

```python
from ratelimit import limits, sleep_and_retry

# 30 calls per minute
@sleep_and_retry
@limits(calls=30, period=60)
def call_api():
    # Your API call here
    pass
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
import { RateLimiter } from 'limiter'

// 30 calls per minute
const limiter = new RateLimiter({ tokensPerInterval: 30, interval: 'minute' })

async function callApi() {
  await limiter.removeTokens(1)
  // Your API call here
}
```

{% endtab %}
{% endtabs %}

## Throttler Utility

{% tabs %}
{% tab title="Python" %}

```python
from tenacity import retry, wait_exponential, stop_after_attempt

@retry(
    wait=wait_exponential(multiplier=1, min=4, max=10),
    stop=stop_after_attempt(5)
)
def call_api_with_retry():
    # Your API call here
    pass
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
import pRetry from 'p-retry'

async function callApiWithRetry() {
  return pRetry(
    async () => {
      // Your API call here
    },
    {
      retries: 5,
      minTimeout: 4000,
      maxTimeout: 10000,
    },
  )
}
```

{% endtab %}
{% endtabs %}

## Advanced Throttling Patterns

### 1. Token Bucket Rate Limiter

{% tabs %}
{% tab title="Python" %}

```python
from pyrate_limiter import Duration, Rate, Limiter

# 10 requests per minute
rate = Rate(10, Duration.MINUTE)
limiter = Limiter(rate)

@limiter.ratelimit("api_calls")
async def call_api():
    # Your API call here
    pass
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
import { TokenBucket } from 'limiter'

// 10 requests per minute
const limiter = new TokenBucket({
  bucketSize: 10,
  tokensPerInterval: 10,
  interval: 'minute',
})

async function callApi() {
  await limiter.removeTokens(1)
  // Your API call here
}
```

{% endtab %}
{% endtabs %}

### 2. Sliding Window Rate Limiter

{% tabs %}
{% tab title="Python" %}

```python
from slidingwindow import SlidingWindowRateLimiter

limiter = SlidingWindowRateLimiter(
    max_requests=100,
    window_size=60  # 60 seconds
)

async def call_api():
    if not limiter.allow_request():
        raise RateLimitExceeded()
    # Your API call here
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
import { SlidingWindowRateLimiter } from 'sliding-window-rate-limiter'

const limiter = SlidingWindowRateLimiter.createLimiter({
  interval: 60, // 60 seconds
  maxInInterval: 100,
})

async function callApi() {
  const isAllowed = await limiter.check('key', 1)
  if (!isAllowed) throw new Error('Rate limit exceeded')
  // Your API call here
}
```

{% endtab %}
{% endtabs %}

{% hint style="info" %}
**Related Concepts**: Many throttling patterns are used with [Batch Processing](./batch.md) operations, particularly when dealing with parallel execution of API calls.
{% endhint %}

## Best Practices

1. **Monitor API Responses**: Watch for 429 (Too Many Requests) responses and adjust your rate limiting accordingly
2. **Implement Retry Logic**: When hitting rate limits, implement exponential backoff for retries
3. **Distribute Load**: If possible, spread requests across multiple API keys or endpoints
4. **Cache Responses**: Cache frequent identical requests to reduce API calls
5. **Batch Requests**: Combine multiple requests into single API calls when possible

## Linking to Related Concepts

For batch processing patterns, see [Batch Processing](./batch.md).




================================================
File: docs/design_pattern/index.md
================================================
# Design Patterns

BrainyFlow makes it easy to implement popular design patterns for LLM applications. This section covers the key patterns you can build with the framework.

## Available Design Patterns

### Core Patterns

1. [Agent](./agent.md): Create autonomous agents that can make decisions and take actions based on context.
2. [Workflow](./workflow.md): Chain multiple tasks into sequential pipelines for complex operations.
3. [RAG](./rag.md): Integrate data retrieval with generation for knowledge-augmented responses.
4. [Map Reduce](./mapreduce.md): Process large datasets by splitting work into parallel tasks and combining results.
5. [Structured Output](./structure.md): Format LLM outputs consistently using structured formats like YAML or JSON.

### Advanced Patterns

6. [Multi-Agents](./multi_agent.md): Coordinate multiple agents working together on complex tasks.

## Choosing the Right Pattern

When building your LLM application, consider these factors when selecting a design pattern:

| Pattern           | Best For                  | When To Use                                             |
| ----------------- | ------------------------- | ------------------------------------------------------- |
| Agent             | Dynamic problem-solving   | When tasks require reasoning and decision-making        |
| Workflow          | Sequential processing     | When steps are well-defined and follow a clear order    |
| RAG               | Knowledge-intensive tasks | When external information is needed for responses       |
| Map Reduce        | Large data processing     | When handling datasets too large for a single operation |
| Structured Output | Consistent formatting     | When outputs need to follow specific schemas            |
| Multi-Agents      | Complex collaboration     | When tasks benefit from specialized agent roles         |

## Decision Tree

Use this decision tree to help determine which pattern best fits your use case:

```mermaid
flowchart TD
    A[Start] --> B{Need to process large data?}
    B -->|Yes| C{Data can be processed independently?}
    B -->|No| D{Need to make decisions?}

    C -->|Yes| E[Map Reduce]
    C -->|No| F[Workflow]

    D -->|Yes| G{Complex, multi-step reasoning?}
    D -->|No| H[Simple Workflow]

    G -->|Yes| I[Agent]
    G -->|No| J{Need external knowledge?}

    J -->|Yes| K[RAG]
    J -->|No| L[Structured Output]
```

## Combining Patterns

Many real-world applications combine multiple patterns. For example:

- An **Agent** that uses **RAG** to retrieve information before making decisions
- A **Workflow** that includes **Map Reduce** steps for processing large datasets
- **Multi-Agents** that each use **Structured Output** for consistent communication

The modular nature of BrainyFlow makes it easy to combine these patterns to solve complex problems.

## Next Steps

Explore each design pattern in detail to understand how to implement it in your application:

- Start with [Agent](./agent.md) to learn about autonomous decision-making
- Check out [Workflow](./workflow.md) for sequential processing patterns
- Dive into [RAG](./rag.md) to see how to augment LLMs with external knowledge




================================================
File: docs/design_pattern/agent.md
================================================
---
title: 'Agent'
---

# Agent

Agent is a powerful design pattern in which nodes can take dynamic actions based on the context.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/agent.png?raw=true" width="350"/>
</div>

## Implement Agent with Graph

1. **Context and Action:** Implement nodes that supply context and perform actions.
2. **Branching:** Use branching to connect each action node to an agent node. Use action to allow the agent to direct the [flow](../core_abstraction/flow.md) between nodes—and potentially loop back for multi-step.
3. **Agent Node:** Provide a prompt to decide action—for example:

{% tabs %}
{% tab title="Python" %}

````python
f"""
### CONTEXT
Task: {task_description}
Previous Actions: {previous_actions}
Current State: {current_state}

### ACTION SPACE
[1] search
  Description: Use web search to get results
  Parameters:
    - query (str): What to search for

[2] answer
  Description: Conclude based on the results
  Parameters:
    - result (str): Final answer to provide

### NEXT ACTION
Decide the next action based on the current context and available action space.
Return your response in the following format:

```yaml
thinking: |
    <your step-by-step reasoning process>
action: <action_name>
parameters:
    <parameter_name>: <parameter_value>
```"""
````

{% endtab %}

{% tab title="TypeScript" %}

```typescript
;`### CONTEXT
Task: ${taskDescription}
Previous Actions: ${previousActions}
Current State: ${currentState}

### ACTION SPACE
[1] search
  Description: Use web search to get results
  Parameters:
    - query (string): What to search for

[2] answer
  Description: Conclude based on the results  
  Parameters:
    - result (string): Final answer to provide

### NEXT ACTION
Decide the next action based on the current context and available action space.
Return your response in the following format:

\`\`\`yaml
thinking: |
    <your step-by-step reasoning process>
action: <action_name>
parameters:
    <parameter_name>: <parameter_value>
\`\`\``
```

{% endtab %}
{% endtabs %}

The core of building **high-performance** and **reliable** agents boils down to:

1. **Context Management:** Provide _relevant, minimal context._ For example, rather than including an entire chat history, retrieve the most relevant via [RAG](./rag.md). Even with larger context windows, LLMs still fall victim to ["lost in the middle"](https://arxiv.org/abs/2307.03172), overlooking mid-prompt content.

2. **Action Space:** Provide _a well-structured and unambiguous_ set of actions—avoiding overlap like separate `read_databases` or `read_csvs`. Instead, import CSVs into the database.

## Example Good Action Design

- **Incremental:** Feed content in manageable chunks (500 lines or 1 page) instead of all at once.

- **Overview-zoom-in:** First provide high-level structure (table of contents, summary), then allow drilling into details (raw texts).

- **Parameterized/Programmable:** Instead of fixed actions, enable parameterized (columns to select) or programmable (SQL queries) actions, for example, to read CSV files.

- **Backtracking:** Let the agent undo the last step instead of restarting entirely, preserving progress when encountering errors or dead ends.

## Example: Search Agent

This agent:

1. Decides whether to search or answer
2. If searches, loops back to decide if more search needed
3. Answers when enough context gathered

{% tabs %}
{% tab title="Python" %}

````python
import asyncio
import yaml # Assuming call_llm and search_web are defined elsewhere

class DecideAction(Node):
    async def prep(self, shared):
        context = shared.get("context", "No previous search")
        query = shared["query"]
        return query, context

    async def exec(self, inputs):
        query, context = inputs
        prompt = f"""
Given input: {query}
Previous search results: {context}
Should I: 1) Search web for more info 2) Answer with current knowledge
Output in yaml:
```yaml
action: search/answer
reason: why this action
search_term: search phrase if action is search
```"""
        resp = call_llm(prompt)
        yaml_str = resp.split("```yaml")[1].split("```")[0].strip()
        result = yaml.safe_load(yaml_str)

        assert isinstance(result, dict)
        assert "action" in result
        assert "reason" in result
        assert result["action"] in ["search", "answer"]
        if result["action"] == "search":
            assert "search_term" in result

        return result

    async def post(self, shared, prep_res, exec_res):
        if exec_res["action"] == "search":
            shared["search_term"] = exec_res["search_term"]
        return exec_res["action"]
````

{% endtab %}

{% tab title="TypeScript" %}

````typescript
class DecideAction extends Node {
  async prep(shared: any): Promise<[string, string]> {
    const context = shared.context || 'No previous search'
    const query = shared.query
    return [query, context]
  }

  async exec(inputs: [string, string]): Promise<any> {
    const [query, context] = inputs
    const prompt = `
Given input: ${query}
Previous search results: ${context}
Should I: 1) Search web for more info 2) Answer with current knowledge
Output in yaml:
\`\`\`yaml
action: search/answer
reason: why this action  
search_term: search phrase if action is search
\`\`\``

    const resp = await callLLM(prompt)
    const yamlStr = resp.split('```yaml')[1].split('```')[0].trim()
    const result = parseYaml(yamlStr)

    if (typeof result !== 'object' || !result) {
      throw new Error('Invalid YAML response')
    }
    if (!('action' in result)) {
      throw new Error('Missing action in response')
    }
    if (!('reason' in result)) {
      throw new Error('Missing reason in response')
    }
    if (!['search', 'answer'].includes(result.action)) {
      throw new Error('Invalid action value')
    }
    if (result.action === 'search' && !('search_term' in result)) {
      throw new Error('Missing search_term for search action')
    }

    return result
  }

  async post(shared: any, prepRes: any, execRes: any): Promise<string> {
    if (execRes.action === 'search') {
      shared.search_term = execRes.search_term
    }
    return execRes.action
  }
}
````

{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Python" %}

```python
class SearchWeb(Node):
    async def prep(self, shared):
        return shared["search_term"]

    async def exec(self, search_term):
        return await search_web(search_term)

    async def post(self, shared, prep_res, exec_res):
        prev_searches = shared.get("context", [])
        shared["context"] = prev_searches + [
            {"term": shared["search_term"], "result": exec_res}
        ]
        return "decide"
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class SearchWeb extends Node {
  async prep(shared: any): Promise<string> {
    return shared.search_term
  }

  async exec(searchTerm: string): Promise<any> {
    return await searchWeb(searchTerm)
  }

  async post(shared: any, prepRes: any, execRes: any): Promise<string> {
    const prevSearches = shared.context || []
    shared.context = [...prevSearches, { term: shared.search_term, result: execRes }]
    return 'decide'
  }
}
```

{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Python" %}

```python
class DirectAnswer(Node):
    async def prep(self, shared):
        return shared["query"], shared.get("context", "")

    async def exec(self, inputs):
        query, context = inputs
        return call_llm(f"Context: {context}\nAnswer: {query}")

    async def post(self, shared, prep_res, exec_res):
       print(f"Answer: {exec_res}")
       shared["answer"] = exec_res
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class DirectAnswer extends Node {
  async prep(shared: any): Promise<[string, string]> {
    return [shared.query, shared.context || '']
  }

  async exec(inputs: [string, string]): Promise<string> {
    const [query, context] = inputs
    return await callLLM(`Context: ${context}\nAnswer: ${query}`)
  }

  async post(shared: any, prepRes: any, execRes: string): Promise<void> {
    console.log(`Answer: ${execRes}`)
    shared.answer = execRes
  }
}
```

{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Python" %}

```python
# Connect nodes
decide = DecideAction()
search = SearchWeb()
answer = DirectAnswer()

decide - "search" >> search
decide - "answer" >> answer
search - "decide" >> decide # Loop back

flow = Flow(start=decide)

async def main():
    shared_data = {"query": "Who won the Nobel Prize in Physics 2024?"}
    result = await flow.run(shared_data)
    print(result) # Or handle result as needed
    print(shared_data) # See final shared state

if __name__ == "__main__":
    asyncio.run(main())
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
// Connect nodes
const decide = new DecideAction()
const search = new SearchWeb()
const answer = new DirectAnswer()

// Using operator overloading equivalents
decide.on('search', search)
decide.on('answer', answer)
search.on('decide', decide) // Loop back

const flow = new Flow(decide)

async function main() {
  const sharedData = { query: 'Who won the Nobel Prize in Physics 2024?' }
  const result = await flow.run(sharedData) // Added await
  console.log(result) // Or handle result as needed
  console.log(sharedData) // See final shared state
}

main().catch(console.error) // Execute async main function
```

{% endtab %}
{% endtabs %}




================================================
File: docs/design_pattern/workflow.md
================================================
---
title: 'Workflow'
---

# Workflow

Many real-world tasks are too complex for one LLM call. The solution is **Task Decomposition**: decompose them into a [chain](../core_abstraction/flow.md) of multiple Nodes.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/workflow.png?raw=true" width="400"/>
</div>

{% hint style="success" %}
You don't want to make each task **too coarse**, because it may be _too complex for one LLM call_.
You don't want to make each task **too granular**, because then _the LLM call doesn't have enough context_ and results are _not consistent across nodes_.

You usually need multiple _iterations_ to find the _sweet spot_. If the task has too many _edge cases_, consider using [Agents](./agent.md).
{% endhint %}

### Example: Article Writing

{% tabs %}
{% tab title="Python" %}

```python
import asyncio
from brainyflow import Node, Flow

class GenerateOutline(Node):
    async def prep(self, shared): return shared["topic"]
    async def exec(self, topic): return call_llm(f"Create a detailed outline for an article about {topic}")
    async def post(self, shared, prep_res, exec_res): shared["outline"] = exec_res

class WriteSection(Node):
    async def prep(self, shared): return shared["outline"]
    async def exec(self, outline): return call_llm(f"Write content based on this outline: {outline}")
    async def post(self, shared, prep_res, exec_res): shared["draft"] = exec_res

class ReviewAndRefine(Node):
    async def prep(self, shared): return shared["draft"]
    async def exec(self, draft): return call_llm(f"Review and improve this draft: {draft}")
    async def post(self, shared, prep_res, exec_res): shared["final_article"] = exec_res

# Connect nodes
outline = GenerateOutline()
write = WriteSection()
review = ReviewAndRefine()

outline >> write >> review

# Create and run flow
writing_flow = Flow(start=outline)

async def main():
    shared = {"topic": "AI Safety"}
    await writing_flow.run(shared)
    print("Final Article:", shared.get("final_article", "Not generated"))

if __name__ == "__main__":
    asyncio.run(main())
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class GenerateOutline extends Node {
  async prep(shared: any): Promise<any> {
    return shared['topic']
  }
  async exec(topic: string): Promise<any> {
    return await callLLM(`Create a detailed outline for an article about ${topic}`)
  }
  async post(shared: any, prepRes: any, execRes: any): Promise<void> {
    shared['outline'] = execRes
  }
}

class WriteSection extends Node {
  async prep(shared: any): Promise<any> {
    return shared['outline']
  }
  async exec(outline: string): Promise<any> {
    return await callLLM(`Write content based on this outline: ${outline}`)
  }
  async post(shared: any, prepRes: any, execRes: any): Promise<void> {
    shared['draft'] = execRes
  }
}

class ReviewAndRefine extends Node {
  async prep(shared: any): Promise<any> {
    return shared['draft']
  }
  async exec(draft: string): Promise<any> {
    return await callLLM(`Review and improve this draft: ${draft}`)
  }
  async post(shared: any, prepRes: any, execRes: any): Promise<void> {
    shared['final_article'] = execRes
  }
}

// Connect nodes
const outline = new GenerateOutline()
const write = new WriteSection()
const review = new ReviewAndRefine()

outline.next(write).next(review)

// Create and run flow
const writingFlow = new Flow(outline)

async function main() {
  const shared = { topic: 'AI Safety' }
  await writingFlow.run(shared)
  console.log(`Final Article: ${shared['final_article'] || 'Not generated'}`)
}

main().catch(console.error) // Execute async main function
```

{% endtab %}
{% endtabs %}

For _dynamic cases_, consider using [Agents](./agent.md).




================================================
File: docs/design_pattern/rag.md
================================================
---
title: 'RAG'
---

# RAG (Retrieval Augmented Generation)

For certain LLM tasks like answering questions, providing relevant context is essential. One common architecture is a **two-stage** RAG pipeline:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/rag.png?raw=true" width="400"/>
</div>

1. **Offline stage**: Preprocess and index documents ("building the index").
2. **Online stage**: Given a question, generate answers by retrieving the most relevant context.

---

## Stage 1: Offline Indexing

We create three Nodes:

1. `ChunkDocs` – [chunks](../utility_function/chunking.md) raw text.
2. `EmbedDocs` – [embeds](../utility_function/embedding.md) each chunk.
3. `StoreIndex` – stores embeddings into a [vector database](../utility_function/vector.md).

{% tabs %}
{% tab title="Python" %}

```python
import asyncio
from brainyflow import Node, Flow, SequentialBatchNode

class ChunkDocs(SequentialBatchNode):
    async def prep(self, shared):
        # A list of file paths in shared["files"]. We process each file.
        return shared["files"]

    async def exec(self, filepath):
        # read file content. In real usage, do error handling.
        with open(filepath, "r", encoding="utf-8") as f:
            text = f.read()
        # chunk by 100 chars each
        chunks = []
        size = 100
        for i in range(0, len(text), size):
            chunks.append(text[i : i + size])
        return chunks

    async def post(self, shared, prep_res, exec_res_list):
        # exec_res_list is a list of chunk-lists, one per file.
        # flatten them all into a single list of chunks.
        all_chunks = []
        for chunk_list in exec_res_list:
            all_chunks.extend(chunk_list)
        shared["all_chunks"] = all_chunks
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class ChunkDocs extends SequentialBatchNode {
  async prep(shared: any): Promise<string[]> {
    // A list of file paths in shared["files"]. We process each file.
    return shared['files']
  }

  async exec(filepath: string): Promise<string[]> {
    // read file content. In real usage, do error handling.
    const text = fs.readFileSync(filepath, 'utf-8')
    // chunk by 100 chars each
    const chunks: string[] = []
    const size = 100
    for (let i = 0; i < text.length; i += size) {
      chunks.push(text.slice(i, i + size))
    }
    return chunks
  }

  async post(shared: any, prepRes: string[], execResList: string[][]): Promise<void> {
    // execResList is a list of chunk-lists, one per file.
    // flatten them all into a single list of chunks.
    const allChunks: string[] = []
    for (const chunkList of execResList) {
      allChunks.push(...chunkList)
    }
    shared['all_chunks'] = allChunks
  }
}
```

{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Python" %}

```python
class EmbedDocs(SequentialBatchNode):
    async def prep(self, shared):
        return shared["all_chunks"]

    async def exec(self, chunk):
        return get_embedding(chunk)

    async def post(self, shared, prep_res, exec_res_list):
        # Store the list of embeddings.
        shared["all_embeds"] = exec_res_list
        print(f"Total embeddings: {len(exec_res_list)}")
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class EmbedDocs extends SequentialBatchNode {
  async prep(shared: any): Promise<string[]> {
    return shared['all_chunks']
  }

  async exec(chunk: string): Promise<number[]> {
    return await getEmbedding(chunk)
  }

  async post(shared: any, prepRes: string[], execResList: number[][]): Promise<void> {
    // Store the list of embeddings.
    shared['all_embeds'] = execResList
    console.log(`Total embeddings: ${execResList.length}`)
  }
}
```

{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Python" %}

```python
class StoreIndex(Node):
    async def prep(self, shared):
        # We'll read all embeds from shared.
        return shared["all_embeds"]

    async def exec(self, all_embeds):
        # Create a vector index (faiss or other DB in real usage).
        index = create_index(all_embeds)
        return index

    async def post(self, shared, prep_res, index):
        shared["index"] = index
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class StoreIndex extends Node {
  async prep(shared: any): Promise<number[][]> {
    // We'll read all embeds from shared.
    return shared['all_embeds']
  }

  async exec(allEmbeds: number[][]): Promise<any> {
    // Create a vector index (faiss or other DB in real usage).
    const index = createIndex(allEmbeds)
    return index
  }

  async post(shared: any, prepRes: number[][], index: any): Promise<void> {
    shared['index'] = index
  }
}
```

{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Python" %}

```python
# Wire them in sequence
chunk_node = ChunkDocs()
embed_node = EmbedDocs()
store_node = StoreIndex()

chunk_node >> embed_node >> store_node

OfflineFlow = Flow(start=chunk_node)
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
// Wire them in sequence
const chunkNode = new ChunkDocs()
const embedNode = new EmbedDocs()
const storeNode = new StoreIndex()

chunkNode.next(embedNode).next(storeNode)

const OfflineFlow = new Flow(chunkNode)
```

{% endtab %}
{% endtabs %}

Usage example:

{% tabs %}
{% tab title="Python" %}

```python
shared = {
    "files": ["doc1.txt", "doc2.txt"],  # any text files
}
await OfflineFlow.run(shared)
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
const shared = {
  files: ['doc1.txt', 'doc2.txt'], // any text files
}
await OfflineFlow.run(shared)
```

{% endtab %}
{% endtabs %}

---

## Stage 2: Online Query & Answer

We have 3 nodes:

1. `EmbedQuery` – embeds the user’s question.
2. `RetrieveDocs` – retrieves top chunk from the index.
3. `GenerateAnswer` – calls the LLM with the question + chunk to produce the final answer.

{% tabs %}
{% tab title="Python" %}

```python
class EmbedQuery(Node):
    async def prep(self, shared):
        return shared["question"]

    async def exec(self, question):
        return get_embedding(question)

    async def post(self, shared, prep_res, q_emb):
        shared["q_emb"] = q_emb
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class EmbedQuery extends Node {
  async prep(shared: any): Promise<string> {
    return shared['question']
  }

  async exec(question: string): Promise<number[]> {
    return await getEmbedding(question)
  }

  async post(shared: any, prepRes: string, qEmb: number[]): Promise<void> {
    shared['q_emb'] = qEmb
  }
}
```

{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Python" %}

```python
class RetrieveDocs(Node):
    async def prep(self, shared):
        # We'll need the query embedding, plus the offline index/chunks
        return shared["q_emb"], shared["index"], shared["all_chunks"]

    async def exec(self, inputs):
        q_emb, index, chunks = inputs
        I, D = search_index(index, q_emb, top_k=1)
        best_id = I[0][0]
        relevant_chunk = chunks[best_id]
        return relevant_chunk

    async def post(self, shared, prep_res, relevant_chunk):
        shared["retrieved_chunk"] = relevant_chunk
        print("Retrieved chunk:", relevant_chunk[:60], "...")
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class RetrieveDocs extends Node {
  async prep(shared: any): Promise<[number[], any, string[]]> {
    // We'll need the query embedding, plus the offline index/chunks
    return [shared['q_emb'], shared['index'], shared['all_chunks']]
  }

  async exec(inputs: [number[], any, string[]]): Promise<string> {
    const [qEmb, index, chunks] = inputs
    const [I, D] = searchIndex(index, qEmb, 1)
    const bestId = I[0][0]
    const relevantChunk = chunks[bestId]
    return relevantChunk
  }

  async post(
    shared: any,
    prepRes: [number[], any, string[]],
    relevantChunk: string,
  ): Promise<void> {
    shared['retrieved_chunk'] = relevantChunk
    console.log(`Retrieved chunk: ${relevantChunk.slice(0, 60)}...`)
  }
}
```

{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Python" %}

```python
class GenerateAnswer(Node):
    async def prep(self, shared):
        return shared["question"], shared["retrieved_chunk"]

    async def exec(self, inputs):
        question, chunk = inputs
        prompt = f"Question: {question}\nContext: {chunk}\nAnswer:"
        return call_llm(prompt)

    async def post(self, shared, prep_res, answer):
        shared["answer"] = answer
        print("Answer:", answer)
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class GenerateAnswer extends Node {
  async prep(shared: any): Promise<[string, string]> {
    return [shared['question'], shared['retrieved_chunk']]
  }

  async exec(inputs: [string, string]): Promise<string> {
    const [question, chunk] = inputs
    const prompt = `Question: ${question}\nContext: ${chunk}\nAnswer:`
    return await callLLM(prompt)
  }

  async post(shared: any, prepRes: [string, string], answer: string): Promise<void> {
    shared['answer'] = answer
    console.log(`Answer: ${answer}`)
  }
}
```

{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Python" %}

```python
embed_qnode = EmbedQuery()
retrieve_node = RetrieveDocs()
generate_node = GenerateAnswer()

embed_qnode >> retrieve_node >> generate_node
OnlineFlow = Flow(start=embed_qnode)
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
const embedQNode = new EmbedQuery()
const retrieveNode = new RetrieveDocs()
const generateNode = new GenerateAnswer()

embedQNode.next(retrieveNode).next(generateNode)
const OnlineFlow = new Flow(embedQNode)
```

{% endtab %}
{% endtabs %}

Usage example:

{% tabs %}
{% tab title="Python" %}

```python
async def run_online(shared_from_offline):
    # Suppose we already ran OfflineFlow (run_offline) and have:
    # shared_from_offline["all_chunks"], shared_from_offline["index"], etc.
    shared_from_offline["question"] = "Why do people like cats?"

    await OnlineFlow.run(shared_from_offline)
    # final answer in shared_from_offline["answer"]
    print("Final Answer:", shared_from_offline["answer"])
    return shared_from_offline

# Example usage combining both stages
async def main():
    offline_shared = await run_offline()
    await run_online(offline_shared)

if __name__ == "__main__":
    asyncio.run(main())
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
async function runOnline(sharedFromOffline: any): Promise<any> {
  // Suppose we already ran OfflineFlow (runOffline) and have:
  // sharedFromOffline["all_chunks"], sharedFromOffline["index"], etc.
  sharedFromOffline['question'] = 'Why do people like cats?'

  await OnlineFlow.run(sharedFromOffline)
  // final answer in sharedFromOffline["answer"]
  console.log(`Final Answer: ${sharedFromOffline['answer']}`)
  return sharedFromOffline
}

// Example usage combining both stages
async function main() {
  const offlineShared = await runOffline()
  await runOnline(offlineShared)
}

main().catch(console.error) // Execute async main function
```

{% endtab %}
{% endtabs %}




================================================
File: docs/design_pattern/mapreduce.md
================================================
---
title: 'Map Reduce'
---

# Map Reduce

MapReduce is a design pattern suitable when you have either:

- Large input data (e.g., multiple files to process), or
- Large output data (e.g., multiple forms to fill)

and there is a logical way to break the task into smaller, ideally independent parts.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/mapreduce.png?raw=true" width="400"/>
</div>

You first break down the task using [BatchNode](../core_abstraction/batch.md) in the map phase, followed by aggregation in the reduce phase.

### Example: Document Summarization

{% tabs %}
{% tab title="Python" %}

```python
import asyncio
from brainyflow import Node, SequentialBatchNode

class SummarizeAllFiles(SequentialBatchNode):
    async def prep(self, shared):
        files_dict = shared["files"]  # e.g. 10 files
        return list(files_dict.items())  # [("file1.txt", "aaa..."), ("file2.txt", "bbb..."), ...]

    async def exec(self, one_file):
        filename, file_content = one_file
        summary_text = call_llm(f"Summarize the following file:\n{file_content}")
        return (filename, summary_text)

    async def post(self, shared, prep_res, exec_res_list):
        shared["file_summaries"] = dict(exec_res_list)

class CombineSummaries(Node):
    async def prep(self, shared):
        return shared["file_summaries"]

    async def exec(self, file_summaries):
        # format as: "File1: summary\nFile2: summary...\n"
        text_list = []
        for fname, summ in file_summaries.items():
            text_list.append(f"{fname} summary:\n{summ}\n")
        big_text = "\n---\n".join(text_list)

        return call_llm(f"Combine these file summaries into one final summary:\n{big_text}")

    async def post(self, shared, prep_res, final_summary):
        shared["all_files_summary"] = final_summary

batch_node = SummarizeAllFiles()
combine_node = CombineSummaries()
batch_node >> combine_node

flow = Flow(start=batch_node)

async def main():
    shared = {
        "files": {
            "file1.txt": "Alice was beginning to get very tired of sitting by her sister...",
            "file2.txt": "Some other interesting text ...",
            # ...
        }
    }
    await flow.run(shared)
    print("Individual Summaries:", shared["file_summaries"])
    print("\nFinal Summary:\n", shared["all_files_summary"])

if __name__ == "__main__":
    asyncio.run(main())
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class SummarizeAllFiles extends SequentialBatchNode {
  async prep(shared: any): Promise<[string, string][]> {
    const filesDict = shared.files // e.g. 10 files
    return Object.entries(filesDict) // [["file1.txt", "aaa..."], ["file2.txt", "bbb..."], ...]
  }

  async exec(oneFile: [string, string]): Promise<[string, string]> {
    const [filename, fileContent] = oneFile
    const summaryText = await callLLM(`Summarize the following file:\n${fileContent}`)
    return [filename, summaryText]
  }

  async post(shared: any, prepRes: any, execResList: [string, string][]): Promise<void> {
    shared.file_summaries = Object.fromEntries(execResList)
  }
}

class CombineSummaries extends Node {
  async prep(shared: any): Promise<Record<string, string>> {
    return shared.file_summaries
  }

  async exec(fileSummaries: Record<string, string>): Promise<string> {
    // format as: "File1: summary\nFile2: summary...\n"
    const textList: string[] = []
    for (const [fname, summ] of Object.entries(fileSummaries)) {
      textList.push(`${fname} summary:\n${summ}\n`)
    }
    const bigText = textList.join('\n---\n')

    return await callLLM(`Combine these file summaries into one final summary:\n${bigText}`)
  }

  async post(shared: any, prepRes: any, finalSummary: string): Promise<void> {
    shared.all_files_summary = finalSummary
  }
}

const batchNode = new SummarizeAllFiles()
const combineNode = new CombineSummaries()
batchNode.next(combineNode)

const flow = new Flow(batchNode)

async function main() {
  const shared = {
    files: {
      'file1.txt': 'Alice was beginning to get very tired of sitting by her sister...',
      'file2.txt': 'Some other interesting text ...',
      // ...
    },
  }
  await flow.run(shared)
  console.log('Individual Summaries:', shared.file_summaries)
  console.log('\nFinal Summary:\n', shared.all_files_summary)
}

main().catch(console.error)
```

{% endtab %}
{% endtabs %}




================================================
File: docs/design_pattern/structure.md
================================================
---
title: 'Structured Output'
---

# Structured Output

In many use cases, you may want the LLM to output a specific structure, such as a list or a dictionary with predefined keys.

There are several approaches to achieve a structured output:

- **Prompting** the LLM to strictly return a defined structure.
- Using LLMs that natively support **schema enforcement**.
- **Post-processing** the LLM's response to extract structured content.

In practice, **Prompting** is simple and reliable for modern LLMs.

### Example Use Cases

- Extracting Key Information

```yaml
product:
  name: Widget Pro
  price: 199.99
  description: |
    A high-quality widget designed for professionals.
    Recommended for advanced users.
```

- Summarizing Documents into Bullet Points

```yaml
summary:
  - This product is easy to use.
  - It is cost-effective.
  - Suitable for all skill levels.
```

- Generating Configuration Files

```yaml
server:
  host: 127.0.0.1
  port: 8080
  ssl: true
```

## Prompt Engineering

When prompting the LLM to produce **structured** output:

1. **Wrap** the structure in code fences (e.g., `yaml`).
2. **Validate** that all required fields exist (and let `Node` handles retry).

### Example Text Summarization

{% tabs %}
{% tab title="Python" %}

````python
import yaml

class SummarizeNode(Node):
    async def exec(self, prep_res):
        # Suppose `prep_res` is the text to summarize.
        prompt = f"""
Please summarize the following text as YAML, with exactly 3 bullet points

{prep_res}

Now, output:
```yaml
summary:
  - bullet 1
  - bullet 2
  - bullet 3
```"""
        response = call_llm(prompt)
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()

        structured_result = yaml.safe_load(yaml_str)

        assert "summary" in structured_result
        assert isinstance(structured_result["summary"], list)

        return structured_result
````

{% endtab %}

{% tab title="TypeScript" %}

````typescript
class SummarizeNode extends Node {
  async exec(prepRes: string): Promise<any> {
    // Suppose prepRes is the text to summarize
    const prompt = `
Please summarize the following text as YAML, with exactly 3 bullet points

${prepRes}

Now, output:
\`\`\`yaml
summary:
  - bullet 1
  - bullet 2
  - bullet 3
\`\`\``

    const response = await callLLM(prompt)
    const yamlStr = response.split('```yaml')[1].split('```')[0].trim()

    // In TypeScript we would typically use a YAML parser like 'yaml'
    const structuredResult = require('yaml').parse(yamlStr)

    if (!('summary' in structuredResult)) {
      throw new Error("Missing 'summary' in result")
    }
    if (!Array.isArray(structuredResult.summary)) {
      throw new Error('Summary must be an array')
    }

    return structuredResult
  }
}
````

{% endtab %}
{% endtabs %}

{% hint style="info" %}
Besides using `assert` statements, another popular way to validate schemas is [Pydantic](https://github.com/pydantic/pydantic)
{% endhint %}

### Why YAML instead of JSON?

Current LLMs struggle with escaping. YAML is easier with strings since they don't always need quotes.

**In JSON**

```json
{
  "dialogue": "Alice said: \"Hello Bob.\\nHow are you?\\nI am good.\""
}
```

- Every double quote inside the string must be escaped with `\"`.
- Each newline in the dialogue must be represented as `\n`.

**In YAML**

```yaml
dialogue: |
  Alice said: "Hello Bob.
  How are you?
  I am good."
```

- No need to escape interior quotes—just place the entire text under a block literal (`|`).
- Newlines are naturally preserved without needing `\n`.




================================================
File: docs/design_pattern/multi_agent.md
================================================
---
title: '(Advanced) Multi-Agents'
---

# (Advanced) Multi-Agents

Multiple [Agents](./flow.md) can work together by handling subtasks and communicating the progress.
Communication between agents is typically implemented using message queues in shared storage.

{% hint style="success" %}
Most of time, you don't need Multi-Agents. Start with a simple solution first.
{% endhint %}

### Example Agent Communication: Message Queue

Here's a simple example showing how to implement agent communication using `asyncio.Queue`.
The agent listens for messages, processes them, and continues listening:

{% tabs %}
{% tab title="Python" %}

```python
import asyncio
from brainyflow import Node, Flow

class AgentNode(Node):
    async def prep(self, _):
        message_queue = self.params["messages"]
        message = await message_queue.get()
        print(f"Agent received: {message}")
        return message

# Create node and flow
agent = AgentNode()
agent >> agent  # connect to self
flow = Flow(start=agent)

# Create heartbeat sender
async def send_system_messages(message_queue):
    counter = 0
    messages = [
        "System status: all systems operational",
        "Memory usage: normal",
        "Network connectivity: stable",
        "Processing load: optimal"
    ]

    while True:
        message = f"{messages[counter % len(messages)]} | timestamp_{counter}"
        await message_queue.put(message)
        counter += 1
        await asyncio.sleep(1)

async def main():
    message_queue = asyncio.Queue()
    shared = {}
    flow.set_params({"messages": message_queue})

    # Run both coroutines
    await asyncio.gather(
        flow.run(shared),
        send_system_messages(message_queue)
    )

asyncio.run(main())
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class AgentNode extends Node {
  async prep(_: any): Promise<string> {
    const messageQueue = this.params.messages as AsyncQueue<string>
    const message = await messageQueue.get()
    console.log(`Agent received: ${message}`)
    return message
  }
}

// Create node and flow
const agent = new AgentNode()
agent.next(agent) // connect to self
const flow = new Flow(agent)

// Create heartbeat sender
async function sendSystemMessages(messageQueue: AsyncQueue<string>) {
  let counter = 0
  const messages = [
    'System status: all systems operational',
    'Memory usage: normal',
    'Network connectivity: stable',
    'Processing load: optimal',
  ]

  while (true) {
    const message = `${messages[counter % messages.length]} | timestamp_${counter}`
    await messageQueue.put(message)
    counter++
    await new Promise((resolve) => setTimeout(resolve, 1000))
  }
}

async function main() {
  const messageQueue = new AsyncQueue<string>()
  const shared = {}
  flow.setParams({ messages: messageQueue })

  // Run both coroutines
  await Promise.all([flow.run(shared), sendSystemMessages(messageQueue)])
}

class AsyncQueue<T> {
  private queue: T[] = []
  private waiting: ((value: T) => void)[] = []

  async get(): Promise<T> {
    if (this.queue.length > 0) {
      return this.queue.shift()!
    }
    return new Promise((resolve) => {
      this.waiting.push(resolve)
    })
  }

  async put(item: T): Promise<void> {
    if (this.waiting.length > 0) {
      const resolve = this.waiting.shift()!
      resolve(item)
    } else {
      this.queue.push(item)
    }
  }
}

main().catch(console.error)
```

{% endtab %}
{% endtabs %}

The output:

```
Agent received: System status: all systems operational | timestamp_0
Agent received: Memory usage: normal | timestamp_1
Agent received: Network connectivity: stable | timestamp_2
Agent received: Processing load: optimal | timestamp_3
```

### Interactive Multi-Agent Example: Taboo Game

Here's a more complex example where two agents play the word-guessing game Taboo.
One agent provides hints while avoiding forbidden words, and another agent tries to guess the target word:

{% tabs %}
{% tab title="Python" %}

```python
import asyncio
from brainyflow import Node, Flow

class Hinter(Node):
    async def prep(self, shared):
        guess = await shared["hinter_queue"].get()
        if guess == "GAME_OVER":
            return None
        return shared["target_word"], shared["forbidden_words"], shared.get("past_guesses", [])

    async def exec(self, inputs):
        if inputs is None:
            return None
        target, forbidden, past_guesses = inputs
        prompt = f"Generate hint for '{target}'\nForbidden words: {forbidden}"
        if past_guesses:
            prompt += f"\nPrevious wrong guesses: {past_guesses}\nMake hint more specific."
        prompt += "\nUse at most 5 words."

        hint = call_llm(prompt)
        print(f"\nHinter: Here's your hint - {hint}")
        return hint

    async def post(self, shared, prep_res, exec_res):
        if exec_res is None:
            return "end"
        await shared["guesser_queue"].put(exec_res)
        return "continue"

class Guesser(Node):
    async def prep(self, shared):
        hint = await shared["guesser_queue"].get()
        return hint, shared.get("past_guesses", [])

    async def exec(self, inputs):
        hint, past_guesses = inputs
        prompt = f"Given hint: {hint}, past wrong guesses: {past_guesses}, make a new guess. Directly reply a single word:"
        guess = call_llm(prompt)
        print(f"Guesser: I guess it's - {guess}")
        return guess

    async def post(self, shared, prep_res, exec_res):
        if exec_res.lower() == shared["target_word"].lower():
            print("Game Over - Correct guess!")
            await shared["hinter_queue"].put("GAME_OVER")
            return "end"

        if "past_guesses" not in shared:
            shared["past_guesses"] = []
        shared["past_guesses"].append(exec_res)

        await shared["hinter_queue"].put(exec_res)
        return "continue"

async def main():
    # Set up game
    shared = {
        "target_word": "nostalgia",
        "forbidden_words": ["memory", "past", "remember", "feeling", "longing"],
        "hinter_queue": asyncio.Queue(),
        "guesser_queue": asyncio.Queue()
    }

    print("Game starting!")
    print(f"Target word: {shared['target_word']}")
    print(f"Forbidden words: {shared['forbidden_words']}")

    # Initialize by sending empty guess to hinter
    await shared["hinter_queue"].put("")

    # Create nodes and flows
    hinter = Hinter()
    guesser = Guesser()

    # Set up flows
    hinter_flow = Flow(start=hinter)
    guesser_flow = Flow(start=guesser)

    # Connect nodes to themselves
    hinter - "continue" >> hinter
    guesser - "continue" >> guesser

    # Run both agents concurrently
    await asyncio.gather(
        hinter_flow.run(shared),
        guesser_flow.run(shared)
    )

asyncio.run(main())
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
class Hinter extends Node {
  async prep(shared: any): Promise<any> {
    const guess = await shared.hinterQueue.get()
    if (guess === 'GAME_OVER') {
      return null
    }
    return [shared.targetWord, shared.forbiddenWords, shared.pastGuesses || []]
  }

  async exec(inputs: any): Promise<string | null> {
    if (inputs === null) return null
    const [target, forbidden, pastGuesses] = inputs
    let prompt = `Generate hint for '${target}'\nForbidden words: ${forbidden}`
    if (pastGuesses.length > 0) {
      prompt += `\nPrevious wrong guesses: ${pastGuesses}\nMake hint more specific.`
    }
    prompt += '\nUse at most 5 words.'

    const hint = await callLLM(prompt)
    console.log(`\nHinter: Here's your hint - ${hint}`)
    return hint
  }

  async post(shared: any, prepRes: any, execRes: any): Promise<string> {
    if (execRes === null) return 'end'
    await shared.guesserQueue.put(execRes)
    return 'continue'
  }
}

class Guesser extends Node {
  async prep(shared: any): Promise<any> {
    const hint = await shared.guesserQueue.get()
    return [hint, shared.pastGuesses || []]
  }

  async exec(inputs: any): Promise<string> {
    const [hint, pastGuesses] = inputs
    const prompt = `Given hint: ${hint}, past wrong guesses: ${pastGuesses}, make a new guess. Directly reply a single word:`
    const guess = await callLLM(prompt)
    console.log(`Guesser: I guess it's - ${guess}`)
    return guess
  }

  async post(shared: any, prepRes: any, execRes: any): Promise<string> {
    if (execRes.toLowerCase() === shared.targetWord.toLowerCase()) {
      console.log('Game Over - Correct guess!')
      await shared.hinterQueue.put('GAME_OVER')
      return 'end'
    }

    if (!shared.pastGuesses) {
      shared.pastGuesses = []
    }
    shared.pastGuesses.push(execRes)

    await shared.hinterQueue.put(execRes)
    return 'continue'
  }
}

async function main() {
  // Set up game
  const shared = {
    targetWord: 'nostalgia',
    forbiddenWords: ['memory', 'past', 'remember', 'feeling', 'longing'],
    hinterQueue: new AsyncQueue<string>(),
    guesserQueue: new AsyncQueue<string>(),
  }

  console.log('Game starting!')
  console.log(`Target word: ${shared.targetWord}`)
  console.log(`Forbidden words: ${shared.forbiddenWords}`)

  // Initialize by sending empty guess to hinter
  await shared.hinterQueue.put('')

  // Create nodes and flows
  const hinter = new Hinter()
  const guesser = new Guesser()

  // Set up flows
  const hinterFlow = new Flow(hinter)
  const guesserFlow = new Flow(guesser)

  // Connect nodes to themselves
  hinter.on('continue', hinter)
  guesser.on('continue', guesser)

  // Run both agents concurrently
  await Promise.all([hinterFlow.run(shared), guesserFlow.run(shared)])
}

// Mock LLM call for TypeScript
async function callLLM(prompt: string): Promise<string> {
  // In a real implementation, this would call an actual LLM API
  return 'Mock LLM response'
}

main().catch(console.error)
```

{% endtab %}
{% endtabs %}

The Output:

```
Game starting!
Target word: nostalgia
Forbidden words: ['memory', 'past', 'remember', 'feeling', 'longing']

Hinter: Here's your hint - Thinking of childhood summer days
Guesser: I guess it's - popsicle

Hinter: Here's your hint - When childhood cartoons make you emotional
Guesser: I guess it's - nostalgic

Hinter: Here's your hint - When old songs move you
Guesser: I guess it's - memories

Hinter: Here's your hint - That warm emotion about childhood
Guesser: I guess it's - nostalgia
Game Over - Correct guess!
```




================================================
File: docs/utility_function/index.md
================================================
---
machine-display: true
---

# Utility Functions

BrainyFlow does not provide built-in utilities. Instead, we offer examples that you can implement yourself. This approach gives you more flexibility and control over your project's dependencies and functionality.

## Available Utility Function Examples

1. [LLM Wrapper](./llm.md): Interact with Language Models
2. [Visualization and Debugging](./viz.md): Tools for visualizing and debugging flows
3. [Web Search](./websearch.md): Perform web searches
4. [Chunking](./chunking.md): Split large texts into manageable chunks
5. [Embedding](./embedding.md): Generate vector embeddings for text
6. [Vector Databases](./vector.md): Store and query vector embeddings
7. [Text-to-Speech](./text_to_speech.md): Convert text to speech

## Why Not Built-in?

We believe it's a bad practice to include vendor-specific APIs in a general framework for several reasons:

1. **API Volatility**: Frequent changes in external APIs lead to heavy maintenance for hardcoded APIs.
2. **Flexibility**: You may want to switch vendors, use fine-tuned models, or run them locally.
3. **Optimizations**: Prompt caching, batching, and streaming are easier to implement without vendor lock-in.

## Implementing Utility Functions

When implementing utility functions for your BrainyFlow project:

1. Create a separate file for each utility function in the `utils/` directory.
2. Include a simple test or example usage in each file.
3. Document the input/output and purpose of each utility function.

Example structure:

{% tabs %}
{% tab title="Python" %}

```
my_project/
├── utils/
│   ├── __init__.py
│   ├── call_llm.py
│   ├── search_web.py
│   └── embed_text.py
└── ...
```

{% endtab %}

{% tab title="TypeScript" %}

```
my_project/
├── utils/
│   ├── callLlm.ts
│   ├── searchWeb.ts
│   └── embedText.ts
└── ...
```

{% endtab %}
{% endtabs %}

By following this approach, you can easily maintain and update your utility functions as needed, without being constrained by the framework's built-in utilities.




================================================
File: docs/guides/agentic_coding.md
================================================
---
title: 'Agentic Coding'
---

# Agentic Coding: Humans Design, Agents code!

{% hint style="warning" %}
If you are an AI agents involved in building LLM Systems, read this guide **VERY, VERY** carefully! This is the most important chapter in the entire document. Throughout development, you should always (1) start with a small and simple solution, (2) design at a high level (`docs/design.md`) before implementation, and (3) frequently ask humans for feedback and clarification.
{% endhint %}

## Introduction

Agentic Coding represents a paradigm shift in software development where humans focus on high-level design and strategic decisions while AI agents handle implementation details. This approach leverages the complementary strengths of both humans and AI to create more efficient, robust, and maintainable LLM applications.

## The Agentic Coding Workflow

The development process follows seven key stages, with varying levels of human and AI involvement:

| Stage             | Human Involvement | AI Involvement | Primary Responsibility                                                                        |
| ----------------- | :---------------: | :------------: | --------------------------------------------------------------------------------------------- |
| 1. Requirements   |     ★★★ High      |    ★☆☆ Low     | Human defines problem scope and success criteria                                              |
| 2. Flow Design    |    ★★☆ Medium     |   ★★☆ Medium   | Human outlines high-level architecture; AI refines details                                    |
| 3. Utilities      |    ★★☆ Medium     |   ★★☆ Medium   | Humans provide available external APIs and integrations, and the AI helps with implementation |
| 4. Node Design    |      ★☆☆ Low      |    ★★★ High    | AI helps design the node types and data handling based on the flow.                           |
| 5. Implementation |      ★☆☆ Low      |    ★★★ High    | AI writes code based on the established design                                                |
| 6. Optimization   |    ★★☆ Medium     |   ★★☆ Medium   | Human evaluates results; AI suggests improvements                                             |
| 7. Reliability    |      ★☆☆ Low      |    ★★★ High    | AI implements error handling and addresses corner cases.                                      |

## Detailed Stage Breakdown

### 1. Requirements Analysis

Before building an LLM application, thoroughly evaluate whether AI is the right solution for your problem:

**Ideal Use Cases:**

- Content generation and transformation (summaries, translations, rewrites)
- Information extraction and classification
- Conversational interfaces with well-defined domains
- Decision support with clear parameters

**Less Suitable Use Cases:**

- Problems requiring perfect accuracy or deterministic outputs
- Tasks needing specialized domain expertise beyond LLM training
- Complex decision-making with high-stakes consequences
- Problems with ambiguous success criteria

**Best Practices:**

- **User-Centric Approach:** Define problems from the user's perspective rather than listing features
- **Concrete Examples:** Develop several example inputs and expected outputs
- **Manual Walkthrough:** Solve examples by hand to understand the process before automation
- **Value vs. Complexity:** Prioritize high-value features with manageable complexity

### 2. Flow Design

{% hint style="warning" %}
**If Humans can't specify the flow, AI Agents can't automate it!** Before building an LLM system, thoroughly understand the problem and potential solution by manually solving example inputs to develop intuition.  
{% endhint %}

The flow design stage establishes the application's architecture and processing pipeline:

**Key Activities:**

- Identify applicable [design patterns](./design_pattern/index.md) (Agent, RAG, MapReduce, etc.)
- Map out information flow between components
  - For each node in the flow, start with a high-level one-line description of what it does.
- Define decision points and branching logic
  - If using **Map Reduce**, specify how to map (what to split) and how to reduce (how to combine).
  - If using **Agent**, specify what are the inputs (context) and what are the possible actions.
  - If using **RAG**, specify what to embed, noting that there's usually both offline (indexing) and online (retrieval) workflows.
- Document expected inputs and outputs for each stage

**Flow Visualization:**
Create a mermaid diagram to visualize your application flow:

```mermaid
flowchart LR
    start[Start] --> batch[Batch]
    batch --> check[Check]
    check -->|OK| process
    check -->|Error| fix[Fix]
    fix --> check

    subgraph process[Process]
      step1[Step 1] --> step2[Step 2]
    end

    process --> endNode[End]
```

**Design Principles:**

- **Modularity:** Break complex tasks into discrete, manageable components
- **Single Responsibility:** Each node should perform one clear function
- **Explicit Data Flow:** Make data dependencies between nodes transparent
- **Appropriate Granularity:** Balance between too coarse (complex) and too granular (fragmented)

### 3. Utility Functions

{% hint style="success" %}
**Sometimes, design Utilies before Flow:** For example, for an LLM project to automate a legacy system, the bottleneck will likely be the available interface to that system. Start by designing the hardest utilities for interfacing, and then build the flow around them.
{% endhint %}

Utility functions serve as the interface between your LLM application and external systems:

<div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/utility.png?raw=true" width="400"/></div>

**Categories of Utilities:**

- **Input Processing:** File reading, API data fetching, database queries
- **External Tools:** Web search, code execution, specialized APIs
- **Output Handling:** Data storage, notification systems, report generation
- **LLM Interaction:** Model calling, embedding generation, prompt management
  - **NOTE**: _LLM-based tasks_ (e.g., summarizing text, analyzing sentiment) are **NOT** utility functions; rather, they are _core functions_ internal in the AI system.

**Implementation Guidelines:**

- Create isolated, testable functions with clear interfaces
- Document input/output specifications and error handling
- Include usage examples and test cases
- Implement proper error handling and retry logic
- Document their input/output, as well as why they are necessary. For example:

```yaml
name: get_embedding (utils/get_embedding.py)
input: string
output: a vector of 3072 floats
necessity: Used by the second node to embed text
```

**Example Utility Implementation:**

{% tabs %}
{% tab title="Python" %}

```python
# utils/call_llm.py
from openai import OpenAI

def call_llm(prompt):
    client = OpenAI(api_key="YOUR_API_KEY_HERE")
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return r.choices[0].message.content

if __name__ == "__main__":
	# Simple test case
    prompt = "What is the meaning of life?"
    print(call_llm(prompt))
```

{% endtab %}

{% tab title="TypeScript" %}

```typescript
// utils/callLLM.ts
import OpenAI from 'openai'

export async function callLLM(prompt: string): Promise<string> {
  const openai = new OpenAI({
    apiKey: 'YOUR_API_KEY_HERE',
  })

  const response = await openai.chat.completions.create({
    model: 'gpt-4o',
    messages: [{ role: 'user', content: prompt }],
  })

  return response.choices[0]?.message?.content || ''
}

// Simple test case
;(async () => {
  const prompt = 'What is the meaning of life?'
  console.log(await callLLM(prompt))
})()
```

{% endtab %}
{% endtabs %}

### 4. Node Design

Nodes are the processing units within your application flow, each with a specific responsibility:

**Node Design Principles:**

- **Data Isolation:** Use the shared store for communication between nodes
- **Clear Lifecycle:** Follow the `prep -> exec -> post` pattern
- **Idempotent Operations:** Design for safe retries when possible
- **Graceful Degradation:** Implement fallbacks for failure scenarios

**Shared Store Design:**
Start by designing your shared store schema to facilitate data flow between nodes:

```python
# Example shared store structure
shared = {
    "input": {
        "query": "How do neural networks learn?",
        "context": { # Another nested dict
            "persona": "The user is a computer science student.",
            "location": "San Francisco"
        }
    },
    "processing": {
        "search_results": [],
        "relevant_chunks": []
    },
    "output": {
        "response": None,
        "confidence": None
    },
    "metadata": {
        "start_time": "2025-04-06T18:38:00Z",
        "processing_steps": []
    }
}
```

**Node Specification:**
For each node, define how it reads and writes data, and which utility function it uses. Keep it specific but high-level without codes. For example:

- `prep`: Input requirements (what it reads from shared store)
- `exec`: Processing logic (what computation it performs)
- `post`: Output format (what it writes to shared store)
- `exec_fallback`: Error handling strategy (how it deals with failures)
- For batch processing, specify if it's a sequential or parallel node

### 5. Implementation

{% hint style="success" %}
🎉 If you've reached this step, humans have finished the design. Now _Agentic Coding_ begins!
{% endhint %}

The implementation stage transforms the design into working code:

**Implementation Guidelines:**

- **Start Simple:** Begin with a minimal viable implementation
- **Incremental Development:** Add features one at a time with testing
- **Fail Fast:** Avoid error suppression as much as you can to quickly identify issues
- **Comprehensive Logging:** Add detailed logging throughout the code for debugging

**Example Node Implementation:**

```python
class RetrieveRelevantDocuments(Node):
    """Node that retrieves relevant documents based on a query."""

    async def prep(self, shared):
        """Extract query and vector database from shared store."""
        query = shared["input"]["query"]
        vector_db = shared["resources"]["vector_db"]
        return query, vector_db

    async def exec(self, inputs):
        """Retrieve relevant documents using vector similarity."""
        query, vector_db = inputs

        # Get query embedding
        query_embedding = await get_embedding(query)

        # Search vector database
        results = await vector_db.search(
            query_embedding,
            limit=5,
            min_score=0.7
        )

        return results

    async def post(self, shared, prep_res, exec_res):
        """Store retrieved documents in shared store."""
        shared["processing"]["relevant_documents"] = exec_res
        shared["metadata"]["processing_steps"].append({
            "step": "document_retrieval",
            "timestamp": datetime.now().isoformat(),
            "document_count": len(exec_res)
        })

        # Determine next action based on results
        if not exec_res:
            return "fallback_search"
        return "generate_response"
```

### 6. Optimization

{% hint style="success" %}
**You'll likely iterate a lot!** Expect to repeat Steps 3–6 countless times.

<div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/success.png?raw=true" width="400"/></div>
{% endhint %}

- **Use Intuition**: For a quick initial evaluation, human intuition is often a good start.
- **Redesign Flow (Back to Step 3)**: Consider breaking down tasks further, introducing agentic decisions, or better managing input contexts.
- **Context Management:** Optimize the information provided to each node
- If your flow design is already solid, move on to micro-optimizations:
- **Prompt Engineering**: Use clear, specific instructions with examples to reduce ambiguity.
- **In-Context Learning**: Provide robust examples for tasks that are difficult to specify with instructions alone.

7. **Reliability**
   - **Node Retries**: Add checks in the node `exec` to ensure outputs meet requirements, and consider increasing `max_retries` and `wait` times.
   - **Logging and Visualization**: Maintain logs of all attempts and visualize node results for easier debugging.
   - **Self-Evaluation**: Add a separate node and have the LLM evaluate and improve its own outputs

## Iterative Development Process

Agentic Coding is inherently iterative. Expect to cycle through these stages multiple times:

1. **Start Small:** Begin with a minimal implementation of the core functionality
2. **Test and Evaluate:** Gather feedback on performance and output quality
3. **Identify Improvements:** Determine which aspects need enhancement
4. **Refine Design:** Update flow, nodes, or utilities as needed
5. **Implement Changes:** Make targeted improvements
6. **Repeat:** Continue the cycle until quality targets are met

## Conclusion

Agentic Coding represents a powerful approach to developing LLM applications by combining human design expertise with AI implementation capabilities. By following this structured process, you can create more effective, maintainable, and reliable AI systems while focusing your human effort where it adds the most value.

Remember that the most successful projects start simple, iterate quickly, and continuously incorporate feedback. Let humans handle the "why" and "what" while AI agents handle the "how" of implementation.

Human-AI collaboration is a journey of continuous learning and improvement. Embrace the iterative nature of the process, and you'll build increasingly sophisticated applications that deliver real value to users.




================================================
File: docs/guides/best_practices.md
================================================
# BrainyFlow Best Practices

## Node Design

1. **Keep Nodes Focused**: Each node should do one thing well
2. **Idempotent Execution**: Design `exec()` methods to be safely retryable
3. **Graceful Degradation**: Implement fallbacks for when things go wrong
4. **Proper Error Handling**: Use `exec_fallback` to handle failures gracefully

## Shared Store Management

1. **Schema Design**: Define a clear schema for your shared store
2. **Namespacing**: Use namespaces to avoid key collisions
3. **Immutability**: Treat shared store values as immutable when possible
4. **Documentation**: Document the expected structure of your shared store

## Flow Design

1. **Visualization First**: Design your flow visually before coding
2. **Test Incrementally**: Build and test one section of your flow at a time
3. **Error Paths**: Always include paths for handling errors
4. **Monitoring**: Add logging at key points in your flow

## Project Structure

A well-organized project structure enhances maintainability and collaboration:

{% tabs %}
{% tab title="Python (simple)" %}

```
my_simple_project/
├── main.py
├── nodes.py
├── flow.py
├── utils/
│   ├── __init__.py
│   ├── call_llm.py
│   └── search_web.py
├── requirements.txt
└── docs/
    └── design.md
```

{% endtab %}

{% tab title="Python (complex)" %}

```
my_complex_project/
├── main.py                # Entry point
├── nodes/                 # Node implementations
│   ├── __init__.py
│   ├── input_nodes.py
│   ├── processing_nodes.py
│   └── output_nodes.py
├── flows/                 # Flow definitions
│   ├── __init__.py
│   └── main_flow.py
├── utils/                 # Utility functions
│   ├── __init__.py
│   ├── llm.py
│   ├── database.py
│   └── web_search.py
├── tests/                 # Test cases
│   ├── test_nodes.py
│   └── test_flows.py
├── config/                # Configuration
│   └── settings.py
├── requirements.txt       # Dependencies
└── docs/                  # Documentation
    ├── design.md          # High-level design
    └── api.md             # API documentation
```

{% endtab %}

{% tab title="TypeScript (simple)" %}

```
my_project/
├── src/
│   ├── main.ts
│   ├── nodes.ts
│   ├── flow.ts
│   └── utils/
│       ├── callLLM.ts
│       └── searchWeb.ts
├── package.json
└── docs/
    └── design.md
```

{% endtab %}

{% tab title="TypeScript (complex)" %}

```
my_complex_project/
├── src/                      # Source code
│   ├── index.ts              # Entry point
│   ├── nodes/                # Node implementations
│   │   ├── index.ts          # Exports all nodes
│   │   ├── inputNodes.ts
│   │   ├── processingNodes.ts
│   │   └── outputNodes.ts
│   ├── flows/                # Flow definitions
│   │   ├── index.ts          # Exports all flows
│   │   └── mainFlow.ts
│   ├── utils/                # Utility functions
│   │   ├── index.ts          # Exports all utilities
│   │   ├── llm.ts
│   │   ├── database.ts
│   │   └── webSearch.ts
│   ├── types/                # Type definitions
│   │   ├── index.ts          # Exports all types
│   │   ├── node.types.ts
│   │   └── flow.types.ts
│   └── config/               # Configuration
│       └── settings.ts
├── dist/                     # Compiled JavaScript
├── tests/                    # Test cases
│   ├── nodes.test.ts
│   └── flows.test.ts
├── package.json              # Dependencies and scripts
└── docs/                     # Documentation
    ├── design.md             # High-level design
    └── api.md                # API documentation
```

{% endtab %}
{% endtabs %}

- **`docs/design.md`**: Contains project documentation for each step designed in [agentic coding](./agentic_coding.md). This should be _high-level_ and _no-code_.
- **`utils/`**: Contains all utility functions.
  - It's recommended to dedicate one file to each API call, for example `call_llm.py` or `search_web.ts`.
  - Each file should also include a `main()` function to try that API call
- **`nodes.py`** or **`nodes.ts`**: Contains all the node definitions.
- **`flow.py`** or **`flow.ts`**: Implements functions that create flows by importing node definitions and connecting them.
- **`main.py`** or **`main.ts`**: Serves as the project's entry point.


