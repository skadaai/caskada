

================================================
File: docs/getting_started.md
================================================
# Getting Started with BrainyFlow

Welcome to BrainyFlow! This framework helps you build powerful, modular AI applications using a simple yet expressive abstraction based on nested directed graphs.

## 1. Installation

First, ensure you have BrainyFlow installed:

```bash
npm install brainyflow
```

For detailed installation options, see the [Installation Guide](./installation.md).

## 2. Core Concepts

BrainyFlow is built around a minimalist yet powerful abstraction that separates data flow from computation:

- **[Node](./core_abstraction/node.md)**: The fundamental building block that performs a single task with a clear lifecycle (`prep → exec → post`).
- **[Flow](./core_abstraction/flow.md)**: Orchestrates nodes in a directed graph, supporting branching, looping, and nesting.
- **[Shared Store](./core_abstraction/communication.md)**: A global data structure that enables communication between nodes.
- **[Batch](./core_abstraction/batch.md)**: Processes multiple items either sequentially or in parallel.

## 3. Your First Flow

Let's build a simple Question-Answering flow to demonstrate BrainyFlow's core concepts:

### Step 1: Design Your Flow

Our flow will have two nodes:

1. `GetQuestionNode`: Captures the user's question
2. `AnswerNode`: Generates an answer using an LLM

```mermaid
graph LR
    A[GetQuestionNode] --> B[AnswerNode]
```

### Step 2: Implement the Nodes

```typescript
import { Node } from 'brainyflow'
import { input } from '@inquirer/prompts'
import { callLLM } from './utils/callLLM' // Your LLM implementation

class GetQuestionNode extends Node {
  async prep(shared: Record): Promise {
    shared.question = await input({ message: 'Enter your question: ' })
  }
}

class AnswerNode extends Node {
  async prep(shared: Record): Promise {
    return shared.question
  }

  async exec(question: string): Promise {
    return await callLLM(question)
  }

  async post(shared: Record, prepRes: string, execRes: string): Promise {
    shared.answer = execRes
  }
}
```

{% hint style="info" %}

**Review:** What was achieved here?

- `GetQuestionNode` writes the user's question to the `shared` store.
- `AnswerNode` reads the question from the `shared` store, calling an LLM utility, and writing the answer back to the `shared` store.

{% endhint %}

### Step 3: Connect the Nodes into a Flow

```typescript
import { Flow } from 'brainyflow'

function createQaFlow(): Flow {
  const getQuestionNode = new GetQuestionNode()
  const answerNode = new AnswerNode()

  // Connect nodes: getQuestionNode → answerNode
  getQuestionNode.next(answerNode)

  return new Flow(getQuestionNode)
}
```

{% hint style="info" %}

**Review:** What was achieved here?

- `qaFlow` has connected the nodes, letting the user's question propagate from `GetQuestionNode` to `AnswerNode` to generate an answer.

{% endhint %}

### Step 4: Run the Flow

```typescript
async function main() {
  const shared: Record = {} // Initialize empty shared store
  const qaFlow = createQaFlow()
  await qaFlow.run(shared)

  console.log(`Question: ${shared.question}`)
  console.log(`Answer: ${shared.answer}`)
}

main().catch(console.error)
```

## 4. Key Design Principles

BrainyFlow follows these core design principles:

1. **Separation of Concerns**: Data storage (shared store) is separate from computation logic (nodes)
2. **Explicit Data Flow**: Data dependencies between steps are clear and traceable
3. **Composability**: Complex systems are built from simple, reusable components
4. **Minimalism**: The framework provides only essential abstractions, avoiding vendor-specific implementations

## 5. Next Steps

Now that you understand the basics, explore these resources to build sophisticated applications:

- [Core Abstractions](./core_abstraction/index.md): Dive deeper into nodes, flows, and communication
- [Design Patterns](./design_pattern/index.md): Learn more complex patterns like Agents, RAG, and MapReduce
- [Agentic Coding Guide](./guides/agentic_coding.md): Best practices for human-AI collaborative development
- [Best Practices](./guides/best_practices.md): Tips for building robust, maintainable applications




================================================
File: docs/core_abstraction/index.md
================================================
# Understanding BrainyFlow's Core Abstractions

BrainyFlow is built around a simple yet powerful abstraction: the **nested directed graph with shared store**. This mental model separates _data flow_ from _computation_, making complex LLM applications more maintainable and easier to reason about.

## Core Philosophy

BrainyFlow follows these fundamental principles:

1. **Modularity & Composability**: Build complex systems from simple, reusable components that are easy to build, test, and maintain
2. **Explicitness**: Make data dependencies between steps clear and traceable
3. **Separation of Concerns**: Data storage (shared store) remains separate from computation logic (nodes)
4. **Minimalism**: The framework provides only essential abstractions, avoiding vendor-specific implementations while supporting various high-level AI design paradigms (agents, workflows, map-reduce, etc.)
5. **Resilience**: Handle failures gracefully with retries and fallbacks

## The Graph + Shared Store Pattern

The fundamental pattern in BrainyFlow combines two key elements:

- **Computation Graph**: A directed graph where nodes represent discrete units of work and edges represent the flow of control
- **Shared Store**: A global data structure that enables communication between nodes

This pattern offers several advantages:

- **Clear visualization** of application logic
- **Easy identification** of bottlenecks
- **Simple debugging** of individual components
- **Natural parallelization** opportunities

## Key Components

BrainyFlow's architecture is based on these fundamental building blocks:

| Component                           | Description             | Key Features                                                                |
| ----------------------------------- | ----------------------- | --------------------------------------------------------------------------- |
| [Node](./node.md)                   | The basic unit of work  | Clear lifecycle (`prep → exec → post`), fault tolerance, graceful fallbacks |
| [Flow](./flow.md)                   | Connects nodes together | Action-based transitions, branching, looping, nesting                       |
| [Communication](./communication.md) | Enables data sharing    | Shared Store (global), Params (node-specific)                               |
| [Batch](./batch.md)                 | Handles multiple items  | Sequential or parallel processing, nested batching                          |
| [Throttling](./throttling.md)       | Manages concurrency     | Rate limiting, concurrency control                                          |

## How They Work Together

1. **Nodes** perform individual tasks with a clear lifecycle:

   - `prep`: Read from shared store and prepare data
   - `exec`: Execute computation (often LLM calls)
   - `post`: Process results and write to shared store

2. **Flows** orchestrate nodes by:

   - Starting with a designated node
   - Following action-based transitions between nodes
   - Supporting branching, looping, and nested flows

3. **Communication** happens through:

   - **Shared Store**: A global dictionary accessible to all nodes
   - **Params**: Node-specific configuration passed down from parent flows

4. **Batch Processing** enables:
   - Processing multiple items sequentially or in parallel
   - Handling large datasets efficiently
   - Supporting nested batch operations

## Getting Started

If you're new to BrainyFlow, we recommend exploring these core abstractions in the following order:

1. [Node](./node.md) - Understand the basic building block
2. [Flow](./flow.md) - Learn how to connect nodes together
3. [Communication](./communication.md) - See how nodes share data
4. [Batch](./batch.md) - Explore handling multiple items
5. [Throttling](./throttling.md) - Learn about managing concurrency

Once you understand these core abstractions, you'll be ready to implement various [Design Patterns](../design_pattern/index.md) to solve real-world problems.




================================================
File: docs/core_abstraction/node.md
================================================
# Node: The Fundamental Building Block

A **Node** is the smallest reusable unit in BrainyFlow. Each Node follows a 3-step lifecycle that enforces the principle of separation of concerns.

## Node Lifecycle

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/node.png?raw=true" width="400"/>
</div>

### 1. `async prep(shared)`

**Purpose**: Read and preprocess data from the shared store.

- Extracts necessary data from the `shared` store
- Performs any required preprocessing or validation
- Returns `prep_res`, which becomes input for `exec()` and `post()`

### 2. `async exec(prep_res)`

**Purpose**: Execute the core computation logic.

- Performs the main computation (often an LLM call or API request)
- ⚠️ Must **NOT** access the `shared` store directly
- ⚠️ Should be designed for idempotence when retries are enabled
- Returns `exec_res`, which is passed to `post()`

### 3. `async post(shared, prep_res, exec_res)`

**Purpose**: Process results and update the shared store.

- Writes computation results back to the `shared` store
- Has access to both the original input (`prep_res`) and result (`exec_res`)
- Returns an action string that determines the next node in the flow
  - If no value is returned, defaults to `"default"`

{% hint style="info" %}
**Why 3 steps?** This design enforces separation of concerns:

- `prep`: Data access and preparation
- `exec`: Pure computation (no side effects)
- `post`: Result processing and state updates

All steps are **optional**. For example, you can implement only `prep` and `post` if you just need to process data without external computation.
{% endhint %}

```mermaid
sequenceDiagram
    participant S as Shared Store
    participant N as Node

    N->>S: 1. prep(): Read from shared store
    Note right of N: Return prep_res

    N->>N: 2. exec(prep_res): Compute result
    Note right of N: Return exec_res

    N->>S: 3. post(shared, prep_res, exec_res): Write to shared store
    Note right of N: Return action string
```

## Fault Tolerance & Retries

Nodes support automatic retries for handling transient failures in `exec()` calls:

```typescript
const myNode = new MyNode({ maxRetries: 3, wait: 10 }) // Retry up to 3 times with 10s delay
```

Key retry parameters:

- `max_retries` (int): Maximum number of execution attempts (default: 1, meaning no retry)
- `wait` (int): Seconds to wait between retries (default: 0)

`wait` is specially helpful when you encounter rate-limits or quota errors from your LLM provider and need to back off.
During retries, you can access the current retry count (0-based) via `self.cur_retry` (Python) or `this.curRetry` (TypeScript).

```typescript
class RetryNode extends Node {
  async exec(prepRes: any): Promise {
    console.log(`Retried ${this.curRetry} times`)
    throw new Error('Failed')
  }
}
```

## Graceful Fallbacks

To handle failures gracefully after all retries are exhausted, override the `exec_fallback` method:

```typescript
async execFallback(prepRes: any, exc: Error): Promise {
  throw exc;  // Default behavior is to re-raise
}
```

By default, this method just re-raises the exception. You can override it to return a fallback result instead, which becomes the `exec_res` passed to `post()`.

## Example: Summarize File

```typescript
class SummarizeFile extends Node {
  async prep(shared: any): Promise<any> {
    return shared['data']
  }

  async exec(prepRes: any): Promise<string> {
    if (!prepRes) {
      return 'Empty file content'
    }
    const prompt = `Summarize this text in 10 words: ${prepRes}`
    const summary = await callLLM(prompt) // might fail
    return summary
  }

  async execFallback(prepRes: any, exc: Error): Promise<string> {
    // Provide a simple fallback instead of crashing
    return 'There was an error processing your request.'
  }

  async post(shared: any, prepRes: any, execRes: any): Promise<string> {
    shared['summary'] = execRes
    // Return "default" by not returning
  }
}

const summarizeNode = new SummarizeFile({ maxRetries: 3 })

// node.run() calls prep->exec->post
// If exec() fails, it retries up to 3 times before calling execFallback()
const actionResult = await summarizeNode.run(shared)

console.log('Action returned:', actionResult) // "default"
console.log('Summary stored:', shared['summary'])
```

## Example: Document Retrieval

```typescript
// Node that retrieves relevant documents based on a query
class RetrieveRelevantDocuments extends Node {
  async prep(shared: any): Promise<any> {
    // Extract query and vector database from shared store
    const query = shared['input']['query']
    const vectorDb = shared['resources']['vector_db']
    return [query, vectorDb]
  }

  async exec(inputs: any): Promise<any> {
    // Retrieve relevant documents using vector similarity
    const [query, vectorDb] = inputs

    // Get query embedding
    const queryEmbedding = await getEmbedding(query)

    // Search vector database
    const results = await vectorDb.search(
      queryEmbedding,
      limit: 5,
      minScore: 0.7
    )

    return results
  }

  async post(shared: any, prepRes: any, execRes: any): Promise<string> {
    // Store retrieved documents in shared store
    shared['processing']['relevant_documents'] = execRes
    shared['metadata']['processing_steps'].push({
      'step': 'document_retrieval',
      'timestamp': new Date().toISOString(),
      'document_count': execRes.length
    })

    // Determine next action based on results
    if (!execRes) {
      return 'fallback_search'
    }
    return 'generate_response'
  }
}
```

## Running Individual Nodes

Nodes have an extra method `run(shared)`, which calls `prep->exec->post` and returns the action.

{% hint style="warning" %}
`Node.run` **does not** proceed to a successor!

This method is useful for debugging or testing a single node, but not for running a [flow](./flow.md)!

Always use `Flow.run` instead to ensure the full pipeline runs correctly.
{% endhint %}

Compare it with `Flow.run`:

- `node.run(shared)`: Just runs that node alone (calls `prep->exec->post()`), returns an Action.
- `flow.run(shared)`: Executes from the start node, follows Actions to the next node, and so on until the flow can't continue.




================================================
File: docs/core_abstraction/flow.md
================================================
# Flow: Orchestrating Nodes in a Directed Graph

A **Flow** orchestrates a graph of Nodes, connecting them through action-based transitions. Flows enable you to create complex application logic including sequences, branches, loops, and nested workflows.

## Action-based Transitions

Each Node's `post()` method returns an **Action** string that determines which node to execute next. If `post()` doesn't return anything, the default action `"default"` is used.

### Defining Transitions

1. **Basic default transition**: `node_a.next(node_b)`
   This means if `node_a.post()` returns `"default"`, go to `node_b`.

2. **Named action transition**: `node_a.on('action_name', node_b)` or `node_a.next(node_b, 'action_name')`
   This means if `node_a.post()` returns `"action_name"`, go to `node_b`.

Note that `node_a.next(node_b)` is equivalent to both `node_a.next(node_b, 'default')` and `node_a.on('default', node_b)`

```typescript
// Basic default transition
node_a.next(node_b) // If node_a returns "default", go to node_b

// Named action transition
node_a.on('success', node_b) // If node_a returns "success", go to node_b
node_a.on('error', node_c) // If node_a returns "error", go to node_c

// Alternative syntax
node_a.next(node_b, 'success') // Same as node_a.on('success', node_b)
```

## Creating a Flow

A Flow begins with a **start node** and follows the action-based transitions until it reaches a node with no matching transition for its returned action.

```typescript
import { Flow } from 'brainyflow'

// Define nodes and transitions
node_a.next(node_b)
node_b.on('success', node_c)
node_b.on('error', node_d)

// Create flow starting with node_a
const flow = new Flow(node_a)

// Run the flow with a shared store
await flow.run(shared)
```

## Flow Execution Process

When you call `flow.run(shared)`:

1. The flow executes the start node
2. It examines the action returned by the node's `post()` method
3. It follows the corresponding transition to the next node
4. This process repeats until it reaches a node with no matching transition for its action

```mermaid
sequenceDiagram
    participant S as Shared Store
    participant F as Flow
    participant N1 as Node A
    participant N2 as Node B

    F->>N1: Execute Node A
    N1->>S: Read from shared store
    N1->>N1: Perform computation
    N1->>S: Write to shared store
    N1-->>F: Return action "default"

    F->>F: Find next node for action "default"
    F->>N2: Execute Node B
    N2->>S: Read from shared store
    N2->>N2: Perform computation
    N2->>S: Write to shared store
    N2-->>F: Return action "success"

    F->>F: No transition defined for "success"
    F-->>F: Flow execution complete
```

## Branching and Looping

Flows support complex patterns like branching (conditionally following different paths) and looping (returning to previous nodes).

### Example: Expense Approval Flow

Here's a simple expense approval flow that demonstrates branching and looping:

```typescript
// Define the nodes first
// const review = new ReviewExpenseNode()
// const revise = new ReviseReportNode()
// const payment = new ProcessPaymentNode()
// const finish = new FinishProcessNode()
// ..

// Define the flow connections
review.on('approved', payment) // If approved, process payment
review.on('needs_revision', revise) // If needs changes, go to revision
review.on('rejected', finish) // If rejected, finish the process

revise.next(review) // After revision, go back for another review
payment.next(finish) // After payment, finish the process

// Create the flow
const expenseFlow = new Flow(review)
```

This flow creates the following execution paths:

1. If `review.post()` returns `"approved"`, the expense moves to the `payment` node
2. If `review.post()` returns `"needs_revision"`, it goes to the `revise` node, which then loops back to `review`
3. If `review.post()` returns `"rejected"`, it moves to the `finish` node and stops

```mermaid
flowchart TD
    review[Review Expense] -->|approved| payment[Process Payment]
    review -->|needs_revision| revise[Revise Report]
    review -->|rejected| finish[Finish Process]

    revise --> review
    payment --> finish
```

## Nested Flows

A Flow can be used as a Node within another Flow, enabling powerful composition patterns. This allows you to:

1. Break down complex applications into manageable sub-flows
2. Reuse flows across different applications
3. Create hierarchical workflows with clear separation of concerns

### Flow as a Node

When a Flow is used as a Node:

- It inherits the Node lifecycle (`prep → exec → post`)
- Its `prep()` and `post()` methods can be overridden
- `post()` always receives `None` for `exec_res` and should instead get the flow execution results from the shared store.
- It won't allow for a custom `exec()` method since its main logic is to orchestrate its internal nodes
- When run, it executes its internal nodes according to their transitions

### Example: Order Processing Pipeline

Here's a practical example that breaks down order processing into nested flows:

```typescript
// Payment processing sub-flow
validatePayment.next(processPayment).next(paymentConfirmation)
const paymentFlow = new Flow(validatePayment)

// Inventory sub-flow
checkStock.next(reserveItems).next(updateInventory)
const inventoryFlow = new Flow(checkStock)

// Shipping sub-flow
createLabel.next(assignCarrier).next(schedulePickup)
const shippingFlow = new Flow(createLabel)

// Connect the flows into a main order pipeline
paymentFlow.next(inventoryFlow).next(shippingFlow)

// Create the master flow
const orderPipeline = new Flow(paymentFlow)

// Run the entire pipeline
await orderPipeline.run(sharedData)
```

This creates a clean separation of concerns while maintaining a clear execution path:

```mermaid
flowchart LR
    subgraph order_pipeline[Order Pipeline]
        subgraph paymentFlow["Payment Flow"]
            A[Validate Payment] --> B[Process Payment] --> C[Payment Confirmation]
        end

        subgraph inventoryFlow["Inventory Flow"]
            D[Check Stock] --> E[Reserve Items] --> F[Update Inventory]
        end

        subgraph shippingFlow["Shipping Flow"]
            G[Create Label] --> H[Assign Carrier] --> I[Schedule Pickup]
        end

        paymentFlow --> inventoryFlow
        inventoryFlow --> shippingFlow
    end
```

## Flow Parameters

When a Flow is used as a Node, you can pass parameters to it using `set_params()`. These parameters are then accessible within the Flow's nodes:

```typescript
// Create a flow
const processFlow = new Flow(someNode)

// Set parameters
processFlow.setParams({ mode: 'fast', maxItems: 10 })

// Use the flow in another flow
const mainFlow = new Flow(processFlow)

// Run the main flow
await mainFlow.run(shared)
```

## Best Practices

1. **Start Simple**: Begin with a linear flow and add branching/looping as needed
2. **Visualize First**: Sketch your flow diagram before coding
3. **Name Actions Clearly**: Use descriptive action names that indicate the decision made
4. **Test Incrementally**: Build and test one section of your flow at a time
5. **Document Transitions**: Add comments explaining the conditions for each transition
6. **Error Handling**: Always include paths for handling errors
7. **Avoid Deep Nesting**: Keep nesting to 2-3 levels for maintainability

By following these principles, you can create complex, maintainable AI applications that are easy to reason about and extend.




================================================
File: docs/core_abstraction/communication.md
================================================
# Communication

Nodes and Flows in BrainyFlow communicate through two primary mechanisms:

1. **Shared Store (recommended for most cases)**

   - A global data structure (typically an in-memory dictionary) that all nodes can read from (`prep()`) and write to (`post()`)
   - Ideal for sharing data results, large content, or information needed by multiple nodes
   - Follows the principle of separation of concerns by keeping data separate from computation logic

2. **Params (primarily for [Batch](./batch.md) operations)**
   - Node-specific configuration passed down from parent flows
   - Best for identifiers like filenames or IDs, especially in Batch processing
   - Parameter keys and values should be **immutable** during execution

If you know memory management, think of the **Shared Store** like a **heap** (shared by all function calls), and **Params** like a **stack** (assigned by the caller).

## Shared Store

The Shared Store is the primary communication mechanism in BrainyFlow, embodying the principle of separation between data storage and computation logic.

### Overview

A shared store is typically an in-memory dictionary that serves as a global data repository:

```typescript
const shared = {
  data: {...},
  results: {...},
  config: {...},
  // Any other data you need to share
}
```

The shared store can also contain file handlers, database connections, or other resources that need to be accessible across nodes.

### Best Practices

1. **Define a Clear Schema**: Plan your shared store structure before implementation
2. **Use Namespaces**: Group related data under descriptive keys
3. **Document Structure**: Comment on expected data types and formats
4. **Avoid Deep Nesting**: Keep the structure reasonably flat for readability

### Example Usage

```typescript
class LoadData extends Node {
  async post(shared: Record): Promise {
    // Write data to shared store
    shared.data = 'Some text content'
  }
}

class Summarize extends Node {
  async prep(shared: Record): Promise {
    // Read data from shared store
    return shared.data
  }

  async exec(prepRes: string): Promise {
    // Call LLM to summarize
    const prompt = `Summarize: ${prepRes}`
    const summary = await callLLM(prompt)
    return summary
  }

  async post(shared: Record, prepRes: string, execRes: string): Promise {
    // Write summary to shared store
    shared.summary = execRes
  }
}

const loadData = new LoadData()
const summarize = new Summarize()
loadData.next(summarize)
const flow = new Flow(loadData)

const shared = {}
await flow.run(shared)
```

## Params

While the Shared Store is the primary communication mechanism, Params provide a way to configure individual nodes with specific settings.

### Key Characteristics

- **Immutable**: Params don't change during a node's execution cycle
- **Hierarchical**: Params are passed down from parent flows to child nodes
- **Local**: Each node or flow has its own params that don't affect other nodes

### When to Use Params

- **Batch Processing**: To identify which item is being processed
- **Configuration**: For node-specific settings that don't need to be shared
- **Identification**: For tracking the source or purpose of a computation

### Example Usage

```typescript
// 1) Create a Node that uses params
class SummarizeFile extends Node {
  async prep(shared: Record): Promise {
    // Access the node's param
    const filename = this.params.filename
    return shared.data[filename] || ''
  }

  async exec(prepRes: string): Promise {
    const prompt = `Summarize: ${prepRes}`
    return await callLLM(prompt)
  }

  async post(shared: Record, prepRes: string, execRes: string): Promise {
    const filename = this.params.filename
    shared.summary[filename] = execRes
  }
}

// 2) Set params directly on a node (for testing)
const node = new SummarizeFile()
node.setParams({ filename: 'doc1.txt' })
await node.run(shared)

// 3) Set params on a flow (overrides node params)
const flow = new Flow(node)
flow.setParams({ filename: 'doc2.txt' })
await flow.run(shared) // The node summarizes doc2.txt, not doc1.txt
```

## Choosing Between Shared Store and Params

| Use Shared Store when...                    | Use Params when...                           |
| ------------------------------------------- | -------------------------------------------- |
| Data needs to be accessed by multiple nodes | Configuration is specific to a single node   |
| Information persists across the entire flow | Working with Batch processing                |
| Storing large amounts of data               | Passing identifiers or simple values         |
| Maintaining state throughout execution      | Configuring behavior without affecting state |

{% hint style="success" %}
**Best Practice**: Use Shared Store for almost all cases to maintain separation of concerns. Params are primarily useful for Batch processing and node-specific configuration.
{% endhint %}




================================================
File: docs/core_abstraction/batch.md
================================================
# Batch Processing

Batch processing in BrainyFlow enables efficient handling of multiple items, whether sequentially or in parallel. This is particularly useful for:

- Processing large datasets or lists (e.g., multiple files, database records)
- Applying the same operation to multiple inputs
- Dividing large tasks into manageable chunks

## Node-Level Batch Processing

BrainyFlow provides two specialized node types for batch processing:

### SequentialBatchNode

A `SequentialBatchNode` processes items one after another, which is useful when:

- Order of processing matters
- Operations have dependencies between items
- You need to conserve resources or manage rate limits

It extends `Node`, with changes to:

- **`async prep(shared)`**: returns an **iterable** (e.g., list, generator).
- **`async exec(item)`**: called **once** per item in that iterable.
- **`async post(shared, prep_res, exec_res_list)`**: after all items are processed, receives a **list** of results (`exec_res_list`) and returns an **Action**.

#### Example: Sequential Summarize File

```typescript
class SequentialSummaries extends SequentialBatchNode {
  async prep(shared: Record): Promise {
    const content = shared.data
    const chunkSize = 10000
    const chunks: string[] = []

    // Suppose we have a big file; chunk it!
    for (let i = 0; i < content.length; i += chunkSize) {
      chunks.push(content.slice(i, i + chunkSize))
    }
    return chunks
  }

  async exec(chunk: string): Promise<string> {
    const prompt = `Summarize this chunk in 10 words: ${chunk}`
    return await callLLM(prompt)
  }

  async post(shared: Record, prepRes: string[], execResList: string[]): Promise {
    shared.summary = execResList.join('\n')
  }
}
```

### ParallelBatchNode

A `ParallelBatchNode` processes items concurrently, which is useful when:

- Operations are independent of each other
- You want to maximize throughput
- Tasks are primarily I/O-bound (like API calls)

{% hint style="warning" %}
**Concurrency Considerations**:

- Ensure operations are truly independent before using parallel processing
- Be mindful of rate limits when making API calls
- Consider using [Throttling](./throttling.md) to control concurrency

{% endhint %}

It extends `Node`, with changes to:

- **`async prep(shared)`**: returns an **iterable** (e.g., list, generator).
- **`async exec(item)`**: called **concurrently** for each item.
- **`async post(shared, prep_res, exec_res_list)`**: after all items are processed, receives a **list** of results (`exec_res_list`) and returns an **Action**.

#### Example: Parallel Summarize of a Large File

```typescript
class ParallelSummaries extends ParallelBatchNode {
  async prep(shared: Record): Promise {
    const content = shared.data
    const chunkSize = 10000
    const chunks: string[] = []

    // Suppose we have a big file; chunk it!
    for (let i = 0; i < content.length; i += chunkSize) {
      chunks.push(content.slice(i, i + chunkSize))
    }
    return chunks
  }

  async exec(chunk: string): Promise<string> {
    const prompt = `Summarize this chunk in 10 words: ${chunk}`
    return await callLLM(prompt)
  }

  async post(shared: Record, prepRes: string[], execResList: string[]): Promise {
    shared.summary = execResList.join('\n')
  }
}
```

## Flow-Level Batch Processing

BrainyFlow also supports batch processing at the flow level, allowing you to run an entire flow multiple times with different parameters:

### SequentialBatchFlow

A `SequentialBatchFlow` runs a flow multiple times in sequence, with different `params` each time. Think of it as a loop that replays the Flow for each parameter set.


{% hint style="info" %}
**When to use**: Choose sequential processing when order matters or when working with APIs that have strict rate limits. See [Throttling](./throttling.md) for managing rate limits.
{% endhint %}

#### Example: Summarize Many Files

```typescript
class SummarizeAllFiles extends SequentialBatchFlow {
  async prep(shared: Record): Promise<Array<Record<string, string>>> {
    const filenames = Object.keys(shared.data) // e.g., ["file1.txt", "file2.txt", ...]
    return filenames.map(fn => ({ filename: fn }))
  }
}

const summarizeFile = new Flow(loadFile)

// Create a batch flow that processes all files sequentially
const summarizeAllFiles = new SummarizeAllFiles(summarizeFile)
await summarizeAllFiles.run(shared)
```

#### Under the Hood

1. `prep(shared)` returns a list of param dicts—e.g., `[{filename: "file1.txt"}, {filename: "file2.txt"}, ...]`.
2. The **BatchFlow** loops through each dict. For each one:
   - It merges the dict with the BatchFlow’s own `params`.
   - It calls `flow.run(shared)` using the merged result.
3. This means the sub-Flow is run **repeatedly**, once for every param dict.


### ParallelBatchFlow

A `ParallelBatchFlow` runs a flow multiple times concurrently, with different `params` each time:

```typescript
class SummarizeAllFiles extends ParallelBatchFlow {
  async prep(shared: Record): Promise>> {
    const filenames = Object.keys(shared.data)
    return filenames.map(fn => ({ filename: fn }))
  }
}

// Create a flow for processing a single file
const summarizeFile = new Flow(loadFile)

// Create a batch flow that processes all files in parallel
const summarizeAllFiles = new ParallelBatchFlow(summarizeFile)
await summarizeAllFiles.run(shared)
```

## Nested Batch Processing

You can nest a **SequentialBatchFlow** or **ParallelBatchFlow** in another batch flow. For instance:

- **Outer** batch: returns a list of diretory param dicts (e.g., `{"directory": "/pathA"}`, `{"directory": "/pathB"}`, ...).
- **Inner** batch: returning a list of per-file param dicts.

At each level, **BatchFlow** merges its own param dict with the parent’s. By the time you reach the **innermost** node, the final `params` is the merged result of **all** parents in the chain. This way, a nested structure can keep track of the entire context (e.g., directory + file name) at once.

```typescript
class FileBatchFlow extends SequentialBatchFlow {
  async prep(shared: Record): Promise<Array<Record<string, string>>> {
    const directory = this.params.directory
    // In a real implementation, use fs.readdirSync or similar
    const files = ['file1.txt', 'file2.txt']
    return files.map(f => ({ filename: f }))
  }
}

class DirectoryBatchFlow extends SequentialBatchFlow {
  async prep(shared: Record): Promise<Array<Record<string, string>>> {
    const directories = ['/path/to/dirA', '/path/to/dirB']
    return directories.map(d => ({ directory: d }))
  }
}

// MapSummaries have params like {"directory": "/path/to/dirA", "filename": "file1.txt"}
const innerFlow = new FileBatchFlow(new MapSummaries())
const outerFlow = new DirectoryBatchFlow(innerFlow)
await outerFlow.run(shared)
```

In this nested batch example:

1. The outer flow iterates through directories
2. For each directory, the inner flow processes all files
3. Parameters are merged, so the innermost node receives both directory and filename

## Best Practices

1. **Choose the Right Type**: Use sequential processing when order matters or when managing rate limits; use parallel processing for independent operations
2. **Manage Chunk Size**: Balance between too many small chunks (overhead) and too few large chunks (memory issues)
3. **Error Handling**: Implement proper error handling to prevent one failure from stopping the entire batch
4. **Progress Tracking**: Add logging or progress indicators for long-running batch operations
5. **Resource Management**: Be mindful of memory usage, especially with large datasets




================================================
File: docs/core_abstraction/throttling.md
================================================
---
title: '(Advanced) Throttling'
---

# (Advanced) Throttling

**Throttling** helps manage concurrency and avoid rate limits when making API calls. This is particularly important when:

1. Calling external APIs with rate limits
2. Managing expensive operations (like LLM calls)
3. Preventing system overload from too many parallel requests

## Concurrency Control Patterns

### 1. Using Semaphores (Python)

```python
import asyncio

class LimitedParallelNode(Node):
    def __init__(self, concurrency=3):
        self.semaphore = asyncio.Semaphore(concurrency)

    async def exec(self, items):
        async def limited_process(item):
            async with self.semaphore:
                return await self.process_item(item)

        tasks = [limited_process(item) for item in items]
        return await asyncio.gather(*tasks)

    async def process_item(self, item):
        # Process a single item
        pass
```

### 2. Using p-limit (TypeScript)

```typescript
import pLimit from 'p-limit'

class LimitedParallelNode extends Node {
  constructor(private concurrency = 3) {
    super()
  }

  async exec(items) {
    const limit = pLimit(this.concurrency)
    return Promise.all(items.map((item) => limit(() => this.processItem(item))))
  }

  async processItem(item) {
    // Process a single item
  }
}
```

## Rate Limiting with Window Limits

```typescript
import { RateLimiter } from 'limiter'

// 30 calls per minute
const limiter = new RateLimiter({ tokensPerInterval: 30, interval: 'minute' })

async function callApi() {
  await limiter.removeTokens(1)
  // Your API call here
}
```

## Throttler Utility

```typescript
import pRetry from 'p-retry'

async function callApiWithRetry() {
  return pRetry(
    async () => {
      // Your API call here
    },
    {
      retries: 5,
      minTimeout: 4000,
      maxTimeout: 10000,
    },
  )
}
```

## Advanced Throttling Patterns

### 1. Token Bucket Rate Limiter

```typescript
import { TokenBucket } from 'limiter'

// 10 requests per minute
const limiter = new TokenBucket({
  bucketSize: 10,
  tokensPerInterval: 10,
  interval: 'minute',
})

async function callApi() {
  await limiter.removeTokens(1)
  // Your API call here
}
```

### 2. Sliding Window Rate Limiter

```typescript
import { SlidingWindowRateLimiter } from 'sliding-window-rate-limiter'

const limiter = SlidingWindowRateLimiter.createLimiter({
  interval: 60, // 60 seconds
  maxInInterval: 100,
})

async function callApi() {
  const isAllowed = await limiter.check('key', 1)
  if (!isAllowed) throw new Error('Rate limit exceeded')
  // Your API call here
}
```

{% hint style="info" %}
**Related Concepts**: Many throttling patterns are used with [Batch Processing](./batch.md) operations, particularly when dealing with parallel execution of API calls.
{% endhint %}

## Best Practices

1. **Monitor API Responses**: Watch for 429 (Too Many Requests) responses and adjust your rate limiting accordingly
2. **Implement Retry Logic**: When hitting rate limits, implement exponential backoff for retries
3. **Distribute Load**: If possible, spread requests across multiple API keys or endpoints
4. **Cache Responses**: Cache frequent identical requests to reduce API calls
5. **Batch Requests**: Combine multiple requests into single API calls when possible

## Linking to Related Concepts

For batch processing patterns, see [Batch Processing](./batch.md).




================================================
File: docs/design_pattern/index.md
================================================
# Design Patterns

BrainyFlow makes it easy to implement popular design patterns for LLM applications. This section covers the key patterns you can build with the framework.

## Available Design Patterns

### Core Patterns

1. [Agent](./agent.md): Create autonomous agents that can make decisions and take actions based on context.
2. [Workflow](./workflow.md): Chain multiple tasks into sequential pipelines for complex operations.
3. [RAG](./rag.md): Integrate data retrieval with generation for knowledge-augmented responses.
4. [Map Reduce](./mapreduce.md): Process large datasets by splitting work into parallel tasks and combining results.
5. [Structured Output](./structure.md): Format LLM outputs consistently using structured formats like YAML or JSON.

### Advanced Patterns

6. [Multi-Agents](./multi_agent.md): Coordinate multiple agents working together on complex tasks.

## Choosing the Right Pattern

When building your LLM application, consider these factors when selecting a design pattern:

| Pattern           | Best For                  | When To Use                                             |
| ----------------- | ------------------------- | ------------------------------------------------------- |
| Agent             | Dynamic problem-solving   | When tasks require reasoning and decision-making        |
| Workflow          | Sequential processing     | When steps are well-defined and follow a clear order    |
| RAG               | Knowledge-intensive tasks | When external information is needed for responses       |
| Map Reduce        | Large data processing     | When handling datasets too large for a single operation |
| Structured Output | Consistent formatting     | When outputs need to follow specific schemas            |
| Multi-Agents      | Complex collaboration     | When tasks benefit from specialized agent roles         |

## Decision Tree

Use this decision tree to help determine which pattern best fits your use case:

```mermaid
flowchart TD
    A[Start] --> B{Need to process large data?}
    B -->|Yes| C{Data can be processed independently?}
    B -->|No| D{Need to make decisions?}

    C -->|Yes| E[Map Reduce]
    C -->|No| F[Workflow]

    D -->|Yes| G{Complex, multi-step reasoning?}
    D -->|No| H[Simple Workflow]

    G -->|Yes| I[Agent]
    G -->|No| J{Need external knowledge?}

    J -->|Yes| K[RAG]
    J -->|No| L[Structured Output]
```

## Combining Patterns

Many real-world applications combine multiple patterns. For example:

- An **Agent** that uses **RAG** to retrieve information before making decisions
- A **Workflow** that includes **Map Reduce** steps for processing large datasets
- **Multi-Agents** that each use **Structured Output** for consistent communication

The modular nature of BrainyFlow makes it easy to combine these patterns to solve complex problems.

## Next Steps

Explore each design pattern in detail to understand how to implement it in your application:

- Start with [Agent](./agent.md) to learn about autonomous decision-making
- Check out [Workflow](./workflow.md) for sequential processing patterns
- Dive into [RAG](./rag.md) to see how to augment LLMs with external knowledge




================================================
File: docs/design_pattern/agent.md
================================================
---
title: 'Agent'
---

# Agent

Agent is a powerful design pattern in which nodes can take dynamic actions based on the context.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/agent.png?raw=true" width="350"/>
</div>

## Implement Agent with Graph

1. **Context and Action:** Implement nodes that supply context and perform actions.
2. **Branching:** Use branching to connect each action node to an agent node. Use action to allow the agent to direct the [flow](../core_abstraction/flow.md) between nodes—and potentially loop back for multi-step.
3. **Agent Node:** Provide a prompt to decide action—for example:

```typescript
;`### CONTEXT
Task: ${taskDescription}
Previous Actions: ${previousActions}
Current State: ${currentState}

### ACTION SPACE
[1] search
  Description: Use web search to get results
  Parameters:
    - query (string): What to search for

[2] answer
  Description: Conclude based on the results  
  Parameters:
    - result (string): Final answer to provide

### NEXT ACTION
Decide the next action based on the current context and available action space.
Return your response in the following format:

\`\`\`yaml
thinking: |
    <your step-by-step reasoning process>
action: <action_name>
parameters:
    <parameter_name>: <parameter_value>
\`\`\``
```

The core of building **high-performance** and **reliable** agents boils down to:

1. **Context Management:** Provide _relevant, minimal context._ For example, rather than including an entire chat history, retrieve the most relevant via [RAG](./rag.md). Even with larger context windows, LLMs still fall victim to ["lost in the middle"](https://arxiv.org/abs/2307.03172), overlooking mid-prompt content.

2. **Action Space:** Provide _a well-structured and unambiguous_ set of actions—avoiding overlap like separate `read_databases` or `read_csvs`. Instead, import CSVs into the database.

## Example Good Action Design

- **Incremental:** Feed content in manageable chunks (500 lines or 1 page) instead of all at once.

- **Overview-zoom-in:** First provide high-level structure (table of contents, summary), then allow drilling into details (raw texts).

- **Parameterized/Programmable:** Instead of fixed actions, enable parameterized (columns to select) or programmable (SQL queries) actions, for example, to read CSV files.

- **Backtracking:** Let the agent undo the last step instead of restarting entirely, preserving progress when encountering errors or dead ends.

## Example: Search Agent

This agent:

1. Decides whether to search or answer
2. If searches, loops back to decide if more search needed
3. Answers when enough context gathered

````typescript
class DecideAction extends Node {
  async prep(shared: any): Promise<[string, string]> {
    const context = shared.context || 'No previous search'
    const query = shared.query
    return [query, context]
  }

  async exec(inputs: [string, string]): Promise<any> {
    const [query, context] = inputs
    const prompt = `
Given input: ${query}
Previous search results: ${context}
Should I: 1) Search web for more info 2) Answer with current knowledge
Output in yaml:
\`\`\`yaml
action: search/answer
reason: why this action  
search_term: search phrase if action is search
\`\`\``

    const resp = await callLLM(prompt)
    const yamlStr = resp.split('```yaml')[1].split('```')[0].trim()
    const result = parseYaml(yamlStr)

    if (typeof result !== 'object' || !result) {
      throw new Error('Invalid YAML response')
    }
    if (!('action' in result)) {
      throw new Error('Missing action in response')
    }
    if (!('reason' in result)) {
      throw new Error('Missing reason in response')
    }
    if (!['search', 'answer'].includes(result.action)) {
      throw new Error('Invalid action value')
    }
    if (result.action === 'search' && !('search_term' in result)) {
      throw new Error('Missing search_term for search action')
    }

    return result
  }

  async post(shared: any, prepRes: any, execRes: any): Promise<string> {
    if (execRes.action === 'search') {
      shared.search_term = execRes.search_term
    }
    return execRes.action
  }
}
````

```typescript
class SearchWeb extends Node {
  async prep(shared: any): Promise<string> {
    return shared.search_term
  }

  async exec(searchTerm: string): Promise<any> {
    return await searchWeb(searchTerm)
  }

  async post(shared: any, prepRes: any, execRes: any): Promise<string> {
    const prevSearches = shared.context || []
    shared.context = [...prevSearches, { term: shared.search_term, result: execRes }]
    return 'decide'
  }
}
```

```typescript
class DirectAnswer extends Node {
  async prep(shared: any): Promise<[string, string]> {
    return [shared.query, shared.context || '']
  }

  async exec(inputs: [string, string]): Promise<string> {
    const [query, context] = inputs
    return await callLLM(`Context: ${context}\nAnswer: ${query}`)
  }

  async post(shared: any, prepRes: any, execRes: string): Promise<void> {
    console.log(`Answer: ${execRes}`)
    shared.answer = execRes
  }
}
```

```typescript
// Connect nodes
const decide = new DecideAction()
const search = new SearchWeb()
const answer = new DirectAnswer()

// Using operator overloading equivalents
decide.on('search', search)
decide.on('answer', answer)
search.on('decide', decide) // Loop back

const flow = new Flow(decide)

async function main() {
  const sharedData = { query: 'Who won the Nobel Prize in Physics 2024?' }
  const result = await flow.run(sharedData) // Added await
  console.log(result) // Or handle result as needed
  console.log(sharedData) // See final shared state
}

main().catch(console.error) // Execute async main function
```




================================================
File: docs/design_pattern/workflow.md
================================================
---
title: 'Workflow'
---

# Workflow

Many real-world tasks are too complex for one LLM call. The solution is **Task Decomposition**: decompose them into a [chain](../core_abstraction/flow.md) of multiple Nodes.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/workflow.png?raw=true" width="400"/>
</div>

{% hint style="success" %}
You don't want to make each task **too coarse**, because it may be _too complex for one LLM call_.
You don't want to make each task **too granular**, because then _the LLM call doesn't have enough context_ and results are _not consistent across nodes_.

You usually need multiple _iterations_ to find the _sweet spot_. If the task has too many _edge cases_, consider using [Agents](./agent.md).
{% endhint %}

### Example: Article Writing

```typescript
class GenerateOutline extends Node {
  async prep(shared: any): Promise<any> {
    return shared['topic']
  }
  async exec(topic: string): Promise<any> {
    return await callLLM(`Create a detailed outline for an article about ${topic}`)
  }
  async post(shared: any, prepRes: any, execRes: any): Promise<void> {
    shared['outline'] = execRes
  }
}

class WriteSection extends Node {
  async prep(shared: any): Promise<any> {
    return shared['outline']
  }
  async exec(outline: string): Promise<any> {
    return await callLLM(`Write content based on this outline: ${outline}`)
  }
  async post(shared: any, prepRes: any, execRes: any): Promise<void> {
    shared['draft'] = execRes
  }
}

class ReviewAndRefine extends Node {
  async prep(shared: any): Promise<any> {
    return shared['draft']
  }
  async exec(draft: string): Promise<any> {
    return await callLLM(`Review and improve this draft: ${draft}`)
  }
  async post(shared: any, prepRes: any, execRes: any): Promise<void> {
    shared['final_article'] = execRes
  }
}

// Connect nodes
const outline = new GenerateOutline()
const write = new WriteSection()
const review = new ReviewAndRefine()

outline.next(write).next(review)

// Create and run flow
const writingFlow = new Flow(outline)

async function main() {
  const shared = { topic: 'AI Safety' }
  await writingFlow.run(shared)
  console.log(`Final Article: ${shared['final_article'] || 'Not generated'}`)
}

main().catch(console.error) // Execute async main function
```

For _dynamic cases_, consider using [Agents](./agent.md).




================================================
File: docs/design_pattern/rag.md
================================================
---
title: 'RAG'
---

# RAG (Retrieval Augmented Generation)

For certain LLM tasks like answering questions, providing relevant context is essential. One common architecture is a **two-stage** RAG pipeline:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/rag.png?raw=true" width="400"/>
</div>

1. **Offline stage**: Preprocess and index documents ("building the index").
2. **Online stage**: Given a question, generate answers by retrieving the most relevant context.

---

## Stage 1: Offline Indexing

We create three Nodes:

1. `ChunkDocs` – [chunks](../utility_function/chunking.md) raw text.
2. `EmbedDocs` – [embeds](../utility_function/embedding.md) each chunk.
3. `StoreIndex` – stores embeddings into a [vector database](../utility_function/vector.md).

```typescript
class ChunkDocs extends SequentialBatchNode {
  async prep(shared: any): Promise<string[]> {
    // A list of file paths in shared["files"]. We process each file.
    return shared['files']
  }

  async exec(filepath: string): Promise<string[]> {
    // read file content. In real usage, do error handling.
    const text = fs.readFileSync(filepath, 'utf-8')
    // chunk by 100 chars each
    const chunks: string[] = []
    const size = 100
    for (let i = 0; i < text.length; i += size) {
      chunks.push(text.slice(i, i + size))
    }
    return chunks
  }

  async post(shared: any, prepRes: string[], execResList: string[][]): Promise<void> {
    // execResList is a list of chunk-lists, one per file.
    // flatten them all into a single list of chunks.
    const allChunks: string[] = []
    for (const chunkList of execResList) {
      allChunks.push(...chunkList)
    }
    shared['all_chunks'] = allChunks
  }
}
```

```typescript
class EmbedDocs extends SequentialBatchNode {
  async prep(shared: any): Promise<string[]> {
    return shared['all_chunks']
  }

  async exec(chunk: string): Promise<number[]> {
    return await getEmbedding(chunk)
  }

  async post(shared: any, prepRes: string[], execResList: number[][]): Promise<void> {
    // Store the list of embeddings.
    shared['all_embeds'] = execResList
    console.log(`Total embeddings: ${execResList.length}`)
  }
}
```

```typescript
class StoreIndex extends Node {
  async prep(shared: any): Promise<number[][]> {
    // We'll read all embeds from shared.
    return shared['all_embeds']
  }

  async exec(allEmbeds: number[][]): Promise<any> {
    // Create a vector index (faiss or other DB in real usage).
    const index = createIndex(allEmbeds)
    return index
  }

  async post(shared: any, prepRes: number[][], index: any): Promise<void> {
    shared['index'] = index
  }
}
```

```typescript
// Wire them in sequence
const chunkNode = new ChunkDocs()
const embedNode = new EmbedDocs()
const storeNode = new StoreIndex()

chunkNode.next(embedNode).next(storeNode)

const OfflineFlow = new Flow(chunkNode)
```

Usage example:

```typescript
const shared = {
  files: ['doc1.txt', 'doc2.txt'], // any text files
}
await OfflineFlow.run(shared)
```

---

## Stage 2: Online Query & Answer

We have 3 nodes:

1. `EmbedQuery` – embeds the user’s question.
2. `RetrieveDocs` – retrieves top chunk from the index.
3. `GenerateAnswer` – calls the LLM with the question + chunk to produce the final answer.

```typescript
class EmbedQuery extends Node {
  async prep(shared: any): Promise<string> {
    return shared['question']
  }

  async exec(question: string): Promise<number[]> {
    return await getEmbedding(question)
  }

  async post(shared: any, prepRes: string, qEmb: number[]): Promise<void> {
    shared['q_emb'] = qEmb
  }
}
```

```typescript
class RetrieveDocs extends Node {
  async prep(shared: any): Promise<[number[], any, string[]]> {
    // We'll need the query embedding, plus the offline index/chunks
    return [shared['q_emb'], shared['index'], shared['all_chunks']]
  }

  async exec(inputs: [number[], any, string[]]): Promise<string> {
    const [qEmb, index, chunks] = inputs
    const [I, D] = searchIndex(index, qEmb, 1)
    const bestId = I[0][0]
    const relevantChunk = chunks[bestId]
    return relevantChunk
  }

  async post(
    shared: any,
    prepRes: [number[], any, string[]],
    relevantChunk: string,
  ): Promise<void> {
    shared['retrieved_chunk'] = relevantChunk
    console.log(`Retrieved chunk: ${relevantChunk.slice(0, 60)}...`)
  }
}
```

```typescript
class GenerateAnswer extends Node {
  async prep(shared: any): Promise<[string, string]> {
    return [shared['question'], shared['retrieved_chunk']]
  }

  async exec(inputs: [string, string]): Promise<string> {
    const [question, chunk] = inputs
    const prompt = `Question: ${question}\nContext: ${chunk}\nAnswer:`
    return await callLLM(prompt)
  }

  async post(shared: any, prepRes: [string, string], answer: string): Promise<void> {
    shared['answer'] = answer
    console.log(`Answer: ${answer}`)
  }
}
```

```typescript
const embedQNode = new EmbedQuery()
const retrieveNode = new RetrieveDocs()
const generateNode = new GenerateAnswer()

embedQNode.next(retrieveNode).next(generateNode)
const OnlineFlow = new Flow(embedQNode)
```

Usage example:

```typescript
async function runOnline(sharedFromOffline: any): Promise<any> {
  // Suppose we already ran OfflineFlow (runOffline) and have:
  // sharedFromOffline["all_chunks"], sharedFromOffline["index"], etc.
  sharedFromOffline['question'] = 'Why do people like cats?'

  await OnlineFlow.run(sharedFromOffline)
  // final answer in sharedFromOffline["answer"]
  console.log(`Final Answer: ${sharedFromOffline['answer']}`)
  return sharedFromOffline
}

// Example usage combining both stages
async function main() {
  const offlineShared = await runOffline()
  await runOnline(offlineShared)
}

main().catch(console.error) // Execute async main function
```




================================================
File: docs/design_pattern/mapreduce.md
================================================
---
title: 'Map Reduce'
---

# Map Reduce

MapReduce is a design pattern suitable when you have either:

- Large input data (e.g., multiple files to process), or
- Large output data (e.g., multiple forms to fill)

and there is a logical way to break the task into smaller, ideally independent parts.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/mapreduce.png?raw=true" width="400"/>
</div>

You first break down the task using [BatchNode](../core_abstraction/batch.md) in the map phase, followed by aggregation in the reduce phase.

### Example: Document Summarization

```typescript
class SummarizeAllFiles extends SequentialBatchNode {
  async prep(shared: any): Promise<[string, string][]> {
    const filesDict = shared.files // e.g. 10 files
    return Object.entries(filesDict) // [["file1.txt", "aaa..."], ["file2.txt", "bbb..."], ...]
  }

  async exec(oneFile: [string, string]): Promise<[string, string]> {
    const [filename, fileContent] = oneFile
    const summaryText = await callLLM(`Summarize the following file:\n${fileContent}`)
    return [filename, summaryText]
  }

  async post(shared: any, prepRes: any, execResList: [string, string][]): Promise<void> {
    shared.file_summaries = Object.fromEntries(execResList)
  }
}

class CombineSummaries extends Node {
  async prep(shared: any): Promise<Record<string, string>> {
    return shared.file_summaries
  }

  async exec(fileSummaries: Record<string, string>): Promise<string> {
    // format as: "File1: summary\nFile2: summary...\n"
    const textList: string[] = []
    for (const [fname, summ] of Object.entries(fileSummaries)) {
      textList.push(`${fname} summary:\n${summ}\n`)
    }
    const bigText = textList.join('\n---\n')

    return await callLLM(`Combine these file summaries into one final summary:\n${bigText}`)
  }

  async post(shared: any, prepRes: any, finalSummary: string): Promise<void> {
    shared.all_files_summary = finalSummary
  }
}

const batchNode = new SummarizeAllFiles()
const combineNode = new CombineSummaries()
batchNode.next(combineNode)

const flow = new Flow(batchNode)

async function main() {
  const shared = {
    files: {
      'file1.txt': 'Alice was beginning to get very tired of sitting by her sister...',
      'file2.txt': 'Some other interesting text ...',
      // ...
    },
  }
  await flow.run(shared)
  console.log('Individual Summaries:', shared.file_summaries)
  console.log('\nFinal Summary:\n', shared.all_files_summary)
}

main().catch(console.error)
```




================================================
File: docs/design_pattern/structure.md
================================================
---
title: 'Structured Output'
---

# Structured Output

In many use cases, you may want the LLM to output a specific structure, such as a list or a dictionary with predefined keys.

There are several approaches to achieve a structured output:

- **Prompting** the LLM to strictly return a defined structure.
- Using LLMs that natively support **schema enforcement**.
- **Post-processing** the LLM's response to extract structured content.

In practice, **Prompting** is simple and reliable for modern LLMs.

### Example Use Cases

- Extracting Key Information

```yaml
product:
  name: Widget Pro
  price: 199.99
  description: |
    A high-quality widget designed for professionals.
    Recommended for advanced users.
```

- Summarizing Documents into Bullet Points

```yaml
summary:
  - This product is easy to use.
  - It is cost-effective.
  - Suitable for all skill levels.
```

- Generating Configuration Files

```yaml
server:
  host: 127.0.0.1
  port: 8080
  ssl: true
```

## Prompt Engineering

When prompting the LLM to produce **structured** output:

1. **Wrap** the structure in code fences (e.g., `yaml`).
2. **Validate** that all required fields exist (and let `Node` handles retry).

### Example Text Summarization

````typescript
class SummarizeNode extends Node {
  async exec(prepRes: string): Promise<any> {
    // Suppose prepRes is the text to summarize
    const prompt = `
Please summarize the following text as YAML, with exactly 3 bullet points

${prepRes}

Now, output:
\`\`\`yaml
summary:
  - bullet 1
  - bullet 2
  - bullet 3
\`\`\``

    const response = await callLLM(prompt)
    const yamlStr = response.split('```yaml')[1].split('```')[0].trim()

    // In TypeScript we would typically use a YAML parser like 'yaml'
    const structuredResult = require('yaml').parse(yamlStr)

    if (!('summary' in structuredResult)) {
      throw new Error("Missing 'summary' in result")
    }
    if (!Array.isArray(structuredResult.summary)) {
      throw new Error('Summary must be an array')
    }

    return structuredResult
  }
}
````

{% hint style="info" %}
Besides using `assert` statements, another popular way to validate schemas is [Pydantic](https://github.com/pydantic/pydantic)
{% endhint %}

### Why YAML instead of JSON?

Current LLMs struggle with escaping. YAML is easier with strings since they don't always need quotes.

**In JSON**

```json
{
  "dialogue": "Alice said: \"Hello Bob.\\nHow are you?\\nI am good.\""
}
```

- Every double quote inside the string must be escaped with `\"`.
- Each newline in the dialogue must be represented as `\n`.

**In YAML**

```yaml
dialogue: |
  Alice said: "Hello Bob.
  How are you?
  I am good."
```

- No need to escape interior quotes—just place the entire text under a block literal (`|`).
- Newlines are naturally preserved without needing `\n`.




================================================
File: docs/design_pattern/multi_agent.md
================================================
---
title: '(Advanced) Multi-Agents'
---

# (Advanced) Multi-Agents

Multiple [Agents](./flow.md) can work together by handling subtasks and communicating the progress.
Communication between agents is typically implemented using message queues in shared storage.

{% hint style="success" %}
Most of time, you don't need Multi-Agents. Start with a simple solution first.
{% endhint %}

### Example Agent Communication: Message Queue

Here's a simple example showing how to implement agent communication using `asyncio.Queue`.
The agent listens for messages, processes them, and continues listening:

```typescript
class AgentNode extends Node {
  async prep(_: any): Promise<string> {
    const messageQueue = this.params.messages as AsyncQueue<string>
    const message = await messageQueue.get()
    console.log(`Agent received: ${message}`)
    return message
  }
}

// Create node and flow
const agent = new AgentNode()
agent.next(agent) // connect to self
const flow = new Flow(agent)

// Create heartbeat sender
async function sendSystemMessages(messageQueue: AsyncQueue<string>) {
  let counter = 0
  const messages = [
    'System status: all systems operational',
    'Memory usage: normal',
    'Network connectivity: stable',
    'Processing load: optimal',
  ]

  while (true) {
    const message = `${messages[counter % messages.length]} | timestamp_${counter}`
    await messageQueue.put(message)
    counter++
    await new Promise((resolve) => setTimeout(resolve, 1000))
  }
}

async function main() {
  const messageQueue = new AsyncQueue<string>()
  const shared = {}
  flow.setParams({ messages: messageQueue })

  // Run both coroutines
  await Promise.all([flow.run(shared), sendSystemMessages(messageQueue)])
}

class AsyncQueue<T> {
  private queue: T[] = []
  private waiting: ((value: T) => void)[] = []

  async get(): Promise<T> {
    if (this.queue.length > 0) {
      return this.queue.shift()!
    }
    return new Promise((resolve) => {
      this.waiting.push(resolve)
    })
  }

  async put(item: T): Promise<void> {
    if (this.waiting.length > 0) {
      const resolve = this.waiting.shift()!
      resolve(item)
    } else {
      this.queue.push(item)
    }
  }
}

main().catch(console.error)
```

The output:

```
Agent received: System status: all systems operational | timestamp_0
Agent received: Memory usage: normal | timestamp_1
Agent received: Network connectivity: stable | timestamp_2
Agent received: Processing load: optimal | timestamp_3
```

### Interactive Multi-Agent Example: Taboo Game

Here's a more complex example where two agents play the word-guessing game Taboo.
One agent provides hints while avoiding forbidden words, and another agent tries to guess the target word:

```typescript
class Hinter extends Node {
  async prep(shared: any): Promise<any> {
    const guess = await shared.hinterQueue.get()
    if (guess === 'GAME_OVER') {
      return null
    }
    return [shared.targetWord, shared.forbiddenWords, shared.pastGuesses || []]
  }

  async exec(inputs: any): Promise<string | null> {
    if (inputs === null) return null
    const [target, forbidden, pastGuesses] = inputs
    let prompt = `Generate hint for '${target}'\nForbidden words: ${forbidden}`
    if (pastGuesses.length > 0) {
      prompt += `\nPrevious wrong guesses: ${pastGuesses}\nMake hint more specific.`
    }
    prompt += '\nUse at most 5 words.'

    const hint = await callLLM(prompt)
    console.log(`\nHinter: Here's your hint - ${hint}`)
    return hint
  }

  async post(shared: any, prepRes: any, execRes: any): Promise<string> {
    if (execRes === null) return 'end'
    await shared.guesserQueue.put(execRes)
    return 'continue'
  }
}

class Guesser extends Node {
  async prep(shared: any): Promise<any> {
    const hint = await shared.guesserQueue.get()
    return [hint, shared.pastGuesses || []]
  }

  async exec(inputs: any): Promise<string> {
    const [hint, pastGuesses] = inputs
    const prompt = `Given hint: ${hint}, past wrong guesses: ${pastGuesses}, make a new guess. Directly reply a single word:`
    const guess = await callLLM(prompt)
    console.log(`Guesser: I guess it's - ${guess}`)
    return guess
  }

  async post(shared: any, prepRes: any, execRes: any): Promise<string> {
    if (execRes.toLowerCase() === shared.targetWord.toLowerCase()) {
      console.log('Game Over - Correct guess!')
      await shared.hinterQueue.put('GAME_OVER')
      return 'end'
    }

    if (!shared.pastGuesses) {
      shared.pastGuesses = []
    }
    shared.pastGuesses.push(execRes)

    await shared.hinterQueue.put(execRes)
    return 'continue'
  }
}

async function main() {
  // Set up game
  const shared = {
    targetWord: 'nostalgia',
    forbiddenWords: ['memory', 'past', 'remember', 'feeling', 'longing'],
    hinterQueue: new AsyncQueue<string>(),
    guesserQueue: new AsyncQueue<string>(),
  }

  console.log('Game starting!')
  console.log(`Target word: ${shared.targetWord}`)
  console.log(`Forbidden words: ${shared.forbiddenWords}`)

  // Initialize by sending empty guess to hinter
  await shared.hinterQueue.put('')

  // Create nodes and flows
  const hinter = new Hinter()
  const guesser = new Guesser()

  // Set up flows
  const hinterFlow = new Flow(hinter)
  const guesserFlow = new Flow(guesser)

  // Connect nodes to themselves
  hinter.on('continue', hinter)
  guesser.on('continue', guesser)

  // Run both agents concurrently
  await Promise.all([hinterFlow.run(shared), guesserFlow.run(shared)])
}

// Mock LLM call for TypeScript
async function callLLM(prompt: string): Promise<string> {
  // In a real implementation, this would call an actual LLM API
  return 'Mock LLM response'
}

main().catch(console.error)
```

The Output:

```
Game starting!
Target word: nostalgia
Forbidden words: ['memory', 'past', 'remember', 'feeling', 'longing']

Hinter: Here's your hint - Thinking of childhood summer days
Guesser: I guess it's - popsicle

Hinter: Here's your hint - When childhood cartoons make you emotional
Guesser: I guess it's - nostalgic

Hinter: Here's your hint - When old songs move you
Guesser: I guess it's - memories

Hinter: Here's your hint - That warm emotion about childhood
Guesser: I guess it's - nostalgia
Game Over - Correct guess!
```




================================================
File: docs/utility_function/index.md
================================================
---
machine-display: true
---

# Utility Functions

BrainyFlow does not provide built-in utilities. Instead, we offer examples that you can implement yourself. This approach gives you more flexibility and control over your project's dependencies and functionality.

## Available Utility Function Examples

1. [LLM Wrapper](./llm.md): Interact with Language Models
2. [Visualization and Debugging](./viz.md): Tools for visualizing and debugging flows
3. [Web Search](./websearch.md): Perform web searches
4. [Chunking](./chunking.md): Split large texts into manageable chunks
5. [Embedding](./embedding.md): Generate vector embeddings for text
6. [Vector Databases](./vector.md): Store and query vector embeddings
7. [Text-to-Speech](./text_to_speech.md): Convert text to speech

## Why Not Built-in?

We believe it's a bad practice to include vendor-specific APIs in a general framework for several reasons:

1. **API Volatility**: Frequent changes in external APIs lead to heavy maintenance for hardcoded APIs.
2. **Flexibility**: You may want to switch vendors, use fine-tuned models, or run them locally.
3. **Optimizations**: Prompt caching, batching, and streaming are easier to implement without vendor lock-in.

## Implementing Utility Functions

When implementing utility functions for your BrainyFlow project:

1. Create a separate file for each utility function in the `utils/` directory.
2. Include a simple test or example usage in each file.
3. Document the input/output and purpose of each utility function.

Example structure:

```
my_project/
├── utils/
│   ├── callLlm.ts
│   ├── searchWeb.ts
│   └── embedText.ts
└── ...
```

By following this approach, you can easily maintain and update your utility functions as needed, without being constrained by the framework's built-in utilities.




================================================
File: docs/guides/agentic_coding.md
================================================
# Agentic Coding with BrainyFlow

{% hint style="warning" %}
**For AI Assistants**: This is your implementation playbook. Use the design document as your single source of truth. Never proceed to code without explicit human approval of the design.

1.  start with a small and simple solution
2.  design at a high level (`docs/design.md`) before implementation
3.  frequently ask humans for feedback and clarification

{% endhint %}

Agentic coding represents a powerful approach to software development where humans focus on high-level design and strategic decisions while AI agents handle implementation details. This guide will help you create effective design documents that enable successful BrainyFlow implementations.

## The AI Implementation Brief

```mermaid
flowchart TD
    A[Human Request] --> B{AI Asks Questions}
    B --> C[AI Generates Structured Design Draft]
    C --> D{Human Validates/Edits}
    D -->|Approved| E[AI Implements]
    D -->|Needs Changes| B
    E --> F[Continuous Co-Refinement]
```

- **AI-Driven Structuring:** Convert vague requests into technical specifications through dialogue
- **Essentialism:** Only capture requirements that directly impact implementation
- **Living Documentation:** Design evolves organically through implementation insights

Before writing any code, create a comprehensive AI Implementation Brief at `docs/design.md`. This document serves as the foundation for human-AI collaboration and should contain all the essential sections listed below.

### 1. Requirements Definition

Clearly articulate what you're building and why:

- **Problem Statement**: Define the problem being solved in 1-2 sentences
- **User Needs**: Describe who will use this and what they need
- **Success Criteria**: List measurable outcomes that define success
- **Constraints**: Note any technical or business limitations

Example:

```
We need a document processing system that extracts key information from legal contracts,
summarizes them, and stores the results for easy retrieval. This will help our legal
team review contracts 70% faster.
```

### 2. Flow Design

Outline the high-level architecture using BrainyFlow's nested directed graph abstraction:

- **Flow Diagram**: Create a mermaid diagram showing node connections
- **Processing Stages**: Describe each major stage in the flow
- **Decision Points**: Identify where branching logic occurs
- **Data Flow**: Explain how information moves through the system

Example:

```mermaid
graph TD
    A[DocumentLoader] --> B[TextExtractor]
    B --> C[EntityExtractor]
    C --> D[ValidationNode]
    D -->|Valid| E[SummaryGenerator]
    D -->|Invalid| C
    E --> F[DatabaseStorage]
```

### 3. Utility Functions

List all external utilities needed:

- **Function Name**: Clear, descriptive name
- **Purpose**: What the function does
- **Inputs/Outputs**: Expected parameters and return values
- **External Dependencies**: Any APIs or libraries required

Example:

```
extract_entities(text: str) -> dict:
- Purpose: Uses NER to identify entities in text
- Input: Document text string
- Output: Dictionary of entity lists by type
- Dependencies: spaCy NLP library with legal model
```

### 4. Node Design

For each node in your flow, define:

- **Purpose**: One-line description of what the node does
- **Shared Store Access**: What data it reads from and writes to the shared store
- **Lifecycle Implementation**: How `prep`, `exec`, and `post` will be implemented
- **Action Returns**: What actions the node might return to direct flow
- **Error Handling**: How failures will be managed

Example:

```
EntityExtractorNode:
- Purpose: Identifies parties, dates, and monetary values in contract text
- Reads: document_text from shared store
- Writes: entities dictionary to shared store
- Actions: Returns "valid" if entities found, "retry" if processing failed
- Error Handling: Will retry up to 3 times with exponential backoff
```

### 5. Shared Store Schema

Define the structure of your shared store:

- **Key Namespaces**: Major sections of your shared store
- **Data Types**: Expected types for each key
- **Data Flow**: How data evolves through processing

Example:

```
shared = {
    "input": {
        "document_path": "path/to/file.pdf"  # Input file path
    },
    "processing": {
        "document_text": "",     # Extracted text content
        "entities": {            # Extracted entities
            "parties": [],
            "dates": [],
            "amounts": []
        },
        "validation_status": ""  # Status after validation
    },
    "output": {
        "summary": "",           # Generated summary
        "storage_id": ""         # Database reference ID
    }
}
```

## Best Practices for Your Design Document

1. **Start Simple**: Begin with the minimal viable solution
2. **Be Explicit**: Clearly define all components and their interactions
3. **Visualize Flows**: Use diagrams to illustrate complex relationships
4. **Define Boundaries**: Clarify what's in and out of scope
5. **Consider Edge Cases**: Note how the system handles failures

## Implementation Process

After completing your design document:

1. **Review with Stakeholders**: Ensure the design meets requirements. If you are the AI agent, ask the user to verify the design document and confirm it fits their needs.
2. **Refine as Needed**: Iterate on the design based on feedback
3. **Hand Off to AI**: Provide the design document to your AI assistant - If you are the AI agent, start working on the solution based on the design document
4. **Collaborative Implementation**: Work with AI to implement the design
5. **Test and Validate**: Verify the implementation against success criteria

## Conclusion: Precision Through Structure

This approach ensures all BrainyFlow solutions maintain:

- **Human Focus:** Strategic requirements and validation

- **AI Precision:** Structured implementation targets

- **System Integrity:** Clear component boundaries

By enforcing these four pillars through adaptive dialogue rather than rigid templates, we achieve flexible yet reliable AI system development. The design document becomes a living contract between human intent and AI execution.

You provide your AI assistant with the clear direction needed to implement an effective BrainyFlow solution while maintaining human oversight of the critical design decisions.

Remember: The quality of your design document directly impacts the quality of the implementation. Invest time in creating a comprehensive brief to ensure successful outcomes.




================================================
File: docs/guides/best_practices.md
================================================
# BrainyFlow Best Practices

## Node Design

1. **Keep Nodes Focused**: Each node should do one thing well
2. **Idempotent Execution**: Design `exec()` methods to be safely retryable
3. **Graceful Degradation**: Implement fallbacks for when things go wrong
4. **Proper Error Handling**: Use `exec_fallback` to handle failures gracefully

## Shared Store Management

1. **Schema Design**: Define a clear schema for your shared store
2. **Namespacing**: Use namespaces to avoid key collisions
3. **Immutability**: Treat shared store values as immutable when possible
4. **Documentation**: Document the expected structure of your shared store

## Flow Design

1. **Visualization First**: Design your flow visually before coding
2. **Test Incrementally**: Build and test one section of your flow at a time
3. **Error Paths**: Always include paths for handling errors
4. **Monitoring**: Add logging at key points in your flow

## Project Structure

A well-organized project structure enhances maintainability and collaboration:

{% tabs %}
{% tab title="TypeScript (simple)" %}

```
my_project/
├── src/
│   ├── main.ts
│   ├── nodes.ts
│   ├── flow.ts
│   └── utils/
│       ├── callLLM.ts
│       └── searchWeb.ts
├── package.json
└── docs/
    └── design.md
```

{% endtab %}
{% tab title="TypeScript (complex)" %}

```
my_complex_project/
├── src/                      # Source code
│   ├── index.ts              # Entry point
│   ├── nodes/                # Node implementations
│   │   ├── index.ts          # Exports all nodes
│   │   ├── inputNodes.ts
│   │   ├── processingNodes.ts
│   │   └── outputNodes.ts
│   ├── flows/                # Flow definitions
│   │   ├── index.ts          # Exports all flows
│   │   └── mainFlow.ts
│   ├── utils/                # Utility functions
│   │   ├── index.ts          # Exports all utilities
│   │   ├── llm.ts
│   │   ├── database.ts
│   │   └── webSearch.ts
│   ├── types/                # Type definitions
│   │   ├── index.ts          # Exports all types
│   │   ├── node.types.ts
│   │   └── flow.types.ts
│   └── config/               # Configuration
│       └── settings.ts
├── dist/                     # Compiled JavaScript
├── tests/                    # Test cases
│   ├── nodes.test.ts
│   └── flows.test.ts
├── package.json              # Dependencies and scripts
└── docs/                     # Documentation
    ├── design.md             # High-level design
    └── api.md                # API documentation
```

{% endtab %}
{% endtabs %}

- **`docs/design.md`**: Contains project documentation for each step designed in [agentic coding](./agentic_coding.md). This should be _high-level_ and _no-code_.
- **`utils/`**: Contains all utility functions.
  - It's recommended to dedicate one file to each API call, for example `call_llm.py` or `search_web.ts`.
  - Each file should also include a `main()` function to try that API call
- **`nodes.py`** or **`nodes.ts`**: Contains all the node definitions.
- **`flow.py`** or **`flow.ts`**: Implements functions that create flows by importing node definitions and connecting them.
- **`main.py`** or **`main.ts`**: Serves as the project's entry point.


