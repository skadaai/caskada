

================================================
File: docs/installation.md
================================================
# Installation

BrainyFlow is currently available for both Python and TypeScript.

{% tabs %}
{% tab title="TypeScript" %}
You can install the TypeScript package using pnpm (or npm/yarn):

```bash
pnpm add brainyflow
# or
npm install brainyflow
# or
yarn add brainyflow
```

{% endtab %}
{% tab title="JavaScript (Browser)" %}
You can import the JavaScript file directly in the browser using a `<script>` tag:

```html
<script type="module">
  import * as brainyflow from 'https://unpkg.com/brainyflow@latest/dist/brainyflow.js'

  new brainyflow.Node(...)
</script>
```

or

```html
<script type="module" src="https://unpkg.com/brainyflow@latest/dist/brainyflow.js"></script>
<script>
  new globalThis.brainyflow.Node(...)
</script>
```

{% endtab %}
{% endtabs %}

## Alternative: Copy the Source Code

Since BrainyFlow is lightweight and dependency-free, you can also install it by simply copying the source code file directly into your project:

Copy [`typescript/brainyflow.ts`](https://github.com/zvictor/BrainyFlow/blob/main/typescript/brainyflow.ts)

## Next Steps

Once you have BrainyFlow installed, check out the [Getting Started](./getting_started.md) guide to build your first flow, or explore the [Core Abstractions](./core_abstraction/node.md) to understand the framework's fundamental concepts.




================================================
File: docs/getting_started.md
================================================
# Getting Started with BrainyFlow

Welcome to BrainyFlow! This framework helps you build powerful, modular AI applications using a simple yet expressive abstraction based on nested directed graphs.

## 1. Installation

First, ensure you have BrainyFlow installed:

```bash
npm install brainyflow # or pnpm/yarn
```

For more installation options, see the [Installation Guide](./installation.md).

## 2. Core Concepts

BrainyFlow is built around a minimalist yet powerful abstraction that separates data flow from computation:

- **[Node](./core_abstraction/node.md)**: The fundamental building block that performs a single task with a clear lifecycle (`prep` → `exec` → `post`).
- **[Flow](./core_abstraction/flow.md)**: Orchestrates nodes in a directed graph, supporting branching, looping, and nesting.
- **[Memory](./core_abstraction/memory.md)**: Manages state, separating it into a shared `global` store and a forkable `local` store for isolated data flow between nodes.

## 3. Your First Flow

Let's build a simple Question-Answering flow to demonstrate BrainyFlow's core concepts:

### Step 1: Design Your Flow

Our flow will have two nodes:

1. `GetQuestionNode`: Captures the user's question
2. `AnswerNode`: Generates an answer using an LLM

```mermaid
graph LR
    A[GetQuestionNode] --> B[AnswerNode]
```

### Step 2: Implement the Nodes

```typescript
import { Flow, Memory, Node } from 'brainyflow'
import { input } from '@inquirer/prompts'
import { callLLM } from './utils/callLLM'

// Define interfaces for Memory stores (optional but good practice)
interface QAGlobalStore {
  question?: string
  answer?: string
}
class GetQuestionNode extends Node<QAGlobalStore> {
  async prep(memory: Memory<QAGlobalStore>): Promise<void> {
    memory.question = await input({ message: 'Enter your question: ' })
  }
}

class AnswerNode extends Node<QAGlobalStore> {
  async prep(memory: Memory<QAGlobalStore>): Promise<string | undefined> {
    return memory.question
  }

  async exec(question: string | undefined): Promise<string> {
    const prompt = `Answer the following question: ${question}`
    return await callLLM(prompt)
  }

  async post(
    memory: Memory<QAGlobalStore>,
    prepRes: string | undefined,
    execRes: string,
  ): Promise<void> {
    memory.answer = execRes
    console.log(`AnswerNode: Stored answer '${execRes}'`)
  }
}
```

{% hint style="info" %}

**Review:** What was achieved here?

- `GetQuestionNode` gets the user's question and writes it to the `memory` object (global store), then explicitly `trigger`s the default next node.
- `AnswerNode` reads the question from the `memory` object, calls an LLM utility, writes the answer back to the `memory` object, and `trigger`s the next step (or ends the flow).

{% endhint %}

### Step 3: Connect the Nodes into a Flow

```typescript
// import { GetQuestionNode, AnswerNode } from './nodes'; // defined in the previous step
import { Flow } from 'brainyflow'

function createQaFlow(): Flow {
  const getQuestionNode = new GetQuestionNode()
  const answerNode = new AnswerNode()

  // Connect nodes getQuestionNode → answerNode using the default action
  getQuestionNode.next(answerNode)

  // Create the Flow, specifying the starting node
  return new Flow(getQuestionNode)
}
```

{% hint style="info" %}

**Review:** What was achieved here?

- We instantiated the nodes and connected them using the default action (`>>` in Python, `.next()` in TypeScript).
- We created a `Flow` instance, telling it to start execution with `getQuestionNode`.

{% endhint %}

### Step 4: Run the Flow

```typescript
import { createQaFlow, QAGlobalStore } from './flow' // defined in the previous steps

async function main() {
  // Initialize the global store (can be an empty object)
  const globalStore: QAGlobalStore = {}
  const qaFlow = createQaFlow()

  console.log('Running QA Flow...')
  // Run the flow, passing the initial global store.
  // The flow modifies the globalStore object in place.
  // The run method returns the final execution tree (we ignore it here).
  await qaFlow.run(globalStore)

  // Access the results stored in the global store
  console.log('\n--- Flow Complete ---')
  console.log(`Question: ${globalStore.question ?? 'N/A'}`)
  console.log(`Answer: ${globalStore.answer ?? 'N/A'}`)
}

main().catch(console.error)
```

{% hint style="info" %}

**Review:** What was achieved here?

- We initialized an empty `memory` object (Python dictionary or TS object) to serve as the global store.
- `qaFlow.run(memory)` executed the flow. The `Memory` instance managed the state internally, reading from and writing to the `memory` object we passed in.
- The final `question` and `answer` are directly accessible in the original `memory` object after the flow completes.

{% endhint %}

## 4. Key Design Principles

BrainyFlow follows these core design principles:

1. **Separation of Concerns**: Data storage (the `memory` object managing global/local stores) is separate from computation logic (`Node` classes).
2. **Explicit Data Flow**: Data dependencies between steps are clear and traceable through `memory` access in `prep`/`post` and the results passed between `prep` → `exec` → `post`.
3. **Composability**: Complex systems (`Flow`s) are built from simple, reusable components (`Node`s), and Flows themselves can be nested within other Flows.
4. **Minimalism**: The framework provides only essential abstractions (`Node`, `Flow`, `Memory`), avoiding vendor-specific implementations or excessive boilerplate.

## 5. Next Steps

Now that you understand the basics, explore these resources to build sophisticated applications:

- [Core Abstractions](./core_abstraction/index.md): Dive deeper into nodes, flows, and communication
- [Design Patterns](./design_pattern/index.md): Learn more complex patterns like Agents, RAG, and MapReduce
- [Agentic Coding Guide](./guides/agentic_coding.md): Best practices for human-AI collaborative development
- [Best Practices](./guides/best_practices.md): Tips for building robust, maintainable applications




================================================
File: docs/core_abstraction/index.md
================================================
# Understanding BrainyFlow's Core Abstractions

BrainyFlow is built around a simple yet powerful abstraction: the **nested directed graph with shared store**. This mental model separates _data flow_ from _computation_, making complex LLM applications more maintainable and easier to reason about.

<div align="center">
  <img src="https://raw.githubusercontent.com/zvictor/brainyflow/main/.github/media/abstraction.jpg" width="1300"/>
</div>

## Core Philosophy

BrainyFlow follows these fundamental principles:

1. **Modularity & Composability**: Build complex systems from simple, reusable components that are easy to build, test, and maintain
2. **Explicitness**: Make data dependencies between steps clear and traceable
3. **Separation of Concerns**: Data storage (shared store) remains separate from computation logic (nodes)
4. **Minimalism**: The framework provides only essential abstractions, avoiding vendor-specific implementations while supporting various high-level AI design paradigms (agents, workflows, map-reduce, etc.)
5. **Resilience**: Handle failures gracefully with retries and fallbacks

## The Graph + Shared Store Pattern

The fundamental pattern in BrainyFlow combines two key elements:

- **Computation Graph**: A directed graph where nodes represent discrete units of work and edges represent the flow of control.
- **Shared Store**: A state management system that enables communication between nodes, separating global and local state.

This pattern offers several advantages:

- **Clear visualization** of application logic
- **Easy identification** of bottlenecks
- **Simple debugging** of individual components
- **Natural parallelization** opportunities

## Key Components

BrainyFlow's architecture is based on these fundamental building blocks:

| Component             | Description                                    | Key Features                                                                                                |
| --------------------- | ---------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| [Node](./node.md)     | The basic unit of work                         | Clear lifecycle (`prep` → `exec` → `post`), fault tolerance (retries), graceful fallbacks                   |
| [Flow](./flow.md)     | Connects nodes together                        | Action-based transitions, branching, looping (with cycle detection), nesting, sequential/parallel execution |
| [Memory](./memory.md) | Manages state accessible during flow execution | Shared `global` store, forkable `local` store, cloning for isolation                                        |

## How They Work Together

1. **Nodes** perform individual tasks with a clear lifecycle:

   - `prep`: Read from shared store and prepare data
   - `exec`: Execute computation (often LLM calls)
   - `post`: Process results, write to shared store, and trigger next actions

2. **Flows** orchestrate nodes by:

   - Starting with a designated `start` node.
   - Following action-based transitions (`trigger` calls in `post`) between nodes.
   - Supporting branching, looping, and nested flows.
   - Executing triggered branches sequentially (`Flow`) or concurrently (`ParallelFlow`).
   - Supporting nested batch operations.

3. **Communication** happens through the `memory` instance provided to each node's lifecycle methods (in `prep` and `post` methods):

   - **Global Store**: A shared object accessible throughout the flow. Nodes typically write results here.
   - **Local Store**: An isolated object specific to a node and its downstream path.

## Getting Started

If you're new to BrainyFlow, we recommend exploring these core abstractions in the following order:

1. [Node](./node.md) - Understand the basic building block
2. [Flow](./flow.md) - Learn how to connect nodes together
3. [Memory](./memory.md) - See how nodes share data

Once you understand these core abstractions, you'll be ready to implement various [Design Patterns](../design_pattern/index.md) to solve real-world problems.




================================================
File: docs/core_abstraction/node.md
================================================
# Nodes

Nodes are the fundamental building blocks in BrainyFlow. Each node performs a specific task within your workflow, processing data and optionally triggering downstream nodes.

## Node Lifecycle

<div align="center">
  <img src="https://github.com/zvictor/brainyflow/raw/main/.github/media/node.jpg" width="400"/>
</div>

Every node follows a three-phase lifecycle:

1.  **`prep`**: Gather and prepare input data from the `memory` object.
2.  **`exec`**: Perform the main processing task (potentially retried). Receives data from `prep`. **Cannot** access the `memory` object directly.
3.  **`post`**: Process results, update the `memory` object (global or local store), and trigger downstream actions. Receives data from `prep` and `exec`, plus the `memory` instance.

{% hint style="info" %}
**Why 3 steps?** This design enforces separation of concerns.

All steps are **optional**. For example, you can implement only `prep` and `post` if you just need to process data without external computation.
{% endhint %}

### 1. `async prep(memory)`

- Receives the current `memory` instance (a proxy managing global and local stores).
- Extracts necessary data by external sources or by accessing properties on the `memory` object (e.g., `memory.someData`). The proxy reads from the local store first, then the global store.
- Performs any required preprocessing or validation.
- Can optionally return a `PrepResult`, which is passed as input to `exec()` and `post()`.

### 2. `async exec(prepRes)`

- Receives the result from `prep()` (`prepRes`).
- Performs the main computation (e.g., LLM call, API request, calculation).
- ⚠️ **Cannot** access the `Memory` instance directly. This enforces separation and aids retry logic.
- ⚠️ Should ideally be idempotent (produce the same result given the same input) and have no side effects if retries (`maxRetries > 1`) are enabled, as it might be called multiple times.
- Returns an `ExecResult`, which is passed to `post()`.
- _Note:_ The actual execution logic, including retries, is handled by the internal `execRunner` method, which calls this `exec` method.

### 3. `async post(memory, prepRes, execRes)`

- Receives the `memory` instance, the result from `prep()` (`prepRes`), and the result from `exec()` (`execRes`).
- Processes results and writes data back to the `memory` object (usually the global store, e.g., `memory.result = execRes`).
- **This is the only place to call `this.trigger()`** to specify which downstream nodes should run next and optionally pass `forkingData` to their local memory.
  - If no `trigger` is called, the flow proceeds via the `DEFAULT_ACTION` ('default') with no specific `forkingData`.

```mermaid
sequenceDiagram
    participant M as Memory (Proxy)
    participant N as Node

    N->>M: 1. prep(memory): Read from memory
    Note right of N: Return prep_res

    N->>N: 2. exec(prep_res): Compute result
    Note right of N: Return exec_res

    N->>M: 3. post(memory, prep_res, exec_res): Write to memory
    Note right of N: Trigger next actions
```

## Creating Custom Nodes

To create a custom node, extend the `Node` class and implement the lifecycle methods:

```typescript
import { Memory, Node } from 'brainyflow'

class TextProcessorNode extends Node {
  async prep(memory: Memory): Promise<string> {
    // Read input data
    return memory.text
  }

  async exec(text: string): Promise<string> {
    // Process the text
    return text.toUpperCase()
  }

  async post(memory: Memory, input: string, result: string): Promise<void> {
    // Store the result in the global store
    memory.processed_text = result

    // Trigger the default next node (optional)
    this.trigger('default')
  }
}
```

## Error Handling

Nodes include built-in retry capabilities for handling transient failures in `exec()` calls.

You can configure retries via the constructor:

- `maxRetries` (number): Maximum number of attempts for `exec()` (default: 1, meaning no retry).
- `wait` (number): Seconds to wait between retry attempts (default: 0).

The `wait` parameter is specially helpful when you encounter rate-limits or quota errors from your LLM provider and need to back off.
During retries, you can access the current retry count (0-based) via `self.cur_retry` (Python) or `this.curRetry` (TypeScript).

To handle failures gracefully after all retry attempts for `exec()` are exhausted, override the `execFallback` method.

By default, `execFallback` just re-raises the exception. You can override it to return a fallback result instead, which becomes the `exec_res` passed to `post()`, allowing the flow to potentially continue.

```typescript
import { Memory, Node, NodeError } from 'brainyflow'

type PrepResult = void
type ExecResult = string

const myNode = new CustomErrorHandlingNode({ maxRetries: 3, wait: 10 })

class CustomErrorHandlingNode extends Node<any, any, [], PrepResult, ExecResult> {
  async exec(prepRes: PrepResult): Promise<ExecResult> {
    console.log(`Exec attempt: ${this.curRetry + 1}`)
    if (this.curRetry < 2) {
      throw new Error('Temporary failure!')
    }
    return 'Success on retry'
  }

  async execFallback(prepRes: PrepResult, error: NodeError): Promise<ExecResult> {
    // This is called only if exec fails on the last attempt
    console.error(`Exec failed after ${error.retryCount + 1} attempts: ${error.message}`)
    // Return a fallback value instead of re-throwing
    return `Fallback response due to repeated errors: ${error.message}`
  }

  async post(memory: Memory<any, any>, prepRes: PrepResult, execRes: ExecResult): Promise<void> {
    // execRes will be "Success on retry" or "Fallback response..."
    console.log(`Post: Received result '${execRes}'`)
    memory.final_result = execRes
    this.trigger('default')
  }
}
```

## Triggering Successors (`trigger`)

Nodes signal which path(s) the flow should take next by calling `this.trigger()` within their `post` method.

```typescript
this.trigger(actionName: string | typeof DEFAULT_ACTION, forkingData?: SharedStore): void
```

- `actionName`: A string identifying the transition (e.g., 'success', 'failure', 'categoryA'). If omitted or if `trigger` is not called, `DEFAULT_ACTION` ('default') is assumed.
- `forkingData` (optional): An object containing data to be added _only_ to the `local` store of the `memory` instance passed to the triggered successor(s). This allows passing specific data down a particular branch without polluting the global store.

The running [Flow](./flow.md) uses the `actionName` to look up the successor nodes defined using `.on()` or `.next()`.

{% hint style="warning" %}
`trigger()` can **only** be called inside the `post()` method. Calling it elsewhere will throw an error.
{% endhint %}

## Defining Connections (`on`, `next`)

While `trigger` determines _which_ path to take _during_ execution, you define the possible paths _before_ execution, by using either `.next()` or `.on()`, as shown below:

1.  **Basic default transition**: `node_a.next(node_b)`
    This means if `node_a` triggers `"default"`, `node_b` will execute next.

2.  **Named action transition**: `node_a.on('action_name', node_b)` or `node_a.next(node_b, 'action_name')`
    This means if `node_a` triggers `"action_name"`, `node_b` will execute next.

Note that `node_a.next(node_b)` is equivalent to both `node_a.next(node_b, 'default')` and `node_a.on('default', node_b)`. Both methods return the _successor_ node (`node_b` in this case), allowing for chaining.

```typescript
// Basic default transition
node_a.next(node_b) // If node_a triggers "default", go to node_b

// Named action transition
node_a.on('success', node_b) // If node_a triggers "success", go to node_b
node_a.on('error', node_c) // If node_a triggers "error", go to node_c

// Alternative syntax
node_a.next(node_b, 'success') // Same as node_a.on('success', node_b)
```

To summarize it:

- `node.on(actionName, successorNode)`: Connects `successorNode` to be executed when `node` triggers `actionName`.
- `node.next(successorNode, actionName = DEFAULT_ACTION)`: A convenience method, equivalent to `node.on(actionName, successorNode)`.

These methods are typically called when constructing your `Flow`. See the [Flow documentation](./flow.md) for detailed examples of graph construction.

### Example: Conditional Branching

```typescript
import { Flow, Memory, Node } from 'brainyflow'

// Assuming detectLanguage, EnglishProcessorNode, SpanishProcessorNode, UnknownProcessorNode are defined elsewhere

interface RouterGlobalStore {
  content: string
  language?: string
}
type RouterActions = 'english' | 'spanish' | 'unknown' // Add unknown action

class RouterNode extends Node<RouterGlobalStore, any, RouterActions[]> {
  async prep(memory: Memory<RouterGlobalStore, any>): Promise<string> {
    return memory.content
  }

  async exec(content: string | undefined): Promise<string> {
    return await detectLanguage(content)
  }

  async post(
    memory: Memory<RouterGlobalStore, any>,
    prepRes: string, // Content
    execRes: string, // Language detected
  ): Promise<void> {
    console.log(`RouterPost: Detected language '${execRes}', storing and triggering.`)
    memory.language = execRes
    // Trigger the specific action based on the detected language
    this.trigger(execRes as RouterActions)
  }
}

// --- Flow Definition ---
const router = new RouterNode()
const englishProcessor = new EnglishProcessorNode()
const spanishProcessor = new SpanishProcessorNode()
const unknownProcessor = new UnknownProcessorNode()

// Define connections for specific actions
router.on('english', englishProcessor)
router.on('spanish', spanishProcessor)
router.on('unknown', unknownProcessor)

const flow = new Flow(router)

// --- Execution Example ---
async function runFlow() {
  const storeEn: RouterGlobalStore = { content: 'Hello world' }
  await flow.run(storeEn)
  console.log('--- English Flow Done ---', storeEn)

  const storeEs: RouterGlobalStore = { content: 'Hola mundo' }
  await flow.run(storeEs)
  console.log('--- Spanish Flow Done ---', storeEs)

  const storeUnk: RouterGlobalStore = { content: 'Bonjour le monde' }
  await flow.run(storeUnk)
  console.log('--- Unknown Flow Done ---', storeUnk)
}
runFlow()
```

### Example: Multiple Triggers (Fan-Out / Batch Processing)

A single node can call `this.trigger()` multiple times within its `post` method to initiate multiple downstream paths simultaneously. Each triggered path receives its own cloned `memory` instance, potentially populated with unique `local` data via the `forkingData` argument.

This "fan-out" capability is the core pattern used for **batch processing** (processing multiple items, often in parallel).

For a detailed explanation and examples of implementing batch processing using this fan-out pattern with `Flow` or `ParallelFlow`, please see the [Flow documentation](./flow.md#implementing-batch-processing-fan-out-pattern).

## Running Individual Nodes

Nodes have an extra method `run(memory)`, which calls `prep → exec → post`. Use it only for **testing or debugging individual nodes in isolation**.

{% hint style="danger" %}
**Do NOT use `node.run()` to execute a workflow.**

`node.run()` executes only the single node it's called on. It **does not** look up or execute any successor nodes defined via `.on()` or `.next()`.

Always use `Flow.run()` or `ParallelFlow.run()` to execute a complete graph workflow. Using `node.run()` directly will lead to incomplete execution.
{% endhint %}

```typescript
// Run with propagate: false (default) - returns ExecResult
async node.run(memory: Memory | GlobalStore): Promise<ExecResult | void> // Accepts Memory object or initial global store

// Run with propagate: true - returns triggers for Flow execution
async node.run(memory: Memory | GlobalStore, propagate: true): Promise<[Action, Memory][]> // Accepts Memory object or initial global store
```

## Best Practices

- **Single Responsibility**: Keep nodes focused on a single, well-defined task.
- **Read in `prep`**: Gather all necessary data from the `memory` object in the `prep` phase. Return only what `exec` needs.
- **Compute in `exec`**: Perform the core computation in `exec`. Keep it free of side effects and direct `memory` object access.
- **Write & Trigger in `post`**: Update the `memory` object (usually the global store) and call `trigger` in the `post` phase.
- **Use `forkingData`**: Pass branch-specific data via `trigger`'s `forkingData` argument to populate the `local` store for successors, keeping the global store clean.
- **Type Safety**: Use TypeScript generics (`Node<G, L, A, P, E>`) to define the expected structure of `memory` stores, actions, and results.
- **Error Handling**: Leverage the built-in retry logic (`maxRetries`, `wait`) and `execFallback` for resilience.




================================================
File: docs/core_abstraction/flow.md
================================================
# Flow: Orchestrating Nodes in a Directed Graph

A **Flow** orchestrates a graph of Nodes, connecting them through action-based transitions. Flows enable you to create complex application logic including sequences, branches, loops, and nested workflows.

They manage the execution order, handle data flow between nodes, and provide error handling and cycle detection.

## Creating a Flow

A Flow begins with a **start node**, a memory state, and follows the [action-based transitions defined by the nodes](./nodes.md) until it reaches a node with no matching transition for its returned action.

```typescript
import { Flow, Node } from 'brainyflow'

// Define nodes and transitions
// const node_a = new Node(); ... etc.
node_a.next(node_b) // Default transition
node_b.on('success', node_c) // Named transition
node_b.on('error', node_d) // Named transition

// Define the expected Global Store structure (optional but recommended)
interface MyGlobalStore {
  input?: any
  result?: any
  error?: any
}

// Create a global store object (can be just an empty object)
const globalStore: MyGlobalStore = { input: 'some data' }

// Create flow starting with node_a
const flow = new Flow<MyGlobalStore>(node_a)

// Run the flow, passing the global store object.
// The flow modifies the globalStore object in place.
await flow.run(globalStore)

// Print the final state of the global store
console.log('Flow finished. Final memory state:', globalStore)
// Example output (depending on flow logic):
// { input: 'some data', result: 'processed data from node_c' }
// or
// { input: 'some data', error: 'error details from node_d' }
```

## Flow Execution Process

When you call `flow.run(memory)`, the flow executes the following steps internally:

1.  It starts with the designated `start` node.
2.  For the current node, it executes its lifecycle (`prep` -> `execRunner` -> `post`), passing a `Memory` instance that wraps the global store and manages local state.
3.  It looks for triggered action (specified by `trigger()` calls), then it finds the corresponding successor node(s) defined by `.on()` or `.next()`.
4.  It recursively executes the successor node(s) with their respective cloned local memories.
5.  This process repeats until it reaches nodes that trigger actions with no defined successors, or the flow completes.
6.  The `run` method modifies the initial global store object in place and returns a nested structure representing the execution tree (often ignored if results are stored in memory).

```mermaid
sequenceDiagram
    participant S as Shared Store
    participant F as Flow
    participant N1 as Node A
    participant N2 as Node B

    F->>N1: Execute Node A
    N1->>S: Read from shared store
    N1->>N1: Perform computation
    N1->>S: Write to shared store
    N1-->>F: Return action "default"

    F->>F: Find next node for action "default"
    F->>N2: Execute Node B
    N2->>S: Read from shared store
    N2->>N1: Read from local store
    N2->>N2: Perform computation
    N2->>S: Write to shared store
    N2-->>F: Return action "success"

    F->>F: No transition defined for "success"
    F-->>F: Flow execution complete
```

## Controlling Flow Execution

### Branching and Looping

Flows support complex patterns like branching (conditionally following different paths) and looping (returning to previous nodes).

#### Example: Expense Approval Flow

Here's a simple expense approval flow that demonstrates branching and looping:

```typescript
import { Flow, Node } from 'brainyflow'

// Define the nodes first
// const review = new ReviewExpenseNode()
// const revise = new ReviseReportNode()
// const payment = new ProcessPaymentNode()
// const finish = new FinishProcessNode()

// Define the flow connections
review.on('approved', payment) // If approved, process payment
review.on('needs_revision', revise) // If needs changes, go to revision
review.on('rejected', finish) // If rejected, finish the process

revise.next(review) // After revision (default trigger), go back for another review
payment.next(finish) // After payment (default trigger), finish the process

// Create the flow, starting with the review node
const expenseFlow = new Flow(review)
```

This flow creates the following execution paths:

1. If `review` triggers `"approved"`, the expense moves to the `payment` node
2. If `review` triggers `"needs_revision"`, it goes to the `revise` node, which then loops back to `review`
3. If `review` triggers `"rejected"`, it moves to the `finish` node and stops

```mermaid
flowchart TD
    review[Review Expense] -->|approved| payment[Process Payment]
    review -->|needs_revision| revise[Revise Report]
    review -->|rejected| finish[Finish Process]

    revise --> review
    payment --> finish
```

### Flow as a Node

Every `Flow` is in fact a specialized type of `Node`. This means a `Flow` itself can be used as a node within another, larger `Flow`, enabling powerful composition and nesting patterns.

{% hint style="info" %}
The difference from Node is that `Flow` has a specialized `execRunner()` method - _the internal caller of `exec()`_ - that orchestrates its internal nodes and which cannot be overridden.
As such:

- A `Flow`'s primary role is orchestration, not direct computation like a standard `Node`.
- You cannot override `exec` or `execRunner` in a `Flow`.
- It still has the `prep` and `post` lifecycle methods, which you _can_ override if you need to perform setup before the sub-flow runs or cleanup/processing after it completes.
- When a `Flow` used as a node finishes its internal execution, it triggers its _own_ successors in the parent flow based on its `post` method's `trigger` calls (or the default action).

{% endhint %}

This allows you to:

1. Break down complex applications into manageable sub-flows
2. Reuse flows across different applications
3. Create hierarchical workflows with clear separation of concerns

#### Example: Order Processing Pipeline

Here's a practical example that breaks down order processing into nested flows:

```typescript
import { Flow, Node } from 'brainyflow'

// Payment processing sub-flow
validatePayment.next(processPayment).next(paymentConfirmation)
const paymentFlow = new Flow(validatePayment)

// Inventory sub-flow
checkStock.next(reserveItems).next(updateInventory)
const inventoryFlow = new Flow(checkStock)

// Shipping sub-flow
createLabel.next(assignCarrier).next(schedulePickup)
const shippingFlow = new Flow(createLabel)

paymentFlow.next(inventoryFlow) // Default transition after paymentFlow completes
inventoryFlow.next(shippingFlow) // Default transition after inventoryFlow completes

// Create the master flow, starting with the paymentFlow
const orderPipeline = new Flow(paymentFlow)

// --- Run the entire pipeline ---
// const globalStore = { orderId: 'XYZ789', customerId: 'CUST123' };
// await orderPipeline.run(globalStore);
// console.log('Order pipeline completed. Final state:', globalStore);
```

This creates a clean separation of concerns while maintaining a clear execution path:

```mermaid
flowchart LR
    subgraph order_pipeline[Order Pipeline]
        subgraph paymentFlow["Payment Flow"]
            A[Validate Payment] --> B[Process Payment] --> C[Payment Confirmation]
        end

        subgraph inventoryFlow["Inventory Flow"]
            D[Check Stock] --> E[Reserve Items] --> F[Update Inventory]
        end

        subgraph shippingFlow["Shipping Flow"]
            G[Create Label] --> H[Assign Carrier] --> I[Schedule Pickup]
        end

        paymentFlow --> inventoryFlow
        inventoryFlow --> shippingFlow
    end
```

### Cycle Detection

Loops are created by connecting a node back to a previously executed node. To prevent infinite loops, `Flow` includes cycle detection controlled by the `maxVisits` option in its constructor (default is 5). If a node is visited more times than `maxVisits` during a single run, an error is thrown.

{% hint style="success" %}
This ensures that the flow does not get stuck in an infinite loop.
{% endhint %}

```typescript
// Limit the number of times a node can be visited within this flow
const flow = new Flow(startNode, { maxVisits: 10 })
```

- The default value for `maxVisits` is `5`.
- Set `maxVisits` to `Infinity` or a very large number for effectively no limit (use with caution!).

## Flow Parallelism

BrainyFlow offers two built-in types - _and the possibility to create custom ones_ - of flows that provide different levels of parallelism when nodes trigger multiple successors:

### 1. `Flow` (Sequential Execution)

The default `Flow` class executes the tasks generated by multiple triggers **sequentially**. It waits for the entire branch initiated by the first trigger to complete before starting the branch for the second trigger, and so on.

```typescript
const sequentialFlow = new Flow(startNode)
```

### 2. `ParallelFlow` (Concurrent Execution)

The `ParallelFlow` class executes the tasks generated by multiple triggers **concurrently** using `Promise.all()` (TypeScript) or `asyncio.gather()` (Python). This is useful for performance when branches are independent (e.g., batch processing items).

```typescript
// Executes triggered branches in parallel
const parallelFlow = new ParallelFlow(startNode)
```

Use `ParallelFlow` when:

1.  A node needs to "fan-out" work into multiple independent branches (e.g., processing items in a batch).
2.  These branches do not have dependencies on each other's immediate results (though they might all write back to the shared global memory).
3.  You want to potentially speed up execution by running these independent branches concurrently.

{% hint style="danger" %}

When using `ParallelFlow`, be mindful of potential race conditions if multiple parallel branches try to modify the same property in the global `Memory` simultaneously without proper synchronization. Often, it's safer for parallel branches to accumulate results locally or use unique keys in the global store, followed by a final sequential aggregation step.

{% endhint %}

### Custom Execution Logic (Overriding `runTasks`)

For more advanced control over how triggered branches are executed, you can extend `Flow` (or `ParallelFlow`) and override the `runTasks` method. This method receives an array of functions, where each function represents the execution of one triggered branch.

```typescript
import { Flow, Memory } from 'brainyflow'

declare function sleep(ms: number): Promise<void> // Assuming sleep is available

class CustomExecutionFlow extends Flow {
  async runTasks<T>(tasks: (() => Promise<T>)[]): Promise<Awaited<T>[]> {
    // Example: Run tasks sequentially with a delay between each
    const results: Awaited<T>[] = []
    for (const task of tasks) {
      results.push(await task())
      console.log('Custom runTasks: Task finished, waiting 1s...')
      await sleep(1000) // Wait 1 second between tasks
    }
    return results
  }
}
```

## Batch Processing (Fan-Out Pattern)

The standard way to process multiple items (sequentially or in parallel) is using the "fan-out" pattern with multiple triggers. All you need for that is a trigger node and a processor node:

1.  **Trigger Node**:
    - A standard `Node` whose `post` method iterates through your items (e.g., from `prepRes.items`).
    - For each item, it calls `this.trigger` with forking data. E.g., `this.trigger("process_item", { item: current_item, index: i })`.
    - The forked data is crucial for passing item-specific data to the local memory of the next node.
2.  **Processor Node**:
    - Another standard `Node` connected via the action triggered by the `TriggerNode`. E.g. `triggerNode.on("process_item", processorNode)`.
    - Its `prep` reads the forked data from its local memory.
    - Its `exec` performs the actual processing for that single item.
    - Its `post` typically stores the result back into global memory (e.g., `memory.results[prepRes.index] = execRes.result`).
3.  **Flow Choice**:
    - Use `Flow(triggerNode)` for **sequential** batch processing (one item completes before the next starts).
    - Use `ParallelFlow(triggerNode)` for **concurrent** batch processing (all items are processed in parallel).

This pattern leverages the core `Flow` and `Node` abstractions to handle batching effectively.

#### Example: Simple Batch Processing

```typescript
interface BatchGlobalStore {
  items_to_process?: any[]
  results?: any[]
}
interface BatchLocalStore {
  item_data?: any
  result_index?: number
}

class TriggerBatchNode extends Node<
  BatchGlobalStore,
  BatchLocalStore,
  ['process_one', 'aggregate']
> {
  async prep(memory: Memory<BatchGlobalStore>): Promise<any[]> {
    return memory.items_to_process ?? []
  }
  async post(memory: Memory<BatchGlobalStore>, items: any[], execRes: void): Promise<void> {
    memory.results = new Array(items.length).fill(null) // Pre-allocate
    items.forEach((item, index) => {
      this.trigger('process_one', { item_data: item, result_index: index })
    })
    // Optional: this.trigger("aggregate");
  }
}

class ProcessOneItemNode extends Node<BatchGlobalStore, BatchLocalStore> {
  async prep(
    memory: Memory<BatchGlobalStore, BatchLocalStore>,
  ): Promise<{ item: any; index: number }> {
    return { item: memory.item_data, index: memory.result_index ?? -1 }
  }
  async exec(prepRes: { item: any; index: number }): Promise<{ result: any; index: number }> {
    const result = `Processed ${prepRes.item}` // Placeholder
    return { result, index: prepRes.index }
  }
  async post(
    memory: Memory<BatchGlobalStore>,
    prepRes: any,
    execRes: { result: any; index: number },
  ): Promise<void> {
    if (!memory.results) memory.results = []
    while (memory.results.length <= execRes.index) {
      memory.results.push(null)
    }
    memory.results[execRes.index] = execRes.result
  }
}

// Setup
const trigger = new TriggerBatchNode()
const processor = new ProcessOneItemNode()
trigger.on('process_one', processor)

// Choose Flow type
// const sequentialBatchFlow = new Flow<BatchGlobalStore>(trigger);
const parallelBatchFlow = new ParallelFlow<BatchGlobalStore>(trigger)

// Run
// async function run() {
//   const memory: BatchGlobalStore = { items_to_process: ["A", "B", "C"] };
//   await parallelBatchFlow.run(memory);
//   console.log(memory.results); // Output: ['Processed A', 'Processed B', 'Processed C']
// }
// run();
```

In this example, `TriggerBatchNode` fans out the work. If run with `ParallelFlow`, each `ItemProcessorNode` instance would execute concurrently. If run with a standard `Flow`, they would execute sequentially.

## Best Practices

- **Start Simple**: Begin with a linear flow and add branching/looping complexity gradually.
- **Visualize First**: Sketch your flow diagram (using Mermaid or similar tools) before coding to clarify logic.
- **Flow Modularity**: Design flows as reusable components. Break down complex processes into smaller, nested sub-flows.
- **Memory Planning**: Define clear interfaces for your `GlobalStore` and `LocalStore` upfront. Decide what state needs to be global versus what can be passed locally via `forkingData`.
- **Action Naming**: Use descriptive, meaningful action names (e.g., 'user_clarification_needed', 'data_validated') instead of generic names like 'next' or 'step2'.
- **Explicit Transitions**: Clearly define transitions for all expected actions a node might trigger. Consider adding default `.next()` transitions for unexpected or completion actions.
- **Cycle Management**: Be mindful of loops. Use the `maxVisits` option in the `Flow` constructor to prevent accidental infinite loops.
- **Error Strategy**: Decide how errors should propagate. Should a node's `execFallback` handle errors and allow the flow to continue, or should errors terminate the flow? Define specific error actions (`node.on('error', errorHandlerNode)`) if needed.
- **Parallelism Choice**: Use `ParallelFlow` when branches are independent and can benefit from concurrent execution. Stick with `Flow` (sequential) if branches have dependencies or shared resource contention.
- **Memory Isolation**: Leverage `forkingData` in `trigger` calls to pass data down specific branches via the `local` store, keeping the `global` store cleaner. This is crucial for parallel execution.
- **Test Incrementally**: Test individual nodes using `node.run()` and test sub-flows before integrating them into larger pipelines.
- **Avoid Deep Nesting**: While nesting flows is powerful, keep the hierarchy reasonably flat (e.g., 2-3 levels deep) for maintainability.

Flows provide the orchestration layer that determines how your nodes interact, ensuring that data moves predictably through your application and that execution follows your intended paths.

By following these principles, you can create complex, maintainable AI applications that are easy to reason about and extend.




================================================
File: docs/core_abstraction/memory.md
================================================
# Memory: Managing State Between Nodes

BrainyFlow provides a streamlined approach for components to communicate with each other. This chapter explains how data is stored, accessed, and isolated.

## Memory Scopes: Global vs. Local

Each `Memory` instance encapsulates two distinct scopes:

1.  **Global Store (`memory`)**: A single object shared across _all_ nodes within a single `flow.run()` execution. Changes made here persist throughout the flow. Think of it as the main shared state.
2.  **Local Store (`memory.local`)**: An object specific to a particular execution path within the flow. It's created when a node `trigger`s a successor. Changes here are isolated to that specific branch and its descendants.

This dual-scope system allows for both shared application state (global) and controlled, path-specific data propagation (local).

{% hint style="success" %}
**Real-World Analogies**:

Think of the memory system like **a river delta**:

- **Global Store**: The main river water that flows everywhere
- **Local Store**: Specific channels that might carry unique properties that only affect downstream areas fed by that channel

Alternatively, think of it like **nested scopes in programming**:

- **Global Store**: Like variables declared in the outermost scope of a program, accessible everywhere.
- **Local Store**: Like variables declared inside a function or block. They are only accessible within that block and any nested blocks (downstream nodes in the flow). If a local variable has the same name as a global one, the local variable "shadows" the global one within its scope.

This model gives you the flexibility to share data across your entire flow (global) or isolate context to specific execution paths (local).
{% endhint %}

## Accessing Memory (Reading)

Nodes access data stored in either scope through the `memory` proxy instance passed to their `prep` and `post` methods. When you read a property (e.g., `memory.someValue`), the proxy automatically performs a lookup:

1.  It checks the **local store (`memory.local`)** first.
2.  If the property is not found locally, it checks the **global store (`memory`)**.

```typescript
import { Memory, Node } from 'brainyflow'

interface MyGlobal {
  config?: object
  commonData?: string
  pathSpecificData?: string
}
interface MyLocal {
  pathSpecificData?: string
} // Can shadow global

class MyNode extends Node<MyGlobal, MyLocal> {
  async prep(memory: Memory<MyGlobal, MyLocal>): Promise<void> {
    // Reads from global store (assuming not set locally)
    const config = memory.config
    const common = memory.commonData

    // Reads from local store if set via forkingData, otherwise reads from global
    const specific = memory.pathSpecificData
  }
  // ... exec, post ...
}
```

When accessing memory, you should always use `memory.someValue` and let the `Memory` manager figure out where to fetch the value for you.
You could also directly access the entire local store object using `memory.local` - or a value at `memory.local.someValue` - but that's an anti-pattern that should be avoided.

```typescript
async post(memory: Memory<MyGlobal, MyLocal>, /*...*/) {
    const allLocalData = memory.local; // Access the internal __local object directly
    console.log('Current local store:', allLocalData);
}
```

## Writing to Memory

- **Writing to Global Store**: Assigning a value directly to a property on the `memory` object (e.g., `memory.someValue = 'new data'`) writes to the **global store**. The proxy automatically removes the property from the local store first if it exists there.
- **Writing to Local Store**: While possible via `memory.local.someValue = 'new data'`, the primary and recommended way to populate the local store for downstream nodes is using the `forkingData` argument in `this.trigger()`.

```typescript
import { Memory, Node } from 'brainyflow'

interface MyGlobal {
  fileList?: string[]
  results?: Record<string, any>
}
interface MyLocal {
  processedCount?: number // Data passed via memory.local assignment
  file?: string // Data passed via forkingData
}

class DataWriterNode extends Node<MyGlobal, MyLocal, ['process_file']> {
  async post(
    memory: Memory<MyGlobal, MyLocal>,
    prepRes: any,
    execRes: { files: string[]; count: number }, // Assume exec returns this format
  ): Promise<void> {
    // --- Writing to Global Store ---
    // Accessible to all nodes in the flow
    memory.fileList = execRes.files
    console.log(`Memory updated globally: fileList=${memory.fileList}`)

    // --- Writing to Local Store ---
    // Accessible to this node and all descendants
    memory.local.processedCount = execRes.count
    console.log(`Memory updated locally: processedCount=${memory.processedCount}`)

    // --- Triggering with Local Data (Forking Data) ---
    // 'file' will be added to the local store of the memory clone
    // passed to the node(s) triggered by the 'process_file' action.
    for (const file of execRes.files) {
      this.trigger('process_file', { file: file })
    }
  }
}

// Example Processor Node (triggered by 'process_file')
class FileProcessorNode extends Node<MyGlobal, MyLocal> {
  async prep(memory: Memory<MyGlobal, MyLocal>): Promise<string | undefined> {
    // Reads 'file' from the local store first, then global
    const fileToProcess = memory.file
    console.log(`Processing file (from local memory): ${fileToProcess}`)
    return fileToProcess
  }
  // ... exec, post ...
}
```

## Best Practices

- **Read in `prep()`**: Gather necessary input data from `memory` at the beginning of a node's execution.
- **Write Global State in `post()`**: Update the shared global store by assigning to `memory` properties (e.g., `memory.results = ...`) in the `post()` phase after processing is complete.
- **Set Local State via `forkingData`**: Pass branch-specific context to successors by providing the `forkingData` argument in `this.trigger()` within the parent's `post()` method. This populates the `local` store for the next node(s).
- **Read Transparently**: Always read data via the `memory` proxy (e.g., `memory.someValue`). It handles the local-then-global lookup automatically. Avoid reading directly from `memory.local` or other internal properties unless strictly needed.

## When to Use The Memory

- **Ideal for**: Sharing data results, large content, or information needed by multiple components
- **Benefits**: Separates data from computation logic (separation of concerns)
- **Global Memory**: Use for application-wide state, configuration, and final results
- **Local Memory**: Use for passing contextual data down a specific execution path

## Technical Concepts

The memory system in BrainyFlow implements several established computer science patterns:

- **Lexical Scoping**: Local memory "shadows" global memory, similar to how local variables in functions can shadow global variables
- **Context Propagation**: Local memory propagates down the execution tree, similar to how context flows in React or middleware systems
- **Transparent Resolution**: The system automatically resolves properties from the appropriate memory scope

## Remember

1.  **Reading**: Always read via the `memory` proxy (e.g., `memory.value`). It checks local then global.
2.  **Writing**: Direct assignment `memory.property = value` writes to the **global** store.
3.  **Local State Creation**: Use `trigger(action, forkingData)` in `post()` to populate the `local` store for the _next_ node(s) in a specific branch.
4.  **Lifecycle**: Read from `memory` in `prep`, compute in `exec` (no memory access), write global state to `memory` and trigger successors (potentially with `forkingData` for local state) in `post`.




================================================
File: docs/design_pattern/index.md
================================================
# Design Patterns

BrainyFlow supports a variety of design patterns that enable you to build complex AI applications. These patterns leverage the core abstractions of nodes, flows, and shared store to implement common AI system architectures.

## Overview of Design Patterns

<div align="center">
  <img src="https://raw.githubusercontent.com/zvictor/brainyflow/main/.github/media/design.jpg" width="1300"/>
</div>

BrainyFlow's minimalist design allows it to support various high-level AI design paradigms:

| Pattern                             | Description                                                     | Use Cases                                       |
| ----------------------------------- | --------------------------------------------------------------- | ----------------------------------------------- |
| [RAG](./rag.md)                     | Retrieval-Augmented Generation for knowledge-grounded responses | Question answering, knowledge-intensive tasks   |
| [Agent](./agent.md)                 | Autonomous entities that can perceive, reason, and act          | Virtual assistants, autonomous decision-making  |
| [Workflow](./workflow.md)           | Sequential or branching business processes                      | Form processing, approval flows                 |
| [MapReduce](./map_reduce.md)        | Distributed processing of large datasets                        | Document summarization, parallel processing     |
| [Structured Output](./structure.md) | Generating outputs that follow specific schemas                 | Data extraction, configuration generation       |
| [Multi-Agents](./multi_agent.md)    | Multiple agents working together on complex tasks               | Collaborative problem-solving, role-based tasks |

## Choosing the Right Pattern

When designing your application, consider these factors when selecting a pattern:

| Pattern           | Best For                  | When To Use                                             |
| ----------------- | ------------------------- | ------------------------------------------------------- |
| RAG               | Knowledge-intensive tasks | When external information is needed for responses       |
| Agent             | Dynamic problem-solving   | When tasks require reasoning and decision-making        |
| Workflow          | Sequential processing     | When steps are well-defined and follow a clear order    |
| Map Reduce        | Large data processing     | When handling datasets too large for a single operation |
| Structured Output | Consistent formatting     | When outputs need to follow specific schemas            |
| Multi-Agents      | Complex collaboration     | When tasks benefit from specialized agent roles         |

### Decision Tree

Use this decision tree to help determine which pattern best fits your use case:

```mermaid
flowchart TD
    A[Start] --> B{Need to process large data?}
    B -->|Yes| C{Data can be processed independently?}
    B -->|No| D{Need to make decisions?}

    C -->|Yes| E[Map Reduce]
    C -->|No| F[Workflow]

    D -->|Yes| G{Complex, multi-step reasoning?}
    D -->|No| H[Simple Workflow]

    G -->|Yes| I{Need multiple specialized roles?}
    G -->|No| J{Need external knowledge?}

    I -->|Yes| K[Multi-Agents]
    I -->|No| L[Agent]

    J -->|Yes| M[RAG]
    J -->|No| N[Structured Output]
```

## Pattern Composition

BrainyFlow's nested flow capability allows you to compose multiple patterns. For instance:

```mermaid
graph TD
    subgraph "Agent Pattern"
        A[Perceive] --> B[Think]
        B --> C[Act]
        C --> A
    end

    subgraph "RAG Pattern"
        D[Query] --> E[Retrieve]
        E --> F[Generate]
    end

    A --> D
    F --> B
```

This composition enables powerful applications that combine the strengths of different patterns.
For example, an agent might use RAG to access knowledge, then apply chain-of-thought reasoning to solve a problem.
Other examples include:

- An **Agent** that uses **RAG** to retrieve information before making decisions
- A **Workflow** that includes **Map Reduce** steps for processing large datasets
- **Multi-Agents** that each use **Structured Output** for consistent communication

## Implementation Examples

Each pattern can be implemented using BrainyFlow's core abstractions. Here's a simple example of the agent pattern:

```typescript
import { Flow, Node } from 'brainyflow'

// Define the agent's components (assuming these classes exist)
const perceive = new PerceiveNode()
const think = new ThinkNode()
const act = new ActNode()

// Connect them in a cycle
perceive.next(think).next(act).next(perceive)

// Create the agent flow
const agentFlow = new Flow(perceive)
```

For more detailed implementations of each pattern, see the individual pattern documentation pages.

## Best Practices

1. **Start Simple**: Begin with the simplest pattern that meets your needs
2. **Modular Design**: Design patterns to be composable and reusable
3. **Clear Interfaces**: Define clear interfaces between pattern components
4. **Test Incrementally**: Test each pattern component before integration
5. **Monitor Performance**: Watch for bottlenecks in your pattern implementation

By understanding and applying these design patterns, you can build sophisticated AI applications that are both powerful and maintainable.




================================================
File: docs/design_pattern/agent.md
================================================
# Agent

Agent is a powerful design pattern in which nodes can take dynamic actions based on the context.

<div align="center">
  <img src="https://github.com/zvictor/brainyflow/raw/main/.github/media/agent.png?raw=true" width="350"/>
</div>

## Implement Agent with Graph

1. **Context and Action:** Implement nodes that supply context and perform actions.
2. **Branching:** Use branching to connect each action node to an agent node. Use action to allow the agent to direct the [flow](../core_abstraction/flow.md) between nodes—and potentially loop back for multi-step.
3. **Agent Node:** Provide a prompt to decide action—for example:

```typescript
;`### CONTEXT
Task: ${taskDescription}
Previous Actions: ${previousActions}
Current State: ${currentState}

### ACTION SPACE
[1] search
  Description: Use web search to get results
  Parameters:
    - query (string): What to search for

[2] answer
  Description: Conclude based on the results  
  Parameters:
    - result (string): Final answer to provide

### NEXT ACTION
Decide the next action based on the current context and available action space.
Return your response in the following format:

\`\`\`yaml
thinking: |
    <your step-by-step reasoning process>
action: <action_name>
parameters:
    <parameter_name>: <parameter_value>
\`\`\``
```

The core of building **high-performance** and **reliable** agents boils down to:

1. **Context Management:** Provide _relevant, minimal context._ For example, rather than including an entire chat history, retrieve the most relevant via [RAG](./rag.md). Even with larger context windows, LLMs still fall victim to ["lost in the middle"](https://arxiv.org/abs/2307.03172), overlooking mid-prompt content.

2. **Action Space:** Provide _a well-structured and unambiguous_ set of actions—avoiding overlap like separate `read_databases` or `read_csvs`. Instead, import CSVs into the database.

## Example Good Action Design

- **Incremental:** Feed content in manageable chunks (500 lines or 1 page) instead of all at once.

- **Overview-zoom-in:** First provide high-level structure (table of contents, summary), then allow drilling into details (raw texts).

- **Parameterized/Programmable:** Instead of fixed actions, enable parameterized (columns to select) or programmable (SQL queries) actions, for example, to read CSV files.

- **Backtracking:** Let the agent undo the last step instead of restarting entirely, preserving progress when encountering errors or dead ends.

## Example: Search Agent

This agent:

1. Decides whether to search or answer
2. If searches, loops back to decide if more search needed
3. Answers when enough context gathered

````typescript
import { Flow, Memory, Node } from 'brainyflow'

// Assume callLLM and parseYaml are defined elsewhere
declare function callLLM(prompt: string): Promise<string>
declare function parseYaml(text: string): any

class DecideAction extends Node {
  async prep(memory: Memory): Promise<{ query: string; context: any }> {
    // Read from memory
    const context = memory.context ?? 'No previous search'
    const query = memory.query
    return { query, context }
  }

  async exec(prepRes: { query: string; context: any }): Promise<any> {
    const { query, context } = prepRes
    const prompt = `
Given input: ${query}
Previous search results: ${JSON.stringify(context)}
Should I: 1) Search web for more info ('search') 2) Answer with current knowledge ('answer')
Output in yaml:
\`\`\`yaml
action: search | answer
reason: <why this action>
search_term: <search phrase if action is search>
\`\`\``
    const resp = await callLLM(prompt)
    // Simplified parsing/validation for docs
    const yamlStr = resp.split(/```(?:yaml)?/)[1]?.trim()
    if (!yamlStr) {
      throw new Error('Missing YAML response')
    }

    const result = parseYaml(yamlStr)

    if (typeof result !== 'object' || !result) {
      throw new Error('Invalid YAML response')
    }
    if (!('action' in result)) {
      throw new Error('Missing action in response')
    }
    if (!('reason' in result)) {
      throw new Error('Missing reason in response')
    }
    if (!['search', 'answer'].includes(result.action)) {
      throw new Error('Invalid action value')
    }
    if (result.action === 'search' && !('search_term' in result)) {
      throw new Error('Missing search_term for search action')
    }

    return result
  }

  async post(memory: Memory, prepRes: any, execRes: any): Promise<void> {
    // Write search term if needed
    if (execRes.action === 'search' && execRes.search_term) {
      memory.search_term = execRes.search_term
    }
    // Trigger the decided action
    this.trigger(execRes.action)
  }
}
````

```typescript
import { Memory, Node } from 'brainyflow'

declare function searchWeb(query: string): Promise<any> // Assume `searchWeb` is defined elsewhere

class SearchWeb extends Node {
  async prep(memory: Memory): Promise<string> {
    // Read search term from memory
    return memory.search_term
  }

  async exec(searchTerm: string): Promise<any> {
    return await searchWeb(searchTerm)
  }

  async post(memory: Memory, prepRes: string, execRes: any): Promise<void> {
    // Add search result to context (simplified)
    const prevContext = memory.context ?? []
    memory.context = [...prevContext, { term: prepRes, result: execRes }]
    // Trigger loop back to decide
    this.trigger('decide')
  }
}
```

```typescript
import { Memory, Node } from 'brainyflow'

declare function callLLM(prompt: string): Promise<string> // Assuming callLLM is defined elsewhere

class DirectAnswer extends Node {
  async prep(memory: Memory): Promise<{ query: string; context: any }> {
    // Read query and context
    return { query: memory.query, context: memory.context ?? 'No context' }
  }

  async exec(prepRes: { query: string; context: any }): Promise<string> {
    // Generate answer based on context
    const prompt = `Context: ${JSON.stringify(prepRes.context)}\nAnswer Query: ${prepRes.query}`
    return await callLLM(prompt)
  }

  async post(memory: Memory, prepRes: any, execRes: string): Promise<void> {
    // Store final answer
    memory.answer = execRes
    console.log(`Answer: ${execRes}`)
    // No trigger needed - end of this path
  }
}
```

```typescript
import { Flow } from 'brainyflow'

// Assuming DecideAction, SearchWeb, DirectAnswer classes are defined

// Instantiate nodes
const decide = new DecideAction()
const search = new SearchWeb()
const answer = new DirectAnswer()

// Define transitions
decide.on('search', search)
decide.on('answer', answer)
search.on('decide', decide) // Loop back

// Create the flow
const agentFlow = new Flow(decide)

// --- Main execution function ---
async function runAgent() {
  const initialMemory = { query: 'Who won the Nobel Prize in Physics 2024?' }
  console.log(`Starting agent flow with query: "${initialMemory.query}"`)

  try {
    await agentFlow.run(initialMemory) // Run the flow with memory object
    console.log('\n--- Flow Complete ---')
    console.log('Final Memory State:', initialMemory) // Log final memory state
    console.log('\nFinal Answer:', initialMemory.answer ?? 'No answer found')
  } catch (error) {
    console.error('\n--- Agent Flow Failed ---', error)
    console.error('Memory State on Failure:', initialMemory) // Log memory state on failure
  }
}

// Run the main function
runAgent()
```




================================================
File: docs/utility_function/index.md
================================================
---
machine-display: true
---

# Utility Functions

BrainyFlow does not provide built-in utilities. Instead, we offer examples that you can implement yourself. This approach gives you more flexibility and control over your project's dependencies and functionality.

## Available Utility Function Examples

1. [LLM Wrapper](./llm.md): Interact with Language Models
2. [Web Search](./websearch.md): Perform web searches
3. [Chunking](./chunking.md): Split large texts into manageable chunks
4. [Embedding](./embedding.md): Generate vector embeddings for text
5. [Vector Databases](./vector.md): Store and query vector embeddings
6. [Text-to-Speech](./text_to_speech.md): Convert text to speech

## Why Not Built-in?

We believe it's a bad practice to include vendor-specific APIs in a general framework for several reasons:

1. **API Volatility**: Frequent changes in external APIs lead to heavy maintenance for hardcoded APIs.
2. **Flexibility**: You may want to switch vendors, use fine-tuned models, or run them locally.
3. **Optimizations**: Prompt caching, batching, and streaming are easier to implement without vendor lock-in.

## Implementing Utility Functions

When implementing utility functions for your BrainyFlow project:

1. Create a separate file for each utility function in the `utils/` directory.
2. Include a simple test or example usage in each file.
3. Document the input/output and purpose of each utility function.

Example structure:

```
my_project/
├── utils/
│   ├── callLlm.ts
│   ├── searchWeb.ts
│   └── embedText.ts
└── ...
```

By following this approach, you can easily maintain and update your utility functions as needed, without being constrained by the framework's built-in utilities.




================================================
File: docs/guides/best_practices.md
================================================
# BrainyFlow Best Practices

## Node Design

1.  **Keep Nodes Focused**: Each node should perform a single, well-defined task.
2.  **Idempotent Execution**: If using retries (`maxRetries > 1`), design the `exec()` method to be idempotent (produce the same result for the same input) as it might be called multiple times.
3.  **Clear Lifecycle**: Use `prep` to read/prepare data, `exec` for computation (no memory access), and `post` to write results and trigger successors.
4.  **Graceful Degradation**: Implement `execFallback` in `Node` subclasses to handle errors gracefully after all retries are exhausted, potentially returning a default value instead of throwing an error.

## Memory (State) Management

1.  **Schema Design**: Define clear interfaces (in TypeScript) or conventions (in Python) for your `GlobalStore` and `LocalStore` structures.
2.  **Global vs. Local**: Use the `GlobalStore` (accessed via `memory.prop = value`) for state shared across the entire flow. Use the `LocalStore` (populated via `forkingData` in `trigger`) for context specific to a particular execution branch.
3.  **Minimize Global State**: Prefer passing data locally via `forkingData` when possible to keep the global state clean and reduce potential conflicts, especially in parallel flows.
4.  **Read Transparently**: Always read via the `memory` proxy (e.g., `memory.value`); it handles the local-then-global lookup.

## Flow Design

1.  **Visualization First**: Sketch your flow diagram (e.g., using Mermaid) before coding to clarify logic and transitions.
2.  **Modularity**: Break complex processes into smaller, potentially nested, sub-flows (`Flow` extends `BaseNode`).
3.  **Explicit Transitions**: Clearly define transitions using descriptive action names (`node.on('action', nextNode)`). Consider default paths (`node.next(defaultNode)`).
4.  **Error Paths**: Define explicit transitions for error conditions (e.g., `node.on('error', errorHandlerNode)`) or handle errors within `execFallback`.
5.  **Cycle Management**: Use the `maxVisits` option in the `Flow` constructor to prevent infinite loops.
6.  **Parallelism**: Choose `ParallelFlow` for independent branches that can run concurrently; use `Flow` (sequential) otherwise or if order matters.
7.  **Test Incrementally**: Test individual nodes (`node.run()`) and sub-flows before integrating them.

## Project Structure

A well-organized project structure enhances maintainability and collaboration:

{% tabs %}
{% tab title="TypeScript (simple)" %}

```haskell
my_project/
├── src/
│   ├── main.ts
│   ├── nodes.ts
│   ├── flow.ts
│   └── utils/
│       ├── callLLM.ts
│       └── searchWeb.ts
├── package.json
└── docs/
    └── design.md
```

{% endtab %}
{% tab title="TypeScript (complex)" %}

```haskell
my_complex_project/
├── src/                      # Source code
│   ├── index.ts              # Entry point
│   ├── nodes/                # Node implementations
│   │   ├── index.ts          # Exports all nodes
│   │   ├── inputNodes.ts
│   │   ├── processingNodes.ts
│   │   └── outputNodes.ts
│   ├── flows/                # Flow definitions
│   │   ├── index.ts          # Exports all flows
│   │   └── mainFlow.ts
│   ├── utils/                # Utility functions
│   │   ├── index.ts          # Exports all utilities
│   │   ├── llm.ts
│   │   ├── database.ts
│   │   └── webSearch.ts
│   ├── types/                # Type definitions
│   │   ├── index.ts          # Exports all types
│   │   ├── node.types.ts
│   │   └── flow.types.ts
│   └── config/               # Configuration
│       └── settings.ts
├── dist/                     # Compiled JavaScript
├── tests/                    # Test cases
│   ├── nodes.test.ts
│   └── flows.test.ts
├── package.json              # Dependencies and scripts
└── docs/                     # Documentation
    ├── design.md             # High-level design
    └── api.md                # API documentation
```

{% endtab %}
{% endtabs %}

- **`docs/design.md`**: Contains project documentation for each step designed in [agentic coding](./agentic_coding.md). This should be _high-level_ and _no-code_.
- **`utils/`**: Contains all utility functions.
  - It's recommended to dedicate one file to each API call, for example `call_llm.py` or `search_web.ts`.
  - Each file should also include a `main()` function to try that API call
- **`nodes.py`** or **`nodes.ts`**: Contains all the node definitions.
- **`flow.py`** or **`flow.ts`**: Implements functions that create flows by importing node definitions and connecting them.
- **`main.py`** or **`main.ts`**: Serves as the project's entry point.




================================================
File: docs/guides/agentic_coding.md
================================================
# Agentic Coding with BrainyFlow

<details>

<summary>Notes For AI Assistants</summary>

{% hint style="warning" %}
**AI only**: If you are a human, skip this section.
{% endhint %}

This is your implementation playbook and most important guideline. Strictly follow the instructions listed in this document.

Use the design document as your single source of truth. Never proceed to code without explicit human approval of the design.

1.  start with a small and simple solution
2.  design at a high level (`docs/design.md`) before implementation and do not start coding until the design is approved
3.  once approved, start coding and iterate on the design based on feedback
4.  do not stop coding until the implementation is working as intended and fully complaint with the design document

</details>

In the context of Human-AI Co-Design, agentic coding involves humans providing high-level guidance while AI agents handle implementation details:
It represents a powerful approach to software development where humans are freed up to focus solely in strategic decisions.
This guide will help you create effective design documents that enable successful BrainyFlow implementations.

## The AI Implementation Brief

```mermaid
flowchart TD
    A[Human Request] --> B{AI Asks Questions}
    B --> C[AI Generates Structured Design Draft]
    C --> D{Human Validates/Edits}
    D -->|Approved| E[AI Implements]
    D -->|Needs Changes| B
    E --> F[Continuous Co-Refinement]
```

- **AI-Driven Structuring:** Convert vague requests into technical specifications through dialogue
- **Essentialism:** Only capture requirements that directly impact implementation
- **Living Documentation:** Design evolves organically through implementation insights

Before writing any code, create a comprehensive AI Implementation Brief at `docs/design.md`. This document serves as the foundation for human-AI collaboration and should contain all the essential sections listed below.

### 1. Requirements Definition

Clearly articulate what you're building and why:

- **Problem Statement**: Define the problem being solved in 1-2 sentences
- **User Needs**: Describe who will use this and what they need
- **Success Criteria**: List measurable outcomes that define success
- **Constraints**: Note any technical or business limitations

Example:

```
We need a document processing system that extracts key information from legal contracts,
summarizes them, and stores the results for easy retrieval. This will help our legal
team review contracts 70% faster.
```

### 2. Flow Design

Outline the high-level architecture using BrainyFlow's nested directed graph abstraction:

- **Flow Diagram**: Create a mermaid diagram showing node connections
- **Processing Stages**: Describe each major stage in the flow
- **Decision Points**: Identify where branching logic occurs
- **Data Flow**: Explain how information moves through the system

Example:

```mermaid
graph TD
    A[DocumentLoader] --> B[TextExtractor]
    B --> C[EntityExtractor]
    C --> D[ValidationNode]
    D -->|Valid| E[SummaryGenerator]
    D -->|Invalid| C
    E --> F[DatabaseStorage]
```

### 3. Utility Functions

List all external utilities needed:

- **Function Name**: Clear, descriptive name
- **Purpose**: What the function does
- **Inputs/Outputs**: Expected parameters and return values
- **External Dependencies**: Any APIs or libraries required

Example:

```
extract_entities(text: str) -> dict:
- Purpose: Uses NER to identify entities in text
- Input: Document text string
- Output: Dictionary of entity lists by type
- Dependencies: spaCy NLP library with legal model
```

### 4. Node Design

For each node in your flow, define:

- **Purpose**: One-line description of what the node does
- **Shared Store Access**: What data it reads from and writes to the shared store
- **Lifecycle Implementation**: How `prep`, `exec`, and `post` will be implemented
- **Action Returns**: What actions the node might return to direct flow
- **Error Handling**: How failures will be managed

Example:

```
EntityExtractorNode:
- Purpose: Identifies parties, dates, and monetary values in contract text
- Reads: document_text from shared store
- Writes: entities dictionary to shared store
- Actions: Returns "valid" if entities found, "retry" if processing failed
- Error Handling: Will retry up to 3 times with exponential backoff
```

### 5. Shared Store Schema

Define the structure of your shared store. Using interfaces (TypeScript) or type hints (Python) is highly recommended.

- **Key Namespaces**: Major sections of your shared store (often represented as nested objects or distinct keys).
- **Data Types**: Expected types for each key.
- **Data Flow**: How data evolves through processing (which nodes read/write which keys).

Example:

```typescript
// Define interfaces for the shared store structure
interface InputStore {
  document_path: string
}

interface ProcessingStore {
  document_text: string
  entities: {
    parties: any[]
    dates: any[]
    amounts: any[]
  }
  validation_status: string
}

interface OutputStore {
  summary: string
  storage_id: string
}

// Combine interfaces for the complete global store (if using nested structure conceptually)
interface GlobalStore extends InputStore, ProcessingStore, OutputStore {}

// Or define a flat global store interface (more common in BrainyFlow usage)
interface FlatGlobalStore {
  document_path?: string
  document_text?: string
  entities?: {
    parties: any[]
    dates: any[]
    amounts: any[]
  }
  validation_status?: string
  summary?: string
  storage_id?: string
}

// Conceptual structure (using the flat interface)
const memoryConceptual: FlatGlobalStore = {
  document_path: 'path/to/file.pdf',
  document_text: '',
  entities: {
    parties: [],
    dates: [],
    amounts: [],
  },
  validation_status: '',
  summary: '',
  storage_id: '',
}

// Note: In BrainyFlow, you'd typically pass an object conforming to
// FlatGlobalStore (or a relevant subset) to flow.run() and access
// properties directly, e.g., memory.document_text = "...", const entities = memory.entities;
```

## Best Practices for Your Design Document

1. **Start Simple**: Begin with the minimal viable solution
2. **Be Explicit**: Clearly define all components and their interactions
3. **Visualize Flows**: Use diagrams to illustrate complex relationships
4. **Define Boundaries**: Clarify what's in and out of scope
5. **Consider Edge Cases**: Note how the system handles failures
6. **Be Concise**: Keep it short and to the point; Do not add any other section to the design document other than the ones listed above

## Implementation Process

After completing your design document:

1. **Review with Stakeholders**: Ensure the design meets requirements. If you are the AI agent, ask the user to verify the design document and confirm it fits their needs.
2. **Refine as Needed**: Iterate on the design based on feedback
3. **Hand Off to AI**: Provide the design document to your AI assistant - If you are the AI agent, start working on the solution based on the design document
4. **Collaborative Implementation**: Work with AI to implement the design
5. **Test and Validate**: Verify the implementation against success criteria

Once the AI assistant starts working on the implementation it's common for it to get lazy midway through the implementation.
Whenever it claims to be done, you can give it an extra push with the following prompt:

```markdown
Review the design document and the whole implemented code, then answer the following questions:

- are all stubs and placeholders gone?
- have all requirements and features been implemented?
- have you successfully run the application and all tests?
- what should be implemented next?

If the answer to any question is "no", or you can name something to be implemented next, then you are not done yet.
Go back to working and finish it! Do not stop until you are done.
```

<details>

<summary>Notes For AI Assistants</summary>

{% hint style="warning" %}
**AI only**: If you are a human, skip this section.
{% endhint %}

Be sure you **always** strictly follow these instructions:

1. **Be Patient**: Do not start the implementation until the design document is approved.
2. **Be Brainy**: Import `brainyflow` and use its archictecture to implement the design document! Adhere to the proposed flow, node and shared store design.
3. **Focus on What Matters**: Do not fix types or edge cases until you are certain the core implementation is working as intended.
4. **Be Adaptive**: Continuously refine the design document based on feedback and changes in the implementation, but keep it clear and concise.
5. **Be Compliant**: You must always write code that is compliant with the design document and the system's architecture. Remember to stick to the flow, node and shared store design:
   - prep(memory)
     Read and preprocess data from the memory object.
   - exec(prep_res)
     Execute compute logic. Receives result from prep. Cannot access memory.
   - post(memory, prep_res, exec_res)
     Postprocess results, write data back to the memory object (usually global store), and trigger next actions.
6. **Be Resilient**: You are not done until the implementation is working as intended.
7. **Be Accountable**: Before finishing the implementation, ask yourself the following questions:
   - are all stubs and placeholders gone?
   - have all requirements and features been implemented?
   - have I successfully run the application and all tests?
   - what should be implemented next?
     If the answer to any question is "no", or you can name something to be implemented next, then you are not done yet.
     Go back to working and finish it!

</details>

## Conclusion: Precision Through Structure

This approach ensures all BrainyFlow solutions maintain:

- **Human Focus:** Strategic requirements and validation

- **AI Precision:** Structured implementation targets

- **System Integrity:** Clear component boundaries

By enforcing these four pillars through adaptive dialogue rather than rigid templates, we achieve flexible yet reliable AI system development. The design document becomes a living contract between human intent and AI execution.

You provide your AI assistant with the clear direction needed to implement an effective BrainyFlow solution while maintaining human oversight of the critical design decisions.

Remember: The quality of your design document directly impacts the quality of the implementation. Invest time in creating a comprehensive brief to ensure successful outcomes.




================================================
File: docs/guides/testing.md
================================================
# Testing and Debugging BrainyFlow Applications

Effective testing and debugging are essential for building reliable applications. This guide covers strategies for testing and debugging complex flows, and monitoring applications in production.

## Testing Approaches

BrainyFlow supports multiple testing approaches to ensure your applications work correctly:

### Unit Testing (Nodes)

Individual nodes can be tested in isolation to verify their behavior:

```typescript
import { describe, expect, it, vi } from 'vitest'
import { SummarizeNode } from './SummarizeNode' // Your Node implementation
import { callLLM } from './utils/callLLM' // Your LLM utility

// Mock the LLM utility
vi.mock('./utils/callLLM', () => ({
  callLLM: vi.fn().mockResolvedValue('Short summary.'),
}))

describe('SummarizeNode', () => {
  it('should summarize text correctly', async () => {
    // Create the node instance
    const summarizeNode = new SummarizeNode()

    // Create initial global memory state
    const memory = { text: 'This is a long text that needs to be summarized.' }

    // Run the node's lifecycle (prep -> exec -> post)
    await summarizeNode.run(memory) // Pass memory object

    // Verify the LLM call
    expect(callLLM).toHaveBeenCalledTimes(1)
    const callArgs = vi.mocked(callLLM).mock.calls[0][0] // Get the first argument of the first call
    expect(callArgs.toLowerCase()).toContain('summarize') // Check if prompt contains 'summarize'

    // Verify the result was stored correctly in the global memory object
    expect(memory.summary).toBe('Short summary.') // Access memory object
  })
})
```

### Integration Testing (Flows)

Test complete flows to verify that nodes work together correctly:

````typescript
import { afterEach, beforeEach, describe, expect, it, vi } from 'vitest'
import { createQaFlow } from './qaFlow' // Your function that creates the Flow
import { callLLM } from './utils/callLLM' // Your LLM utility

// Mock the LLM utility
vi.mock('./utils/callLLM', () => ({
  callLLM: vi.fn(),
}))

describe('Question Answering Flow', () => {
  beforeEach(() => {
    // Clear any previous mock calls before each test
    vi.clearAllMocks()
  })

  it('should generate an answer using the flow', async () => {
    // Configure mock to return different values based on the prompt
    vi.mocked(callLLM).mockImplementation((prompt: string) => {
      // Simulate different stages of a potential QA flow (e.g., search vs. answer)
      if (prompt.toLowerCase().includes('search')) {
        return Promise.resolve('Paris is the capital of France.')
      } else if (prompt.toLowerCase().includes('answer')) {
        return Promise.resolve('The capital of France is Paris.')
      }
      return Promise.resolve('Unexpected prompt')
    })

    // Create the flow
    const qaFlow = createQaFlow()

    // Create initial memory state
    const memory = { question: 'What is the capital of France?' }

    // Run the flow
    await qaFlow.run(memory) // Pass memory object

    // Verify the final answer
    expect(memory.answer).toBe('The capital of France is Paris.') // Access memory object

    // Verify the LLM was called the expected number of times
    expect(callLLM).toHaveBeenCalledTimes(2)

    // Verify the calls were made with appropriate prompts
    const calls = vi.mocked(callLLM).mock.calls
    const retrieveCall = calls.some(
      (call) => typeof call === 'string' && call.toLowerCase().includes('retrieve'),
    )
    const generateCall = calls.some(
      (call) => typeof call === 'string' && call.toLowerCase().includes('generate'),
    )

    expect(retrieveCall).toBe(true)
    expect(generateCall).toBe(true)
  })
})

// Example testing a MapReduce flow (Trigger, Processor, Reducer)
describe('MapReduce Flow Test', () => {
  // Mock the nodes used in the MapReduce example
class TriggerNode extends Node<Memory, any, ['process_item','reduce']> {
  async post(memory: Memory, prepRes: any, execRes: any): Promise<void> {
      const items = memory.items || []
      memory.results = [] // Initialize results
      items.forEach((item: any, index: number) => {
        this.trigger('process_item', { item, index })
      })
      this.trigger('reduce')
    }
  }
  const ProcessorNode = class extends Node {
     async prep(memory: Memory): Promise<any> { return { item: memory.item, index: memory.index }; }
     async exec(prepRes: { item: any, index: number }): Promise<string> { return `Processed ${prepRes.item}`; }
     async post(memory: Memory, prepRes: { item: any, index: number }, execRes: string): Promise<void> {
         if (!memory.results) memory.results = [];
         // Store result at the correct index if possible, or just push
         memory.results[prepRes.index] = execRes;
     }
  }
  const ReducerNode = class extends Node {
     async prep(memory: Memory): Promise<any[]> { return memory.results || []; }
     async exec(results: any[]): Promise<string> { return `Combined: ${results.join(', ')}`; }
     async post(memory: Memory, prepRes: any, execRes: string): Promise<void> { memory.final_result = execRes; }
  }

  it('should process items via map and reduce steps', async () => {
    // Instantiate nodes
    const trigger = new TriggerNode()
    const processor = new ProcessorNode()
    const reducer = new ReducerNode()

    // Connect nodes
    trigger.on('process_item', processor)
    trigger.on('reduce', reducer) // This action is triggered after all 'process_item'

    // Use ParallelFlow for the map phase
    const mapReduceFlow = new ParallelFlow(trigger)

    // Initial memory
    const memory = { items: ['A', 'B', 'C'] }

    // Run the flow
    await mapReduceFlow.run(memory)

    // Verify final result in memory
    expect(memory.results).toEqual(['Processed A', 'Processed B', 'Processed C'])
    expect(memory.final_result).toBe('Combined: Processed A, Processed B, Processed C')
  })
})

## Testing Approaches

### Unit Testing Individual Nodes

1. **Isolate Dependencies**: Mock external services and LLM calls
2. **Test Each Lifecycle Method**: Verify `prep`, `exec`, and `post` individually
3. **Test Error Handling**: Ensure `exec_fallback` works as expected
4. **Verify Memory Updates**: Check if memory is modified correctly
5. **Test Triggers**: Ensure the right actions are triggered

### Integration Testing Flows

1. **Mock External Services**: Keep tests deterministic by mocking APIs
2. **Verify End-to-End Behavior**: Test the entire flow from start to finish
3. **Test Branching Logic**: Ensure different paths work correctly
4. **Check Final Memory State**: Verify that the memory contains expected results
5. **Test Error Handling**: Make sure flows handle errors gracefully

### Testing Strategies

#### Testing LLM-Based Nodes

For nodes that call LLMs, you can use these approaches:

1.  **Canned Responses**: Prepare fixed responses for specific prompts.
2.  **Prompt Verification**: Check if prompts contain expected information.
3.  **Response Validation**: Test if the node correctly handles various LLM responses.

```typescript
import { describe, expect, it, vi } from 'vitest'
import { callLLM } from './utils/callLLM' // Your LLM utility

// import { MyLlmNode } from './MyLlmNode'; // Your Node implementation
// import { Memory } from 'brainyflow'; // Assuming Memory is imported if needed

// Mock the LLM utility module
vi.mock('./utils/callLLM', () => ({
  callLLM: vi.fn(), // Create a mock function
}))

describe('Testing LLM Nodes', () => {
  it('should use canned responses based on prompt', async () => {
    // Configure the mock implementation
    vi.mocked(callLLM).mockImplementation(async (prompt: string): Promise<string> => {
      if (prompt.toLowerCase().includes('summarize')) {
        return 'This is a summary.'
      } else if (prompt.toLowerCase().includes('extract')) {
        return JSON.stringify({ key: 'value' }) // Return JSON string
      } else {
        return 'Default response'
      }
    })

    // const node = new MyLlmNode();
    // const memory = { input: 'some text to summarize' }; // Initial memory state

    // await node.run(memory); // Run the node

    // Add assertions here
    // expect(callLLM).toHaveBeenCalled();
    // expect(memory.summary).toBe('This is a summary.');
  })
})
```

#### Testing Retry Logic

To test retry behavior:

1.  **Simulate Transient Failures**: Make the mock function fail a few times before succeeding.
2.  **Check Retry Count**: Verify that retries happened the expected number of times (e.g., by checking `node.cur_retry` inside the mock or tracking calls).
3.  **Test Backoff**: If using `wait`, mock `asyncio.sleep` (Python) or `setTimeout` (TypeScript) to verify delays without actually waiting.

```typescript
import { beforeEach, describe, expect, it, vi } from 'vitest'

// import { Node, Memory } from 'brainyflow'; // Assuming imports
// import { someExternalCall } from './utils/externalCall'; // The function called by exec

// Mock the external call module
vi.mock('./utils/externalCall', () => ({
  someExternalCall: vi.fn(),
}))

// Mock setTimeout used for 'wait' (if applicable)
vi.useFakeTimers()

// Example Node (conceptual)
// class NodeWithRetry extends Node<any, any, [], any, string> {
//   constructor() {
//     super({ maxRetries: 3, wait: 100 }); // Retry up to 3 times, wait 100ms
//   }
//   async exec(prepRes: any): Promise<string> {
//     // This method calls the function we will mock
//     return await someExternalCall(prepRes);
//   }
// }

describe('Retry Logic Testing', () => {
  let callCountRetry = 0

  beforeEach(() => {
    callCountRetry = 0 // Reset counter
    vi.clearAllMocks() // Clear mock history
    vi.clearAllTimers() // Clear pending timers
  })

  it('should retry exec on failure and succeed eventually', async () => {
    // Configure the mock to fail twice, then succeed
    vi.mocked(someExternalCall).mockImplementation(async () => {
      callCountRetry++
      console.log(`Mock called (Attempt ${callCountRetry})`) // For debugging test
      if (callCountRetry <= 2) {
        throw new Error('Temporary network failure')
      }
      return 'Success on third try'
    })

    // const node = new NodeWithRetry();
    // const memory = {}; // Initial memory

    // await node.run(memory); // Run the node

    // Advance timers to simulate waiting (if wait > 0)
    // vi.advanceTimersByTime(100); // Advance by wait time
    // await Promise.resolve(); // Allow promises to settle after timer advance
    // vi.advanceTimersByTime(100); // Advance for second wait
    // await Promise.resolve();

    // Assertions
    // expect(callCountRetry).toBe(3); // Called 3 times
    // expect(memory.result).toBe('Success on third try'); // Check final result
    // expect(vi.getTimerCount()).toBe(0); // Ensure all timers were cleared/run
  })
})
```

## Test Fixtures and Helpers

Creating helper functions can make tests more readable and maintainable.

```typescript
import { expect } from 'vitest'

// import { Node, Memory } from 'brainyflow'; // Assuming imports

// Define a type for your standard test memory if desired
interface TestMemory {
  input?: string
  config?: { setting: string }
  output?: any
  status?: string
  [key: string]: any // Allow other properties
}

export function createDefaultTestMemory(): TestMemory {
  /** Creates a standard object for test memory. */
  return { input: 'test data', config: { setting: 'value' } }
}

export async function runNodeWithMemory(
  node: Node,
  initialMemory?: TestMemory,
): Promise<TestMemory> {
  /** Runs a node with provided or default initial memory. */
  const memory = initialMemory ?? createDefaultTestMemory()
  // Assumes node.run modifies the object in place
  await node.run(memory)
  return memory
}

export function assertMemoryContains(memory: TestMemory, expectedData: Partial<TestMemory>): void {
  /** Asserts that the memory object contains the expected key-value pairs. */
  for (const key in expectedData) {
    expect(memory).toHaveProperty(key)
    expect(memory[key]).toEqual(expectedData[key])
  }
}

// Example usage in a test
/*
import { MyProcessingNode } from './MyProcessingNode';
import { runNodeWithMemory, assertMemoryContains } from './testHelpers';

it('should produce correct output in memory', async () => {
    const node = new MyProcessingNode();
    const finalMemory = await runNodeWithMemory(node);
    assertMemoryContains(finalMemory, { output: "processed data", status: "completed" });
});
*/
```

## Common Testing Patterns

### 1. Input Validation Testing

Test that nodes properly handle invalid or unexpected inputs.

```typescript
import { describe, expect, it } from 'vitest'

// import { MyNodeThatValidates } from './MyNodeThatValidates'; // Your node
// import { Memory } from 'brainyflow'; // Assuming imports

describe('Input Validation', () => {
  const invalidInputs = [null, undefined, '', {}, [], { wrongKey: 1 }]

  it.each(invalidInputs)('should handle invalid input: %s', async (invalidInput) => {
    /** Tests if the node handles various invalid inputs gracefully. */
    // const node = new MyNodeThatValidates(); // Node that should validate memory.input_data
    const memory: Record<string, any> = { input_data: invalidInput } // Pass invalid data

    // Expect the node to run without unhandled exceptions
    // Use try/catch if specific errors are expected, otherwise just run
    await node.run(memory)

    // Example assertions: Check for an error flag or a specific state
    expect(memory.error_message || memory.status).toBeDefined() // Check if either is set
    expect(memory.status === 'validation_failed' || memory.error_message).toBeTruthy()
    // Or assert that a default value was set
    // expect(memory.output).toBe('default_value');
  })
})
```

### 2. Flow Path Testing

Test that flows follow the expected paths based on node triggers.

```typescript
import { describe, expect, it } from 'vitest'

// import { Node, Flow, Memory, BaseNode } from 'brainyflow'; // Assuming imports

describe('Flow Path Testing', () => {
  it('should follow the correct path based on triggers', async () => {
    /** Tests if the flow executes nodes in the expected sequence. */
    const visitedNodesLog: string[] = []

    // Define simple tracking nodes
    class SimpleTrackingNode extends Node<any, any, ['next_step', 'finish']> {
      private nodeName: string
      private triggerAction: 'next_step' | 'finish' | 'default'

      constructor(name: string, triggerAction: 'next_step' | 'finish' | 'default' = 'default') {
        super()
        this.nodeName = name
        this.triggerAction = triggerAction
      }

      async exec(prepRes: any): Promise<string> {
        // No real work, just track visit
        visitedNodesLog.push(this.nodeName)
        return `Processed by ${this.nodeName}` // Return something for post
      }

      async post(memory: Memory, prepRes: any, execRes: string): Promise<void> {
        // Trigger the specified action
        this.trigger(this.triggerAction)
      }
    }

    // Create nodes for a path: A -> B -> C (where B triggers 'finish')
    const nodeA = new SimpleTrackingNode('A', 'next_step')
    const nodeB = new SimpleTrackingNode('B', 'finish')
    const nodeC = new SimpleTrackingNode('C') // This node shouldn't be reached

    // Connect nodes based on actions
    nodeA.on('next_step', nodeB)
    nodeB.on('finish', nodeC) // Connect C, but B will trigger 'finish'

    // Create and run the flow
    const flow = new Flow(nodeA)
    await flow.run({}) // Pass empty memory

    // Verify the execution path
    expect(visitedNodesLog).toEqual(['A', 'B'])
  })
})
```

## Best Practices

### Testing Best Practices

1. **Test Each Node Individually**: Verify that each node performs its specific task correctly
2. **Test Flows as Integration Tests**: Ensure nodes work together as expected
3. **Mock External Dependencies**: Use mocks for LLMs, APIs, and databases to ensure consistent testing
4. **Test Error Handling**: Explicitly test how your application handles failures
5. **Automate Tests**: Include BrainyFlow tests in your CI/CD pipeline

### Debugging Best Practices

1. **Start Simple**: Begin with a minimal flow and add complexity incrementally
2. **Visualize Your Flow**: Generate flow diagrams to understand the structure
3. **Isolate Issues**: Test individual nodes to narrow down problems
4. **Check Shared Store**: Verify that data is correctly passed between nodes
5. **Monitor Actions**: Ensure nodes are returning the expected actions

### Monitoring Best Practices

1. **Monitor Node Performance**: Track execution time for each node
2. **Watch for Bottlenecks**: Identify nodes that take longer than expected
3. **Track Error Rates**: Monitor how often nodes and flows fail
4. **Set Up Alerts**: Configure alerts for critical failures
5. **Log Judiciously**: Log important events without overwhelming storage
6. **Implement Distributed Tracing**: Use tracing for complex, distributed applications

By applying these testing techniques, you can ensure your BrainyFlow applications are reliable and maintainable.


