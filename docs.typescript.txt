

================================================
File: docs/installation.md
================================================
# Installation

BrainyFlow is currently available for both Python and TypeScript.

{% tabs %}
{% tab title="TypeScript" %}
You can install the TypeScript package using pnpm (or npm/yarn):

```bash
pnpm add brainyflow
# or
npm install brainyflow
# or
yarn add brainyflow
```

{% endtab %}
{% tab title="JavaScript (Browser)" %}
You can import the JavaScript file directly in the browser using a `<script>` tag:

```html
<script type="module">
  import * as brainyflow from 'https://unpkg.com/brainyflow@latest/dist/brainyflow.js'

  new brainyflow.Node(...)
</script>
```

or

```html
<script type="module" src="https://unpkg.com/brainyflow@latest/dist/brainyflow.js"></script>
<script>
  new globalThis.brainyflow.Node(...)
</script>
```

{% endtab %}
{% endtabs %}

## Alternative: Copy the Source Code

Since BrainyFlow is lightweight and dependency-free, you can also install it by simply copying the source code file directly into your project:

Copy [`typescript/brainyflow.ts`](https://github.com/zvictor/BrainyFlow/blob/main/typescript/brainyflow.ts)

## Next Steps

Once you have BrainyFlow installed, check out the [Getting Started](./getting_started.md) guide to build your first flow, or explore the [Core Abstractions](./core_abstraction/node.md) to understand the framework's fundamental concepts.




================================================
File: docs/getting_started.md
================================================
# Getting Started with BrainyFlow

Welcome to BrainyFlow! This framework helps you build powerful, modular AI applications using a simple yet expressive abstraction based on nested directed graphs.

## 1. Installation

First, ensure you have BrainyFlow installed:

```bash
npm install brainyflow # or pnpm/yarn
```

For more installation options, see the [Installation Guide](./installation.md).

## 2. Core Concepts

BrainyFlow is built around a minimalist yet powerful abstraction that separates data flow from computation:

- **[Node](./core_abstraction/node.md)**: The fundamental building block that performs a single task with a clear lifecycle (`prep` → `exec` → `post`).
- **[Flow](./core_abstraction/flow.md)**: Orchestrates nodes in a directed graph, supporting branching, looping, and nesting.
- **[Memory](./core_abstraction/memory.md)**: Manages state, separating it into a shared `global` store and a forkable `local` store for isolated data flow between nodes.

## 3. Your First Flow

Let's build a simple Question-Answering flow to demonstrate BrainyFlow's core concepts:

### Step 1: Design Your Flow

Our flow will have two nodes:

1. `GetQuestionNode`: Captures the user's question
2. `AnswerNode`: Generates an answer using an LLM

```mermaid
graph LR
    A[GetQuestionNode] --> B[AnswerNode]
```

### Step 2: Implement the Nodes

```typescript
import { Flow, Memory, Node } from 'brainyflow'
import { input } from '@inquirer/prompts'
import { callLLM } from './utils/callLLM'

// Define interfaces for Memory stores (optional but good practice)
interface QAGlobalStore {
  question?: string
  answer?: string
}
class GetQuestionNode extends Node<QAGlobalStore> {
  async prep(memory: Memory<QAGlobalStore>): Promise<void> {
    memory.question = await input({ message: 'Enter your question: ' })
  }
}

class AnswerNode extends Node<QAGlobalStore> {
  async prep(memory: Memory<QAGlobalStore>): Promise<string | undefined> {
    return memory.question
  }

  async exec(question: string | undefined): Promise<string> {
    const prompt = `Answer the following question: ${question}`
    return await callLLM(prompt)
  }

  async post(
    memory: Memory<QAGlobalStore>,
    prepRes: string | undefined,
    execRes: string,
  ): Promise<void> {
    memory.answer = execRes
    console.log(`AnswerNode: Stored answer '${execRes}'`)
  }
}
```

{% hint style="info" %}

**Review:** What was achieved here?

- `GetQuestionNode` gets the user's question and writes it to the `memory` object (global store), then explicitly `trigger`s the default next node.
- `AnswerNode` reads the question from the `memory` object, calls an LLM utility, writes the answer back to the `memory` object, and `trigger`s the next step (or ends the flow).

{% endhint %}

### Step 3: Connect the Nodes into a Flow

```typescript
// import { GetQuestionNode, AnswerNode } from './nodes'; // defined in the previous step
import { Flow } from 'brainyflow'

function createQaFlow(): Flow {
  const getQuestionNode = new GetQuestionNode()
  const answerNode = new AnswerNode()

  // Connect nodes getQuestionNode → answerNode using the default action
  getQuestionNode.next(answerNode)

  // Create the Flow, specifying the starting node
  return new Flow(getQuestionNode)
}
```

{% hint style="info" %}

**Review:** What was achieved here?

- We instantiated the nodes and connected them using the default action (`>>` in Python, `.next()` in TypeScript).
- We created a `Flow` instance, telling it to start execution with `getQuestionNode`.

{% endhint %}

### Step 4: Run the Flow

```typescript
import { createQaFlow, QAGlobalStore } from './flow' // defined in the previous steps

async function main() {
  // Initialize the global store (can be an empty object)
  const globalStore: QAGlobalStore = {}
  const qaFlow = createQaFlow()

  console.log('Running QA Flow...')
  // Run the flow, passing the initial global store.
  // The flow modifies the globalStore object in place.
  // The run method returns the final execution tree (we ignore it here).
  await qaFlow.run(globalStore)

  // Access the results stored in the global store
  console.log('\n--- Flow Complete ---')
  console.log(`Question: ${globalStore.question ?? 'N/A'}`)
  console.log(`Answer: ${globalStore.answer ?? 'N/A'}`)
}

main().catch(console.error)
```

{% hint style="info" %}

**Review:** What was achieved here?

- We initialized an empty `memory` object (Python dictionary or TS object) to serve as the global store.
- `qaFlow.run(memory)` executed the flow. The `Memory` instance managed the state internally, reading from and writing to the `memory` object we passed in.
- The final `question` and `answer` are directly accessible in the original `memory` object after the flow completes.

{% endhint %}

## 4. Key Design Principles

BrainyFlow follows these core design principles:

1. **Separation of Concerns**: Data storage (the `memory` object managing global/local stores) is separate from computation logic (`Node` classes).
2. **Explicit Data Flow**: Data dependencies between steps are clear and traceable through `memory` access in `prep`/`post` and the results passed between `prep` → `exec` → `post`.
3. **Composability**: Complex systems (`Flow`s) are built from simple, reusable components (`Node`s), and Flows themselves can be nested within other Flows.
4. **Minimalism**: The framework provides only essential abstractions (`Node`, `Flow`, `Memory`), avoiding vendor-specific implementations or excessive boilerplate.

## 5. Next Steps

Now that you understand the basics, explore these resources to build sophisticated applications:

- [Core Abstractions](./core_abstraction/index.md): Dive deeper into nodes, flows, and communication
- [Design Patterns](./design_pattern/index.md): Learn more complex patterns like Agents, RAG, and MapReduce
- [Agentic Coding Guide](./guides/agentic_coding.md): Best practices for human-AI collaborative development
- [Best Practices](./guides/best_practices.md): Tips for building robust, maintainable applications




================================================
File: docs/comparison.md
================================================
# Framework Comparison: BrainyFlow vs Alternatives

BrainyFlow exists in an ecosystem of frameworks designed to build AI applications, particularly those centered around Large Language Models (LLMs). This chapter provides a comparative analysis of BrainyFlow against other popular frameworks in the AI orchestration space, highlighting the unique design principles and trade-offs that differentiate BrainyFlow from its alternatives.

## BrainyFlow's Philosophy Recap

Before diving into comparisons, let's reiterate BrainyFlow's core tenets:

- **Minimalist Core:** A tiny codebase (~200 lines in Python) providing essential abstractions (`Node`, `Flow`, `Memory`).
- **Graph-Based Abstraction:** Uses nested directed graphs to model application logic, separating data flow (`Memory`) from computation (`Node`).
- **Zero Dependencies:** The core framework has no external dependencies, offering maximum flexibility.
- **No Vendor Lock-in:** Encourages using external utilities directly, avoiding framework-specific wrappers for APIs or databases.
- **Agentic Coding Friendly:** Designed to be intuitive for both human developers and AI assistants collaborating on code.
- **Composability:** Flows can be nested within other flows, enabling modular design.

## Comparison Points

We'll compare BrainyFlow against competitors based on:

- **Core Abstraction:** The fundamental building blocks and mental model.
- **Dependencies & Size:** The footprint and external requirements.
- **Flexibility vs. Opinionation:** How much structure the framework imposes versus how much freedom the developer has.
- **Vendor Integrations:** Built-in support for specific LLMs, databases, etc.
- **Learning Curve:** Perceived difficulty in getting started and mastering the framework.

## BrainyFlow vs. PocketFlow (Origin)

BrainyFlow originated as a fork of PocketFlow, inheriting its core philosophy of minimalism and a graph-based abstraction. However, BrainyFlow has evolved with some key differences:

- **Core Abstraction:** PocketFlow is bloated with unnecessary specialized classes (e.g. `AsyncNode`, `BatchNode`, `AsyncBatchNode`, `AsyncParallelBatchNode`, `AsyncFlow`, `BatchFlow`, `AsyncBatchFlow`, `AsyncParallelBatchFlow`). BrainyFlow simplifies this by removing all of them from the core. Instead, it relies only on standard `Node` lifecycle methods (which can be `async`) combined with `Flow` (or `ParallelFlow`) and the use of multiple `trigger` calls within a single node's `post` method to achieve batch-like fan-out operations.
- **State Management:** While both use a shared store, BrainyFlow places greater emphasis on the `Memory` object's `global` vs. `local` stores and using `forkingData` during `trigger` calls to manage branch-specific context, eliminating the need for PocketFlow's `Params` concept and all the complex `Batch` classes that come with it.
- **Focus:** BrainyFlow sharpens the focus on the fundamental `Node`, `Flow`, and `Memory` abstractions as the absolute core, reinforcing the idea that patterns like batching or parallelism are handled by how flows orchestrate standard nodes rather than requiring specialized node types.

Essentially, BrainyFlow refines PocketFlow's minimalist approach, aiming for an even leaner core by handling execution patterns like batching and parallelism primarily at the `Flow` orchestration level.

On top of that, BrainyFlow has been designed to be more agentic-friendly, with a focus on building flows that can be used by both humans and AI assistants. Its code is more readable and maintainable, prioritizing developer experience over an arbitrarily defined amount of lines of code.

## BrainyFlow vs. LangChain

- **Core Abstraction:** LangChain offers a vast array of components (Chains, LCEL, Agents, Tools, Retrievers, etc.). BrainyFlow focuses solely on the Node/Flow/Memory graph.
- **Dependencies & Size:** LangChain has numerous dependencies depending on the components used, leading to a larger footprint. BrainyFlow core is dependency-free.
- **Flexibility vs. Opinionation:** LangChain provides many pre-built components, which can be faster but potentially more opinionated. BrainyFlow offers higher flexibility, requiring developers to build or integrate utilities themselves.
- **Vendor Integrations:** LangChain has extensive built-in integrations. BrainyFlow intentionally avoids these in its core.
- **Learning Curve:** LangChain's breadth can be overwhelming. BrainyFlow's core is small, but mastering its flexible application requires understanding the graph pattern well.

## BrainyFlow vs. LangGraph

- **Core Abstraction:** LangGraph is built on LangChain and specifically focuses on cyclical graphs using a state-based approach. BrainyFlow uses action-based transitions between nodes within its graph structure.
- **Dependencies & Size:** LangGraph inherits LangChain's dependencies. BrainyFlow remains dependency-free.
- **Flexibility vs. Opinionation:** LangGraph is tied to the LangChain ecosystem and state management patterns. BrainyFlow offers more fundamental graph control.
- **Vendor Integrations:** Inherited from LangChain. BrainyFlow has none.
- **Learning Curve:** Requires understanding LangChain concepts plus LangGraph's state model. BrainyFlow focuses only on its core abstractions.

## BrainyFlow vs. CrewAI

- **Core Abstraction:** CrewAI provides higher-level abstractions like Agent, Task, and Crew, focusing on collaborative agent workflows. BrainyFlow provides the lower-level graph building blocks upon which such agent systems _can be built_.
- **Dependencies & Size:** CrewAI has dependencies related to its agent and tooling features. BrainyFlow is minimal.
- **Flexibility vs. Opinionation:** CrewAI is more opinionated towards specific multi-agent structures. BrainyFlow is more general-purpose.
- **Vendor Integrations:** CrewAI integrates with tools and LLMs, often via LangChain. BrainyFlow does not.
- **Learning Curve:** CrewAI's high-level concepts might be quicker for specific agent tasks. BrainyFlow requires building the agent logic from its core components.

## BrainyFlow vs. AutoGen

- **Core Abstraction:** AutoGen focuses on conversational agents (`ConversableAgent`) and multi-agent frameworks, often emphasizing automated chat orchestration. BrainyFlow focuses on the underlying execution graph.
- **Dependencies & Size:** AutoGen has a core set of dependencies, with optional ones for specific tools/models. BrainyFlow core has none.
- **Flexibility vs. Opinionation:** AutoGen is geared towards conversational agent patterns. BrainyFlow is a more general graph execution engine.
- **Vendor Integrations:** AutoGen offers integrations, particularly for LLMs. BrainyFlow avoids them.
- **Learning Curve:** AutoGen's conversational focus might be specific. BrainyFlow's graph is general but requires explicit construction.

## Feature Comparison Matrix

| Feature                   | BrainyFlow        | LangChain         | LangGraph            | CrewAI                    | AutoGen               | PocketFlow       |
| ------------------------- | ----------------- | ----------------- | -------------------- | ------------------------- | --------------------- | ---------------- |
| **Core Abstraction**      | Nodes & Flows     | Chains & Agents   | State Graphs         | Agents & Crews            | Conversational Agents | Nodes & Flows    |
| **Dependencies**          | None              | Many              | Many (via LangChain) | Several                   | Several               | None             |
| **Codebase Size**         | Tiny (~200 lines) | Large             | Medium               | Medium                    | Medium                | Tiny (100 lines) |
| **Flexibility**           | High              | Medium            | Medium               | Low                       | Medium                | High             |
| **Built-in Integrations** | None              | Extensive         | Via LangChain        | Several                   | Several               | None             |
| **Learning Curve**        | Moderate          | Steep             | Very Steep           | Moderate                  | Moderate              | Moderate         |
| **Primary Focus**         | Graph Execution   | Component Library | State Machines       | Multi-Agent Collaboration | Conversational Agents | Graph Execution  |

## Conclusion: When to Choose BrainyFlow?

BrainyFlow excels when you prioritize:

- **Minimalism and Control:** You want a lightweight core without unnecessary bloat or dependencies.
- **Flexibility:** You prefer to integrate your own utilities and avoid framework-specific wrappers.
- **Understanding the Core:** You value a simple, fundamental abstraction (the graph) that you can build upon.
- **Avoiding Vendor Lock-in:** You want the freedom to choose and switch external services easily.
- **Agentic Coding:** You plan to collaborate with AI assistants, leveraging a framework they can easily understand and manipulate.

If you need extensive pre-built integrations, higher-level abstractions for specific patterns (like multi-agent collaboration out-of-the-box), or prefer a more opinionated framework, other options might be a better fit initially. However, BrainyFlow provides the fundamental building blocks to implement _any_ of these patterns with maximum transparency and control.




================================================
File: docs/core_abstraction/index.md
================================================
# Understanding BrainyFlow's Core Abstractions

BrainyFlow is built around a simple yet powerful abstraction: the **nested directed graph with shared store**. This mental model separates _data flow_ from _computation_, making complex LLM applications more maintainable and easier to reason about.

<div align="center">
  <img src="https://raw.githubusercontent.com/zvictor/brainyflow/main/.github/media/abstraction.jpg" width="1300"/>
</div>

## Core Philosophy

BrainyFlow follows these fundamental principles:

1. **Modularity & Composability**: Build complex systems from simple, reusable components that are easy to build, test, and maintain
2. **Explicitness**: Make data dependencies between steps clear and traceable
3. **Separation of Concerns**: Data storage (shared store) remains separate from computation logic (nodes)
4. **Minimalism**: The framework provides only essential abstractions, avoiding vendor-specific implementations while supporting various high-level AI design paradigms (agents, workflows, map-reduce, etc.)
5. **Resilience**: Handle failures gracefully with retries and fallbacks

## The Graph + Shared Store Pattern

The fundamental pattern in BrainyFlow combines two key elements:

- **Computation Graph**: A directed graph where nodes represent discrete units of work and edges represent the flow of control.
- **Shared Store**: A state management system that enables communication between nodes, separating global and local state.

This pattern offers several advantages:

- **Clear visualization** of application logic
- **Easy identification** of bottlenecks
- **Simple debugging** of individual components
- **Natural parallelization** opportunities

## Key Components

BrainyFlow's architecture is based on these fundamental building blocks:

| Component             | Description                                    | Key Features                                                                                                |
| --------------------- | ---------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| [Node](./node.md)     | The basic unit of work                         | Clear lifecycle (`prep` → `exec` → `post`), fault tolerance (retries), graceful fallbacks                   |
| [Flow](./flow.md)     | Connects nodes together                        | Action-based transitions, branching, looping (with cycle detection), nesting, sequential/parallel execution |
| [Memory](./memory.md) | Manages state accessible during flow execution | Shared `global` store, forkable `local` store, cloning for isolation                                        |

## How They Work Together

1. **Nodes** perform individual tasks with a clear lifecycle:

   - `prep`: Read from shared store and prepare data
   - `exec`: Execute computation (often LLM calls)
   - `post`: Process results, write to shared store, and trigger next actions

2. **Flows** orchestrate nodes by:

   - Starting with a designated `start` node.
   - Following action-based transitions (`trigger` calls in `post`) between nodes.
   - Supporting branching, looping, and nested flows.
   - Executing triggered branches sequentially (`Flow`) or concurrently (`ParallelFlow`).
   - Supporting nested batch operations.

3. **Communication** happens through the `memory` instance provided to each node's lifecycle methods (in `prep` and `post` methods):

   - **Global Store**: A shared object accessible throughout the flow. Nodes typically write results here.
   - **Local Store**: An isolated object specific to a node and its downstream path.

## Getting Started

If you're new to BrainyFlow, we recommend exploring these core abstractions in the following order:

1. [Node](./node.md) - Understand the basic building block
2. [Flow](./flow.md) - Learn how to connect nodes together
3. [Memory](./memory.md) - See how nodes share data

Once you understand these core abstractions, you'll be ready to implement various [Design Patterns](../design_pattern/index.md) to solve real-world problems.




================================================
File: docs/core_abstraction/node.md
================================================
# Nodes

Nodes are the fundamental building blocks in BrainyFlow. Each node performs a specific task within your workflow, processing data and optionally triggering downstream nodes.

## Node Lifecycle

<div align="center">
  <img src="https://github.com/zvictor/brainyflow/raw/main/.github/media/node.jpg" width="400"/>
</div>

Every node follows a three-phase lifecycle:

1.  **`prep`**: Gather and prepare input data from the `memory` object.
2.  **`exec`**: Perform the main processing task (potentially retried). Receives data from `prep`. **Cannot** access the `memory` object directly.
3.  **`post`**: Process results, update the `memory` object (global or local store), and trigger downstream actions. Receives data from `prep` and `exec`, plus the `memory` instance.

{% hint style="info" %}
**Why 3 steps?** This design enforces separation of concerns.

All steps are **optional**. For example, you can implement only `prep` and `post` if you just need to process data without external computation.
{% endhint %}

### 1. `async prep(memory)`

- Receives the current `memory` instance (a proxy managing global and local stores).
- Extracts necessary data by external sources or by accessing properties on the `memory` object (e.g., `memory.someData`). The proxy reads from the local store first, then the global store.
- Performs any required preprocessing or validation.
- Can optionally return a `PrepResult`, which is passed as input to `exec()` and `post()`.

### 2. `async exec(prepRes)`

- Receives the result from `prep()` (`prepRes`).
- Performs the main computation (e.g., LLM call, API request, calculation).
- ⚠️ **Cannot** access the `Memory` instance directly. This enforces separation and aids retry logic.
- ⚠️ Should ideally be idempotent (produce the same result given the same input) and have no side effects if retries (`maxRetries > 1`) are enabled, as it might be called multiple times.
- Returns an `ExecResult`, which is passed to `post()`.
- _Note:_ The actual execution logic, including retries, is handled by the internal `execRunner` method, which calls this `exec` method.

### 3. `async post(memory, prepRes, execRes)`

- Receives the `memory` instance, the result from `prep()` (`prepRes`), and the result from `exec()` (`execRes`).
- Processes results and writes data back to the `memory` object (usually the global store, e.g., `memory.result = execRes`).
- **This is the only place to call `this.trigger()`** to specify which downstream nodes should run next and optionally pass `forkingData` to their local memory.
  - If no `trigger` is called, the flow proceeds via the `DEFAULT_ACTION` ('default') with no specific `forkingData`.

```mermaid
sequenceDiagram
    participant M as Memory (Proxy)
    participant N as Node

    N->>M: 1. prep(memory): Read from memory
    Note right of N: Return prep_res

    N->>N: 2. exec(prep_res): Compute result
    Note right of N: Return exec_res

    N->>M: 3. post(memory, prep_res, exec_res): Write to memory
    Note right of N: Trigger next actions
```

## Creating Custom Nodes

To create a custom node, extend the `Node` class and implement the lifecycle methods:

```typescript
import { Memory, Node } from 'brainyflow'

class TextProcessorNode extends Node {
  async prep(memory: Memory): Promise<string> {
    // Read input data
    return memory.text
  }

  async exec(text: string): Promise<string> {
    // Process the text
    return text.toUpperCase()
  }

  async post(memory: Memory, input: string, result: string): Promise<void> {
    // Store the result in the global store
    memory.processed_text = result

    // Trigger the default next node (optional)
    this.trigger('default')
  }
}
```

## Error Handling

Nodes include built-in retry capabilities for handling transient failures in `exec()` calls.

You can configure retries via the constructor:

- `maxRetries` (number): Maximum number of attempts for `exec()` (default: 1, meaning no retry).
- `wait` (number): Seconds to wait between retry attempts (default: 0).

The `wait` parameter is specially helpful when you encounter rate-limits or quota errors from your LLM provider and need to back off.
During retries, you can access the current retry count (0-based) via `self.cur_retry` (Python) or `this.curRetry` (TypeScript).

To handle failures gracefully after all retry attempts for `exec()` are exhausted, override the `execFallback` method.

By default, `execFallback` just re-raises the exception. You can override it to return a fallback result instead, which becomes the `exec_res` passed to `post()`, allowing the flow to potentially continue.

```typescript
import { Memory, Node, NodeError } from 'brainyflow'

type PrepResult = void
type ExecResult = string

const myNode = new CustomErrorHandlingNode({ maxRetries: 3, wait: 10 })

class CustomErrorHandlingNode extends Node<any, any, [], PrepResult, ExecResult> {
  async exec(prepRes: PrepResult): Promise<ExecResult> {
    console.log(`Exec attempt: ${this.curRetry + 1}`)
    if (this.curRetry < 2) {
      throw new Error('Temporary failure!')
    }
    return 'Success on retry'
  }

  async execFallback(prepRes: PrepResult, error: NodeError): Promise<ExecResult> {
    // This is called only if exec fails on the last attempt
    console.error(`Exec failed after ${error.retryCount + 1} attempts: ${error.message}`)
    // Return a fallback value instead of re-throwing
    return `Fallback response due to repeated errors: ${error.message}`
  }

  async post(memory: Memory<any, any>, prepRes: PrepResult, execRes: ExecResult): Promise<void> {
    // execRes will be "Success on retry" or "Fallback response..."
    console.log(`Post: Received result '${execRes}'`)
    memory.final_result = execRes
    this.trigger('default')
  }
}
```

## Triggering Successors (`trigger`)

Nodes signal which path(s) the flow should take next by calling `this.trigger()` within their `post` method.

```typescript
this.trigger(actionName: string | typeof DEFAULT_ACTION, forkingData?: SharedStore): void
```

- `actionName`: A string identifying the transition (e.g., 'success', 'failure', 'categoryA'). If omitted or if `trigger` is not called, `DEFAULT_ACTION` ('default') is assumed.
- `forkingData` (optional): An object containing data to be added _only_ to the `local` store of the `memory` instance passed to the triggered successor(s). This allows passing specific data down a particular branch without polluting the global store.

The running [Flow](./flow.md) uses the `actionName` to look up the successor nodes defined using `.on()` or `.next()`.

{% hint style="warning" %}
`trigger()` can **only** be called inside the `post()` method. Calling it elsewhere will throw an error.
{% endhint %}

## Defining Connections (`on`, `next`)

While `trigger` determines _which_ path to take _during_ execution, you define the possible paths _before_ execution, by using either `.next()` or `.on()`, as shown below:

1.  **Basic default transition**: `node_a.next(node_b)`
    This means if `node_a` triggers `"default"`, `node_b` will execute next.

2.  **Named action transition**: `node_a.on('action_name', node_b)` or `node_a.next(node_b, 'action_name')`
    This means if `node_a` triggers `"action_name"`, `node_b` will execute next.

Note that `node_a.next(node_b)` is equivalent to both `node_a.next(node_b, 'default')` and `node_a.on('default', node_b)`. Both methods return the _successor_ node (`node_b` in this case), allowing for chaining.

```typescript
// Basic default transition
node_a.next(node_b) // If node_a triggers "default", go to node_b

// Named action transition
node_a.on('success', node_b) // If node_a triggers "success", go to node_b
node_a.on('error', node_c) // If node_a triggers "error", go to node_c

// Alternative syntax
node_a.next(node_b, 'success') // Same as node_a.on('success', node_b)
```

To summarize it:

- `node.on(actionName, successorNode)`: Connects `successorNode` to be executed when `node` triggers `actionName`.
- `node.next(successorNode, actionName = DEFAULT_ACTION)`: A convenience method, equivalent to `node.on(actionName, successorNode)`.

These methods are typically called when constructing your `Flow`. See the [Flow documentation](./flow.md) for detailed examples of graph construction.

### Example: Conditional Branching

```typescript
import { Flow, Memory, Node } from 'brainyflow'

// Assuming detectLanguage, EnglishProcessorNode, SpanishProcessorNode, UnknownProcessorNode are defined elsewhere

interface RouterGlobalStore {
  content: string
  language?: string
}
type RouterActions = 'english' | 'spanish' | 'unknown' // Add unknown action

class RouterNode extends Node<RouterGlobalStore, any, RouterActions[]> {
  async prep(memory: Memory<RouterGlobalStore, any>): Promise<string> {
    return memory.content
  }

  async exec(content: string | undefined): Promise<string> {
    return await detectLanguage(content)
  }

  async post(
    memory: Memory<RouterGlobalStore, any>,
    prepRes: string, // Content
    execRes: string, // Language detected
  ): Promise<void> {
    console.log(`RouterPost: Detected language '${execRes}', storing and triggering.`)
    memory.language = execRes
    // Trigger the specific action based on the detected language
    this.trigger(execRes as RouterActions)
  }
}

// --- Flow Definition ---
const router = new RouterNode()
const englishProcessor = new EnglishProcessorNode()
const spanishProcessor = new SpanishProcessorNode()
const unknownProcessor = new UnknownProcessorNode()

// Define connections for specific actions
router.on('english', englishProcessor)
router.on('spanish', spanishProcessor)
router.on('unknown', unknownProcessor)

const flow = new Flow(router)

// --- Execution Example ---
async function runFlow() {
  const storeEn: RouterGlobalStore = { content: 'Hello world' }
  await flow.run(storeEn)
  console.log('--- English Flow Done ---', storeEn)

  const storeEs: RouterGlobalStore = { content: 'Hola mundo' }
  await flow.run(storeEs)
  console.log('--- Spanish Flow Done ---', storeEs)

  const storeUnk: RouterGlobalStore = { content: 'Bonjour le monde' }
  await flow.run(storeUnk)
  console.log('--- Unknown Flow Done ---', storeUnk)
}
runFlow()
```

### Example: Multiple Triggers (Fan-Out / Batch Processing)

A single node can call `this.trigger()` multiple times within its `post` method to initiate multiple downstream paths simultaneously. Each triggered path receives its own cloned `memory` instance, potentially populated with unique `local` data via the `forkingData` argument.

This "fan-out" capability is the core pattern used for **batch processing** (processing multiple items, often in parallel).

For a detailed explanation and examples of implementing batch processing using this fan-out pattern with `Flow` or `ParallelFlow`, please see the [Flow documentation](./flow.md#implementing-batch-processing-fan-out-pattern).

## Running Individual Nodes

Nodes have an extra method `run(memory)`, which calls `prep → exec → post`. Use it only for **testing or debugging individual nodes in isolation**.

{% hint style="danger" %}
**Do NOT use `node.run()` to execute a workflow.**

`node.run()` executes only the single node it's called on. It **does not** look up or execute any successor nodes defined via `.on()` or `.next()`.

Always use `Flow.run()` or `ParallelFlow.run()` to execute a complete graph workflow. Using `node.run()` directly will lead to incomplete execution.
{% endhint %}

```typescript
// Run with propagate: false (default) - returns ExecResult
async node.run(memory: Memory | GlobalStore): Promise<ExecResult | void> // Accepts Memory object or initial global store

// Run with propagate: true - returns triggers for Flow execution
async node.run(memory: Memory | GlobalStore, propagate: true): Promise<[Action, Memory][]> // Accepts Memory object or initial global store
```

## Best Practices

- **Single Responsibility**: Keep nodes focused on a single, well-defined task.
- **Read in `prep`**: Gather all necessary data from the `memory` object in the `prep` phase. Return only what `exec` needs.
- **Compute in `exec`**: Perform the core computation in `exec`. Keep it free of side effects and direct `memory` object access.
- **Write & Trigger in `post`**: Update the `memory` object (usually the global store) and call `trigger` in the `post` phase.
- **Use `forkingData`**: Pass branch-specific data via `trigger`'s `forkingData` argument to populate the `local` store for successors, keeping the global store clean.
- **Type Safety**: Use TypeScript generics (`Node<G, L, A, P, E>`) to define the expected structure of `memory` stores, actions, and results.
- **Error Handling**: Leverage the built-in retry logic (`maxRetries`, `wait`) and `execFallback` for resilience.




================================================
File: docs/core_abstraction/flow.md
================================================
# Flow: Orchestrating Nodes in a Directed Graph

A **Flow** orchestrates a graph of Nodes, connecting them through action-based transitions. Flows enable you to create complex application logic including sequences, branches, loops, and nested workflows.

They manage the execution order, handle data flow between nodes, and provide error handling and cycle detection.

## Creating a Flow

A Flow begins with a **start node**, a memory state, and follows the [action-based transitions defined by the nodes](./nodes.md) until it reaches a node with no matching transition for its returned action.

```typescript
import { Flow, Node } from 'brainyflow'

// Define nodes and transitions
// const node_a = new Node(); ... etc.
node_a.next(node_b) // Default transition
node_b.on('success', node_c) // Named transition
node_b.on('error', node_d) // Named transition

// Define the expected Global Store structure (optional but recommended)
interface MyGlobalStore {
  input?: any
  result?: any
  error?: any
}

// Create a global store object (can be just an empty object)
const globalStore: MyGlobalStore = { input: 'some data' }

// Create flow starting with node_a
const flow = new Flow<MyGlobalStore>(node_a)

// Run the flow, passing the global store object.
// The flow modifies the globalStore object in place.
await flow.run(globalStore)

// Print the final state of the global store
console.log('Flow finished. Final memory state:', globalStore)
// Example output (depending on flow logic):
// { input: 'some data', result: 'processed data from node_c' }
// or
// { input: 'some data', error: 'error details from node_d' }
```

## Flow Execution Process

When you call `flow.run(memory)`, the flow executes the following steps internally:

1.  It starts with the designated `start` node.
2.  For the current node, it executes its lifecycle (`prep` -> `execRunner` -> `post`), passing a `Memory` instance that wraps the global store and manages local state.
3.  It looks for triggered action (specified by `trigger()` calls), then it finds the corresponding successor node(s) defined by `.on()` or `.next()`.
4.  It recursively executes the successor node(s) with their respective cloned local memories.
5.  This process repeats until it reaches nodes that trigger actions with no defined successors, or the flow completes.
6.  The `run` method modifies the initial global store object in place and returns a nested structure representing the execution tree (often ignored if results are stored in memory).

```mermaid
sequenceDiagram
    participant S as Shared Store
    participant F as Flow
    participant N1 as Node A
    participant N2 as Node B

    F->>N1: Execute Node A
    N1->>S: Read from shared store
    N1->>N1: Perform computation
    N1->>S: Write to shared store
    N1-->>F: Return action "default"

    F->>F: Find next node for action "default"
    F->>N2: Execute Node B
    N2->>S: Read from shared store
    N2->>N1: Read from local store
    N2->>N2: Perform computation
    N2->>S: Write to shared store
    N2-->>F: Return action "success"

    F->>F: No transition defined for "success"
    F-->>F: Flow execution complete
```

## Controlling Flow Execution

### Branching and Looping

Flows support complex patterns like branching (conditionally following different paths) and looping (returning to previous nodes).

#### Example: Expense Approval Flow

Here's a simple expense approval flow that demonstrates branching and looping:

```typescript
import { Flow, Node } from 'brainyflow'

// Define the nodes first
// const review = new ReviewExpenseNode()
// const revise = new ReviseReportNode()
// const payment = new ProcessPaymentNode()
// const finish = new FinishProcessNode()

// Define the flow connections
review.on('approved', payment) // If approved, process payment
review.on('needs_revision', revise) // If needs changes, go to revision
review.on('rejected', finish) // If rejected, finish the process

revise.next(review) // After revision (default trigger), go back for another review
payment.next(finish) // After payment (default trigger), finish the process

// Create the flow, starting with the review node
const expenseFlow = new Flow(review)
```

This flow creates the following execution paths:

1. If `review` triggers `"approved"`, the expense moves to the `payment` node
2. If `review` triggers `"needs_revision"`, it goes to the `revise` node, which then loops back to `review`
3. If `review` triggers `"rejected"`, it moves to the `finish` node and stops

```mermaid
flowchart TD
    review[Review Expense] -->|approved| payment[Process Payment]
    review -->|needs_revision| revise[Revise Report]
    review -->|rejected| finish[Finish Process]

    revise --> review
    payment --> finish
```

### Flow as a Node

Every `Flow` is in fact a specialized type of `Node`. This means a `Flow` itself can be used as a node within another, larger `Flow`, enabling powerful composition and nesting patterns.

{% hint style="info" %}
The difference from Node is that `Flow` has a specialized `execRunner()` method - _the internal caller of `exec()`_ - that orchestrates its internal nodes and which cannot be overridden.
As such:

- A `Flow`'s primary role is orchestration, not direct computation like a standard `Node`.
- You cannot override `exec` or `execRunner` in a `Flow`.
- It still has the `prep` and `post` lifecycle methods, which you _can_ override if you need to perform setup before the sub-flow runs or cleanup/processing after it completes.
- When a `Flow` used as a node finishes its internal execution, it triggers its _own_ successors in the parent flow based on its `post` method's `trigger` calls (or the default action).

{% endhint %}

This allows you to:

1. Break down complex applications into manageable sub-flows
2. Reuse flows across different applications
3. Create hierarchical workflows with clear separation of concerns

#### Example: Order Processing Pipeline

Here's a practical example that breaks down order processing into nested flows:

```typescript
import { Flow, Node } from 'brainyflow'

// Payment processing sub-flow
validatePayment.next(processPayment).next(paymentConfirmation)
const paymentFlow = new Flow(validatePayment)

// Inventory sub-flow
checkStock.next(reserveItems).next(updateInventory)
const inventoryFlow = new Flow(checkStock)

// Shipping sub-flow
createLabel.next(assignCarrier).next(schedulePickup)
const shippingFlow = new Flow(createLabel)

paymentFlow.next(inventoryFlow) // Default transition after paymentFlow completes
inventoryFlow.next(shippingFlow) // Default transition after inventoryFlow completes

// Create the master flow, starting with the paymentFlow
const orderPipeline = new Flow(paymentFlow)

// --- Run the entire pipeline ---
// const globalStore = { orderId: 'XYZ789', customerId: 'CUST123' };
// await orderPipeline.run(globalStore);
// console.log('Order pipeline completed. Final state:', globalStore);
```

This creates a clean separation of concerns while maintaining a clear execution path:

```mermaid
flowchart LR
    subgraph order_pipeline[Order Pipeline]
        subgraph paymentFlow["Payment Flow"]
            A[Validate Payment] --> B[Process Payment] --> C[Payment Confirmation]
        end

        subgraph inventoryFlow["Inventory Flow"]
            D[Check Stock] --> E[Reserve Items] --> F[Update Inventory]
        end

        subgraph shippingFlow["Shipping Flow"]
            G[Create Label] --> H[Assign Carrier] --> I[Schedule Pickup]
        end

        paymentFlow --> inventoryFlow
        inventoryFlow --> shippingFlow
    end
```

### Cycle Detection

Loops are created by connecting a node back to a previously executed node. To prevent infinite loops, `Flow` includes cycle detection controlled by the `maxVisits` option in its constructor (default is 5). If a node is visited more times than `maxVisits` during a single run, an error is thrown.

{% hint style="success" %}
This ensures that the flow does not get stuck in an infinite loop.
{% endhint %}

```typescript
// Limit the number of times a node can be visited within this flow
const flow = new Flow(startNode, { maxVisits: 10 })
```

- The default value for `maxVisits` is `5`.
- Set `maxVisits` to `Infinity` or a very large number for effectively no limit (use with caution!).

## Flow Parallelism

BrainyFlow offers two built-in types - and the possibility to create custom ones\_ - of flows that provide different levels of parallelism when nodes trigger multiple successors:

### 1. `Flow` (Sequential Execution)

The default `Flow` class executes the tasks generated by multiple triggers **sequentially**. It waits for the entire branch initiated by the first trigger to complete before starting the branch for the second trigger, and so on.

```typescript
const sequentialFlow = new Flow(startNode)
```

### 2. `ParallelFlow` (Concurrent Execution)

The `ParallelFlow` class executes the tasks generated by multiple triggers **concurrently** using `Promise.all()` (TypeScript) or `asyncio.gather()` (Python). This is useful for performance when branches are independent (e.g., batch processing items).

```typescript
// Executes triggered branches in parallel
const parallelFlow = new ParallelFlow(startNode)
```

Use `ParallelFlow` when:

1.  A node needs to "fan-out" work into multiple independent branches (e.g., processing items in a batch).
2.  These branches do not have dependencies on each other's immediate results (though they might all write back to the shared global memory).
3.  You want to potentially speed up execution by running these independent branches concurrently.

{% hint style="danger" %}

When using `ParallelFlow`, be mindful of potential race conditions if multiple parallel branches try to modify the same property in the global `Memory` simultaneously without proper synchronization. Often, it's safer for parallel branches to accumulate results locally or use unique keys in the global store, followed by a final sequential aggregation step.

{% endhint %}

### Custom Execution Logic (Overriding `runTasks`)

For more advanced control over how triggered branches are executed, you can extend `Flow` (or `ParallelFlow`) and override the `runTasks` method. This method receives an array of functions, where each function represents the execution of one triggered branch.

```typescript
import { Flow, Memory } from 'brainyflow'

declare function sleep(ms: number): Promise<void> // Assuming sleep is available

class CustomExecutionFlow extends Flow {
  async runTasks<T>(tasks: (() => Promise<T>)[]): Promise<Awaited<T>[]> {
    // Example: Run tasks sequentially with a delay between each
    const results: Awaited<T>[] = []
    for (const task of tasks) {
      results.push(await task())
      console.log('Custom runTasks: Task finished, waiting 1s...')
      await sleep(1000) // Wait 1 second between tasks
    }
    return results
  }
}
```

## Batch Processing (Fan-Out Pattern)

The standard way to process multiple items (sequentially or in parallel) is using the "fan-out" pattern with multiple triggers:

1.  **Trigger Node**: A standard `Node` whose `post` method iterates through your items (e.g., from `memory.items`). For each item, it calls `this.trigger("process_item", { item: current_item, index: i })`. The `forkingData` (`{ item: ..., index: ... }`) is crucial for passing item-specific data to the local memory of the next node.
2.  **Processor Node**: Another standard `Node` connected via the `"process_item"` action (`triggerNode.on("process_item", processorNode)`). Its `prep` reads the specific `item` and `index` from its local memory. Its `exec` performs the actual processing for that single item. Its `post` typically stores the result back into global memory (e.g., `memory.results[index] = result`).
3.  **Flow Choice**:
    - Use `Flow(triggerNode)` for **sequential** batch processing (one item completes before the next starts).
    - Use `ParallelFlow(triggerNode)` for **concurrent** batch processing (all items are processed in parallel).

This pattern leverages the core `Flow` and `Node` abstractions to handle batching effectively.

#### Example: Simple Batch Processing

```typescript
// Assume processSingleItem exists
// declare function processSingleItem(itemData: any): Promise<any>;

interface BatchGlobalStore {
  items_to_process?: any[]
  results?: any[]
}
interface BatchLocalStore {
  item_data?: any
  result_index?: number
}

class TriggerBatchNode extends Node<
  BatchGlobalStore,
  BatchLocalStore,
  ['process_one', 'aggregate']
> {
  async prep(memory: Memory<BatchGlobalStore>): Promise<any[]> {
    return memory.items_to_process ?? []
  }
  async post(memory: Memory<BatchGlobalStore>, items: any[], execRes: void): Promise<void> {
    memory.results = new Array(items.length).fill(null) // Pre-allocate
    items.forEach((item, index) => {
      this.trigger('process_one', { item_data: item, result_index: index })
    })
    // Optional: this.trigger("aggregate");
  }
}

class ProcessOneItemNode extends Node<BatchGlobalStore, BatchLocalStore> {
  async prep(
    memory: Memory<BatchGlobalStore, BatchLocalStore>,
  ): Promise<{ item: any; index: number }> {
    return { item: memory.item_data, index: memory.result_index ?? -1 }
  }
  async exec(prepRes: { item: any; index: number }): Promise<{ result: any; index: number }> {
    // const result = await processSingleItem(prepRes.item);
    const result = `Processed ${prepRes.item}` // Placeholder
    return { result, index: prepRes.index }
  }
  async post(
    memory: Memory<BatchGlobalStore>,
    prepRes: any,
    execRes: { result: any; index: number },
  ): Promise<void> {
    if (!memory.results) memory.results = []
    while (memory.results.length <= execRes.index) {
      memory.results.push(null)
    }
    memory.results[execRes.index] = execRes.result
  }
}

// Setup
const trigger = new TriggerBatchNode()
const processor = new ProcessOneItemNode()
trigger.on('process_one', processor)

// Choose Flow type
// const sequentialBatchFlow = new Flow<BatchGlobalStore>(trigger);
const parallelBatchFlow = new ParallelFlow<BatchGlobalStore>(trigger)

// Run
// async function run() {
//   const memory: BatchGlobalStore = { items_to_process: ["A", "B", "C"] };
//   await parallelBatchFlow.run(memory);
//   console.log(memory.results); // Output: ['Processed A', 'Processed B', 'Processed C']
// }
// run();
```

In this example, `TriggerBatchNode` fans out the work. If run with `ParallelFlow`, each `ItemProcessorNode` instance would execute concurrently. If run with a standard `Flow`, they would execute sequentially.

## Best Practices

- **Start Simple**: Begin with a linear flow and add branching/looping complexity gradually.
- **Visualize First**: Sketch your flow diagram (using Mermaid or similar tools) before coding to clarify logic.
- **Flow Modularity**: Design flows as reusable components. Break down complex processes into smaller, nested sub-flows.
- **Memory Planning**: Define clear interfaces for your `GlobalStore` and `LocalStore` upfront. Decide what state needs to be global versus what can be passed locally via `forkingData`.
- **Action Naming**: Use descriptive, meaningful action names (e.g., 'user_clarification_needed', 'data_validated') instead of generic names like 'next' or 'step2'.
- **Explicit Transitions**: Clearly define transitions for all expected actions a node might trigger. Consider adding default `.next()` transitions for unexpected or completion actions.
- **Cycle Management**: Be mindful of loops. Use the `maxVisits` option in the `Flow` constructor to prevent accidental infinite loops.
- **Error Strategy**: Decide how errors should propagate. Should a node's `execFallback` handle errors and allow the flow to continue, or should errors terminate the flow? Define specific error actions (`node.on('error', errorHandlerNode)`) if needed.
- **Parallelism Choice**: Use `ParallelFlow` when branches are independent and can benefit from concurrent execution. Stick with `Flow` (sequential) if branches have dependencies or shared resource contention.
- **Memory Isolation**: Leverage `forkingData` in `trigger` calls to pass data down specific branches via the `local` store, keeping the `global` store cleaner. This is crucial for parallel execution.
- **Test Incrementally**: Test individual nodes using `node.run()` and test sub-flows before integrating them into larger pipelines.
- **Avoid Deep Nesting**: While nesting flows is powerful, keep the hierarchy reasonably flat (e.g., 2-3 levels deep) for maintainability.

Flows provide the orchestration layer that determines how your nodes interact, ensuring that data moves predictably through your application and that execution follows your intended paths.

By following these principles, you can create complex, maintainable AI applications that are easy to reason about and extend.




================================================
File: docs/core_abstraction/memory.md
================================================
# Memory: Managing State Between Nodes

BrainyFlow provides a streamlined approach for components to communicate with each other. This chapter explains how data is stored, accessed, and isolated.

## Memory Scopes: Global vs. Local

Each `Memory` instance encapsulates two distinct scopes:

1.  **Global Store (`memory`)**: A single object shared across _all_ nodes within a single `flow.run()` execution. Changes made here persist throughout the flow. Think of it as the main shared state.
2.  **Local Store (`memory.local`)**: An object specific to a particular execution path within the flow. It's created when a node `trigger`s a successor. Changes here are isolated to that specific branch and its descendants.

This dual-scope system allows for both shared application state (global) and controlled, path-specific data propagation (local).

{% hint style="success" %}
**Real-World Analogies**:

Think of the memory system like **a river delta**:

- **Global Store**: The main river water that flows everywhere
- **Local Store**: Specific channels that might carry unique properties that only affect downstream areas fed by that channel

Alternatively, think of it like **nested scopes in programming**:

- **Global Store**: Like variables declared in the outermost scope of a program, accessible everywhere.
- **Local Store**: Like variables declared inside a function or block. They are only accessible within that block and any nested blocks (downstream nodes in the flow). If a local variable has the same name as a global one, the local variable "shadows" the global one within its scope.

This model gives you the flexibility to share data across your entire flow (global) or isolate context to specific execution paths (local).
{% endhint %}

## Accessing Memory (Reading)

Nodes access data stored in either scope through the `memory` proxy instance passed to their `prep` and `post` methods. When you read a property (e.g., `memory.someValue`), the proxy automatically performs a lookup:

1.  It checks the **local store (`memory.local`)** first.
2.  If the property is not found locally, it checks the **global store (`memory`)**.

```typescript
import { Memory, Node } from 'brainyflow'

interface MyGlobal {
  config?: object
  commonData?: string
  pathSpecificData?: string
}
interface MyLocal {
  pathSpecificData?: string
} // Can shadow global

class MyNode extends Node<MyGlobal, MyLocal> {
  async prep(memory: Memory<MyGlobal, MyLocal>): Promise<void> {
    // Reads from global store (assuming not set locally)
    const config = memory.config
    const common = memory.commonData

    // Reads from local store if set via forkingData, otherwise reads from global
    const specific = memory.pathSpecificData
  }
  // ... exec, post ...
}
```

When accessing memory, you should always use `memory.someValue` and let the `Memory` manager figure out where to fetch the value for you.
You could also directly access the entire local store object using `memory.local` - or a value at `memory.local.someValue` - but that's an anti-pattern that should be avoided.

```typescript
async post(memory: Memory<MyGlobal, MyLocal>, /*...*/) {
    const allLocalData = memory.local; // Access the internal __local object directly
    console.log('Current local store:', allLocalData);
}
```

## Writing to Memory

- **Writing to Global Store**: Assigning a value directly to a property on the `memory` object (e.g., `memory.someValue = 'new data'`) writes to the **global store**. The proxy automatically removes the property from the local store first if it exists there.
- **Writing to Local Store**: While possible via `memory.local.someValue = 'new data'`, the primary and recommended way to populate the local store for downstream nodes is using the `forkingData` argument in `this.trigger()`.

```typescript
import { Memory, Node } from 'brainyflow'

interface MyGlobal {
  fileList?: string[]
  results?: Record<string, any>
}
interface MyLocal {
  processedCount?: number // Data passed via memory.local assignment
  file?: string // Data passed via forkingData
}

class DataWriterNode extends Node<MyGlobal, MyLocal, ['process_file']> {
  async post(
    memory: Memory<MyGlobal, MyLocal>,
    prepRes: any,
    execRes: { files: string[]; count: number }, // Assume exec returns this format
  ): Promise<void> {
    // --- Writing to Global Store ---
    // Accessible to all nodes in the flow
    memory.fileList = execRes.files
    console.log(`Memory updated globally: fileList=${memory.fileList}`)

    // --- Writing to Local Store ---
    // Accessible to this node and all descendants
    memory.local.processedCount = execRes.count
    console.log(`Memory updated locally: processedCount=${memory.processedCount}`)

    // --- Triggering with Local Data (Forking Data) ---
    // 'file' will be added to the local store of the memory clone
    // passed to the node(s) triggered by the 'process_file' action.
    for (const file of execRes.files) {
      this.trigger('process_file', { file: file })
    }
  }
}

// Example Processor Node (triggered by 'process_file')
class FileProcessorNode extends Node<MyGlobal, MyLocal> {
  async prep(memory: Memory<MyGlobal, MyLocal>): Promise<string | undefined> {
    // Reads 'file' from the local store first, then global
    const fileToProcess = memory.file
    console.log(`Processing file (from local memory): ${fileToProcess}`)
    return fileToProcess
  }
  // ... exec, post ...
}
```

## Best Practices

- **Read in `prep()`**: Gather necessary input data from `memory` at the beginning of a node's execution.
- **Write Global State in `post()`**: Update the shared global store by assigning to `memory` properties (e.g., `memory.results = ...`) in the `post()` phase after processing is complete.
- **Set Local State via `forkingData`**: Pass branch-specific context to successors by providing the `forkingData` argument in `this.trigger()` within the parent's `post()` method. This populates the `local` store for the next node(s).
- **Read Transparently**: Always read data via the `memory` proxy (e.g., `memory.someValue`). It handles the local-then-global lookup automatically. Avoid reading directly from `memory.local` or other internal properties unless strictly needed.

## When to Use The Memory

- **Ideal for**: Sharing data results, large content, or information needed by multiple components
- **Benefits**: Separates data from computation logic (separation of concerns)
- **Global Memory**: Use for application-wide state, configuration, and final results
- **Local Memory**: Use for passing contextual data down a specific execution path

## Technical Concepts

The memory system in BrainyFlow implements several established computer science patterns:

- **Lexical Scoping**: Local memory "shadows" global memory, similar to how local variables in functions can shadow global variables
- **Context Propagation**: Local memory propagates down the execution tree, similar to how context flows in React or middleware systems
- **Transparent Resolution**: The system automatically resolves properties from the appropriate memory scope

## Remember

1.  **Reading**: Always read via the `memory` proxy (e.g., `memory.value`). It checks local then global.
2.  **Writing**: Direct assignment `memory.property = value` writes to the **global** store.
3.  **Local State Creation**: Use `trigger(action, forkingData)` in `post()` to populate the `local` store for the _next_ node(s) in a specific branch.
4.  **Lifecycle**: Read from `memory` in `prep`, compute in `exec` (no memory access), write global state to `memory` and trigger successors (potentially with `forkingData` for local state) in `post`.




================================================
File: docs/design_pattern/index.md
================================================
# Design Patterns

BrainyFlow supports a variety of design patterns that enable you to build complex AI applications. These patterns leverage the core abstractions of nodes, flows, and shared store to implement common AI system architectures.

## Overview of Design Patterns

<div align="center">
  <img src="https://raw.githubusercontent.com/zvictor/brainyflow/main/.github/media/design.jpg" width="1300"/>
</div>

BrainyFlow's minimalist design allows it to support various high-level AI design paradigms:

| Pattern                             | Description                                                     | Use Cases                                       |
| ----------------------------------- | --------------------------------------------------------------- | ----------------------------------------------- |
| [RAG](./rag.md)                     | Retrieval-Augmented Generation for knowledge-grounded responses | Question answering, knowledge-intensive tasks   |
| [Agent](./agent.md)                 | Autonomous entities that can perceive, reason, and act          | Virtual assistants, autonomous decision-making  |
| [Workflow](./workflow.md)           | Sequential or branching business processes                      | Form processing, approval flows                 |
| [MapReduce](./map_reduce.md)        | Distributed processing of large datasets                        | Document summarization, parallel processing     |
| [Structured Output](./structure.md) | Generating outputs that follow specific schemas                 | Data extraction, configuration generation       |
| [Multi-Agents](./multi_agent.md)    | Multiple agents working together on complex tasks               | Collaborative problem-solving, role-based tasks |

## Choosing the Right Pattern

When designing your application, consider these factors when selecting a pattern:

| Pattern           | Best For                  | When To Use                                             |
| ----------------- | ------------------------- | ------------------------------------------------------- |
| RAG               | Knowledge-intensive tasks | When external information is needed for responses       |
| Agent             | Dynamic problem-solving   | When tasks require reasoning and decision-making        |
| Workflow          | Sequential processing     | When steps are well-defined and follow a clear order    |
| Map Reduce        | Large data processing     | When handling datasets too large for a single operation |
| Structured Output | Consistent formatting     | When outputs need to follow specific schemas            |
| Multi-Agents      | Complex collaboration     | When tasks benefit from specialized agent roles         |

### Decision Tree

Use this decision tree to help determine which pattern best fits your use case:

```mermaid
flowchart TD
    A[Start] --> B{Need to process large data?}
    B -->|Yes| C{Data can be processed independently?}
    B -->|No| D{Need to make decisions?}

    C -->|Yes| E[Map Reduce]
    C -->|No| F[Workflow]

    D -->|Yes| G{Complex, multi-step reasoning?}
    D -->|No| H[Simple Workflow]

    G -->|Yes| I{Need multiple specialized roles?}
    G -->|No| J{Need external knowledge?}

    I -->|Yes| K[Multi-Agents]
    I -->|No| L[Agent]

    J -->|Yes| M[RAG]
    J -->|No| N[Structured Output]
```

## Pattern Composition

BrainyFlow's nested flow capability allows you to compose multiple patterns. For instance:

```mermaid
graph TD
    subgraph "Agent Pattern"
        A[Perceive] --> B[Think]
        B --> C[Act]
        C --> A
    end

    subgraph "RAG Pattern"
        D[Query] --> E[Retrieve]
        E --> F[Generate]
    end

    A --> D
    F --> B
```

This composition enables powerful applications that combine the strengths of different patterns.
For example, an agent might use RAG to access knowledge, then apply chain-of-thought reasoning to solve a problem.
Other examples include:

- An **Agent** that uses **RAG** to retrieve information before making decisions
- A **Workflow** that includes **Map Reduce** steps for processing large datasets
- **Multi-Agents** that each use **Structured Output** for consistent communication

## Implementation Examples

Each pattern can be implemented using BrainyFlow's core abstractions. Here's a simple example of the agent pattern:

```typescript
import { Flow, Node } from 'brainyflow'

// Define the agent's components (assuming these classes exist)
const perceive = new PerceiveNode()
const think = new ThinkNode()
const act = new ActNode()

// Connect them in a cycle
perceive.next(think).next(act).next(perceive)

// Create the agent flow
const agentFlow = new Flow(perceive)
```

For more detailed implementations of each pattern, see the individual pattern documentation pages.

## Best Practices

1. **Start Simple**: Begin with the simplest pattern that meets your needs
2. **Modular Design**: Design patterns to be composable and reusable
3. **Clear Interfaces**: Define clear interfaces between pattern components
4. **Test Incrementally**: Test each pattern component before integration
5. **Monitor Performance**: Watch for bottlenecks in your pattern implementation

By understanding and applying these design patterns, you can build sophisticated AI applications that are both powerful and maintainable.




================================================
File: docs/design_pattern/agent.md
================================================
# Agent

Agent is a powerful design pattern in which nodes can take dynamic actions based on the context.

<div align="center">
  <img src="https://github.com/zvictor/brainyflow/raw/main/.github/media/agent.png?raw=true" width="350"/>
</div>

## Implement Agent with Graph

1. **Context and Action:** Implement nodes that supply context and perform actions.
2. **Branching:** Use branching to connect each action node to an agent node. Use action to allow the agent to direct the [flow](../core_abstraction/flow.md) between nodes—and potentially loop back for multi-step.
3. **Agent Node:** Provide a prompt to decide action—for example:

```typescript
;`### CONTEXT
Task: ${taskDescription}
Previous Actions: ${previousActions}
Current State: ${currentState}

### ACTION SPACE
[1] search
  Description: Use web search to get results
  Parameters:
    - query (string): What to search for

[2] answer
  Description: Conclude based on the results  
  Parameters:
    - result (string): Final answer to provide

### NEXT ACTION
Decide the next action based on the current context and available action space.
Return your response in the following format:

\`\`\`yaml
thinking: |
    <your step-by-step reasoning process>
action: <action_name>
parameters:
    <parameter_name>: <parameter_value>
\`\`\``
```

The core of building **high-performance** and **reliable** agents boils down to:

1. **Context Management:** Provide _relevant, minimal context._ For example, rather than including an entire chat history, retrieve the most relevant via [RAG](./rag.md). Even with larger context windows, LLMs still fall victim to ["lost in the middle"](https://arxiv.org/abs/2307.03172), overlooking mid-prompt content.

2. **Action Space:** Provide _a well-structured and unambiguous_ set of actions—avoiding overlap like separate `read_databases` or `read_csvs`. Instead, import CSVs into the database.

## Example Good Action Design

- **Incremental:** Feed content in manageable chunks (500 lines or 1 page) instead of all at once.

- **Overview-zoom-in:** First provide high-level structure (table of contents, summary), then allow drilling into details (raw texts).

- **Parameterized/Programmable:** Instead of fixed actions, enable parameterized (columns to select) or programmable (SQL queries) actions, for example, to read CSV files.

- **Backtracking:** Let the agent undo the last step instead of restarting entirely, preserving progress when encountering errors or dead ends.

## Example: Search Agent

This agent:

1. Decides whether to search or answer
2. If searches, loops back to decide if more search needed
3. Answers when enough context gathered

````typescript
import { Flow, Memory, Node } from 'brainyflow'

// Assume callLLM and parseYaml are defined elsewhere
declare function callLLM(prompt: string): Promise<string>
declare function parseYaml(text: string): any

class DecideAction extends Node {
  async prep(memory: Memory): Promise<{ query: string; context: any }> {
    // Read from memory
    const context = memory.context ?? 'No previous search'
    const query = memory.query
    return { query, context }
  }

  async exec(prepRes: { query: string; context: any }): Promise<any> {
    const { query, context } = prepRes
    const prompt = `
Given input: ${query}
Previous search results: ${JSON.stringify(context)}
Should I: 1) Search web for more info ('search') 2) Answer with current knowledge ('answer')
Output in yaml:
\`\`\`yaml
action: search | answer
reason: <why this action>
search_term: <search phrase if action is search>
\`\`\``
    const resp = await callLLM(prompt)
    // Simplified parsing/validation for docs
    const yamlStr = resp.split(/```(?:yaml)?/)[1]?.trim()
    if (!yamlStr) {
      throw new Error('Missing YAML response')
    }

    const result = parseYaml(yamlStr)

    if (typeof result !== 'object' || !result) {
      throw new Error('Invalid YAML response')
    }
    if (!('action' in result)) {
      throw new Error('Missing action in response')
    }
    if (!('reason' in result)) {
      throw new Error('Missing reason in response')
    }
    if (!['search', 'answer'].includes(result.action)) {
      throw new Error('Invalid action value')
    }
    if (result.action === 'search' && !('search_term' in result)) {
      throw new Error('Missing search_term for search action')
    }

    return result
  }

  async post(memory: Memory, prepRes: any, execRes: any): Promise<void> {
    // Write search term if needed
    if (execRes.action === 'search' && execRes.search_term) {
      memory.search_term = execRes.search_term
    }
    // Trigger the decided action
    this.trigger(execRes.action)
  }
}
````

```typescript
import { Memory, Node } from 'brainyflow'

declare function searchWeb(query: string): Promise<any> // Assume `searchWeb` is defined elsewhere

class SearchWeb extends Node {
  async prep(memory: Memory): Promise<string> {
    // Read search term from memory
    return memory.search_term
  }

  async exec(searchTerm: string): Promise<any> {
    return await searchWeb(searchTerm)
  }

  async post(memory: Memory, prepRes: string, execRes: any): Promise<void> {
    // Add search result to context (simplified)
    const prevContext = memory.context ?? []
    memory.context = [...prevContext, { term: prepRes, result: execRes }]
    // Trigger loop back to decide
    this.trigger('decide')
  }
}
```

```typescript
import { Memory, Node } from 'brainyflow'

declare function callLLM(prompt: string): Promise<string> // Assuming callLLM is defined elsewhere

class DirectAnswer extends Node {
  async prep(memory: Memory): Promise<{ query: string; context: any }> {
    // Read query and context
    return { query: memory.query, context: memory.context ?? 'No context' }
  }

  async exec(prepRes: { query: string; context: any }): Promise<string> {
    // Generate answer based on context
    const prompt = `Context: ${JSON.stringify(prepRes.context)}\nAnswer Query: ${prepRes.query}`
    return await callLLM(prompt)
  }

  async post(memory: Memory, prepRes: any, execRes: string): Promise<void> {
    // Store final answer
    memory.answer = execRes
    console.log(`Answer: ${execRes}`)
    // No trigger needed - end of this path
  }
}
```

```typescript
import { Flow } from 'brainyflow'

// Assuming DecideAction, SearchWeb, DirectAnswer classes are defined

// Instantiate nodes
const decide = new DecideAction()
const search = new SearchWeb()
const answer = new DirectAnswer()

// Define transitions
decide.on('search', search)
decide.on('answer', answer)
search.on('decide', decide) // Loop back

// Create the flow
const agentFlow = new Flow(decide)

// --- Main execution function ---
async function runAgent() {
  const initialMemory = { query: 'Who won the Nobel Prize in Physics 2024?' }
  console.log(`Starting agent flow with query: "${initialMemory.query}"`)

  try {
    await agentFlow.run(initialMemory) // Run the flow with memory object
    console.log('\n--- Flow Complete ---')
    console.log('Final Memory State:', initialMemory) // Log final memory state
    console.log('\nFinal Answer:', initialMemory.answer ?? 'No answer found')
  } catch (error) {
    console.error('\n--- Agent Flow Failed ---', error)
    console.error('Memory State on Failure:', initialMemory) // Log memory state on failure
  }
}

// Run the main function
runAgent()
```




================================================
File: docs/design_pattern/workflow.md
================================================
# Workflow

Many real-world tasks are too complex for one LLM call. The solution is **Task Decomposition**: decompose them into a [chain](../core_abstraction/flow.md) of multiple Nodes.

<div align="center">
  <img src="https://github.com/zvictor/brainyflow/raw/main/.github/media/workflow.png?raw=true" width="400"/>
</div>

{% hint style="success" %}
You don't want to make each task **too coarse**, because it may be _too complex for one LLM call_.
You don't want to make each task **too granular**, because then _the LLM call doesn't have enough context_ and results are _not consistent across nodes_.

You usually need multiple _iterations_ to find the _sweet spot_. If the task has too many _edge cases_, consider using [Agents](./agent.md).
{% endhint %}

### Example: Article Writing

```typescript
import { Flow, Memory, Node } from 'brainyflow'

// Assuming callLLM is defined elsewhere
declare function callLLM(prompt: string): Promise<string>

class GenerateOutline extends Node {
  async prep(memory: Memory): Promise<string> {
    return memory.topic // Read topic from memory
  }
  async exec(topic: string): Promise<string> {
    console.log(`Generating outline for: ${topic}`)
    return await callLLM(`Create a detailed outline for an article about ${topic}`)
  }
  async post(memory: Memory, prepRes: any, outline: string): Promise<void> {
    memory.outline = outline // Store outline in memory
    this.trigger('default')
  }
}

class WriteSection extends Node {
  async prep(memory: Memory): Promise<string> {
    return memory.outline // Read outline from memory
  }
  async exec(outline: string): Promise<string> {
    console.log('Writing draft based on outline...')
    return await callLLM(`Write content based on this outline: ${outline}`)
  }
  async post(memory: Memory, prepRes: any, draft: string): Promise<void> {
    memory.draft = draft // Store draft in memory
    this.trigger('default')
  }
}

class ReviewAndRefine extends Node {
  async prep(memory: Memory): Promise<string> {
    return memory.draft // Read draft from memory
  }
  async exec(draft: string): Promise<string> {
    console.log('Reviewing and refining draft...')
    return await callLLM(`Review and improve this draft: ${draft}`)
  }
  async post(memory: Memory, draft: any, finalArticle: string): Promise<void> {
    memory.final_article = finalArticle // Store final article
    console.log('Final article generated.')
    // No trigger needed - end of workflow
  }
}

// --- Flow Definition ---
const outline = new GenerateOutline()
const write = new WriteSection()
const review = new ReviewAndRefine()

// Connect nodes sequentially using default trigger
outline.next(write).next(review)

// Create the flow
const writingFlow = new Flow(outline)

// --- Execution ---
async function main() {
  const data = { topic: 'AI Safety' }
  console.log(`Starting writing workflow for topic: "${data.topic}"`)

  await writingFlow.run(data) // Run the flow

  console.log('\n--- Workflow Complete ---')
  console.log('Final Memory State:', data)
  console.log(`\nFinal Article:\n${data.final_article ?? 'Not generated'}`)
}

main().catch(console.error)
```

For _dynamic cases_, consider using [Agents](./agent.md).




================================================
File: docs/design_pattern/rag.md
================================================
# RAG (Retrieval Augmented Generation)

For certain LLM tasks like answering questions, providing relevant context is essential. One common architecture is a **two-stage** RAG pipeline:

<div align="center">
  <img src="https://github.com/zvictor/brainyflow/raw/main/.github/media/rag.png?raw=true" width="400"/>
</div>

1. **Offline stage**: Preprocess and index documents ("building the index").
2. **Online stage**: Given a question, generate answers by retrieving the most relevant context.

---

## Stage 1: Offline Indexing

We create three Nodes:

1. `ChunkDocs` – [chunks](../utility_function/chunking.md) raw text.
2. `EmbedDocs` – [embeds](../utility_function/embedding.md) each chunk.
3. `StoreIndex` – stores embeddings into a [vector database](../utility_function/vector.md).

```typescript
import * as fs from 'fs'
import { Flow, Memory, Node, ParallelFlow } from 'brainyflow'

// Assume getEmbedding and createIndex/searchIndex are defined elsewhere
declare function getEmbedding(text: string): Promise<number[]>
declare function createIndex(embeddings: number[][]): any // Returns index object
declare function searchIndex(
  index: any,
  queryEmbedding: number[],
  topK: number,
): [number[][], number[][]] // Returns [[ids]], [[distances]]

// --- Stage 1: Offline Indexing Nodes ---

// 1a. Node to trigger chunking for each file
class TriggerChunkingNode extends Node {
  async prep(memory: Memory): Promise<string[]> {
    return memory.files ?? [] // Expects memory.files = ['doc1.txt', ...]
  }
  async exec(files: string[]): Promise<number> {
    return files.length
  }
  async post(memory: Memory, prepRes: string[], fileCount: number): Promise<void> {
    console.log(`Triggering chunking for ${fileCount} files.`)
    memory.all_chunks = [] // Initialize chunk store
    ;(prepRes as string[]).forEach((filepath, index) => {
      this.trigger('chunk_file', { filepath, index }) // Pass filepath via local memory
    })
    this.trigger('embed_chunks') // Trigger embedding after all files are processed
  }
}

// 1b. Node to chunk a single file
class ChunkFileNode extends Node {
  async prep(memory: Memory): Promise<{ filepath: string; index: number }> {
    return { filepath: memory.filepath, index: memory.index } // Read from local memory
  }
  async exec(prepRes: { filepath: string; index: number }): Promise<string[]> {
    console.log(`Chunking ${prepRes.filepath}`)
    const text = fs.readFileSync(prepRes.filepath, 'utf-8')
    const chunks: string[] = []
    const size = 100 // Simple fixed-size chunking
    for (let i = 0; i < text.length; i += size) {
      chunks.push(text.slice(i, i + size))
    }
    return chunks
  }
  async post(memory: Memory, prepRes: { index: number }, chunks: string[]): Promise<void> {
    // Add chunks to the global list (careful with concurrency if using ParallelFlow)
    // A safer parallel approach might store chunks per file then combine later.
    memory.all_chunks.push(...chunks)
  }
}

// 1c. Node to trigger embedding for each chunk
class TriggerEmbeddingNode extends Node {
  async prep(memory: Memory): Promise<string[]> {
    return memory.all_chunks ?? []
  }
  async exec(chunks: string[]): Promise<number> {
    return chunks.length
  }
  async post(memory: Memory, prepRes: string[], chunkCount: number): Promise<void> {
    console.log(`Triggering embedding for ${chunkCount} chunks.`)
    memory.all_embeds = [] // Initialize embedding store
    ;(prepRes as string[]).forEach((chunk, index) => {
      this.trigger('embed_chunk', { chunk, index })
    })
    this.trigger('store_index') // Trigger storing after all chunks processed
  }
}

// 1d. Node to embed a single chunk
class EmbedChunkNode extends Node {
  async prep(memory: Memory): Promise<{ chunk: string; index: number }> {
    return { chunk: memory.chunk, index: memory.index } // Read from local memory
  }
  async exec(prepRes: { chunk: string; index: number }): Promise<number[]> {
    console.log(`Embedding chunk ${prepRes.index}`)
    return await getEmbedding(prepRes.chunk)
  }
  async post(memory: Memory, prepRes: { index: number }, embedding: number[]): Promise<void> {
    // Store embedding in global list (careful with concurrency)
    memory.all_embeds[prepRes.index] = embedding
  }
}
```

```typescript
// 1e. Node to store the final index
class StoreIndexNode extends Node {
  async prep(memory: Memory): Promise<number[][]> {
    // Read all embeddings from global memory
    return memory.all_embeds ?? []
  }

  async exec(allEmbeds: number[][]): Promise<any> {
    console.log(`Storing index for ${allEmbeds.length} embeddings.`)
    // Create a vector index (implementation depends on library)
    const index = createIndex(allEmbeds)
    return index
  }

  async post(memory: Memory, prepRes: any, index: any): Promise<void> {
    // Store the created index in global memory
    memory.index = index
    console.log('Index created and stored.')
  }
}
```

```typescript
// --- Offline Flow Definition ---
const triggerChunking = new TriggerChunkingNode()
const chunkFile = new ChunkFileNode()
const triggerEmbedding = new TriggerEmbeddingNode()
const embedChunk = new EmbedChunkNode()
const storeIndex = new StoreIndexNode()

// Define transitions
triggerChunking.on('chunk_file', chunkFile)
triggerChunking.on('embed_chunks', triggerEmbedding)
triggerEmbedding.on('embed_chunk', embedChunk)
triggerEmbedding.on('store_index', storeIndex)

// Use ParallelFlow for chunking and embedding if desired
const OfflineFlow = new ParallelFlow(triggerChunking)
// Or sequential: const OfflineFlow = new Flow(triggerChunking);
```

Usage example:

```typescript
// --- Offline Flow Execution ---
async function runOffline() {
  const initialMemory = {
    files: ['doc1.txt', 'doc2.txt'], // Example file paths
  }
  console.log('Starting offline indexing flow...')
  // Create dummy files for example
  fs.writeFileSync('doc1.txt', 'Alice was beginning to get very tired.')
  fs.writeFileSync('doc2.txt', 'The quick brown fox jumps over the lazy dog.')

  await OfflineFlow.run(initialMemory)
  console.log('Offline indexing complete.')
  // Clean up dummy files
  fs.unlinkSync('doc1.txt')
  fs.unlinkSync('doc2.txt')
  return initialMemory // Return memory containing index, chunks, embeds
}
// runOffline(); // Example call
```

---

## Stage 2: Online Query & Answer

We have 3 nodes:

1. `EmbedQuery` – embeds the user’s question.
2. `RetrieveDocs` – retrieves top chunk from the index.
3. `GenerateAnswer` – calls the LLM with the question + chunk to produce the final answer.

```typescript
// --- Stage 2: Online Query Nodes ---

// 2a. Embed Query Node
class EmbedQueryNode extends Node {
  async prep(memory: Memory): Promise<string> {
    return memory.question // Expects question in global memory
  }
  async exec(question: string): Promise<number[]> {
    console.log(`Embedding query: "${question}"`)
    return await getEmbedding(question)
  }
  async post(memory: Memory, prepRes: any, qEmb: number[]): Promise<void> {
    memory.q_emb = qEmb // Store query embedding
    this.trigger('retrieve_docs')
  }
}
```

```typescript
// 2b. Retrieve Docs Node
class RetrieveDocsNode extends Node {
  async prep(memory: Memory): Promise<{ qEmb: number[]; index: any; chunks: string[] }> {
    // Need query embedding, index, and original chunks
    return { qEmb: memory.q_emb, index: memory.index, chunks: memory.all_chunks }
  }
  async exec(prepRes: { qEmb: number[]; index: any; chunks: string[] }): Promise<string> {
    const { qEmb, index, chunks } = prepRes
    if (!qEmb || !index || !chunks) {
      throw new Error('Missing data for retrieval')
    }
    console.log('Retrieving relevant chunk...')
    const [I, D] = searchIndex(index, qEmb, 1) // Find top 1 chunk
    const bestId = I?.[0]?.[0]
    if (bestId === undefined || bestId >= chunks.length) {
      return 'Could not find relevant chunk.'
    }
    const relevantChunk = chunks[bestId]
    return relevantChunk
  }
  async post(memory: Memory, prepRes: any, relevantChunk: string): Promise<void> {
    memory.retrieved_chunk = relevantChunk
    console.log(`Retrieved chunk: ${relevantChunk.slice(0, 60)}...`)
    this.trigger('generate_answer')
  }
}
```

```typescript
// 2c. Generate Answer Node
class GenerateAnswerNode extends Node {
  async prep(memory: Memory): Promise<{ question: string; chunk: string }> {
    return { question: memory.question, chunk: memory.retrieved_chunk }
  }
  async exec(prepRes: { question: string; chunk: string }): Promise<string> {
    const { question, chunk } = prepRes
    const prompt = `Using the following context, answer the question.
Context: ${chunk}
Question: ${question}
Answer:`
    console.log('Generating final answer...')
    return await callLLM(prompt)
  }
  async post(memory: Memory, prepRes: any, answer: string): Promise<void> {
    memory.answer = answer // Store final answer
    console.log(`Answer: ${answer}`)
    // End of flow
  }
}
```

```typescript
// --- Online Flow Definition ---
const embedQueryNode = new EmbedQueryNode()
const retrieveDocsNode = new RetrieveDocsNode()
const generateAnswerNode = new GenerateAnswerNode()

// Define transitions
embedQueryNode.on('retrieve_docs', retrieveDocsNode)
retrieveDocsNode.on('generate_answer', generateAnswerNode)

const OnlineFlow = new Flow(embedQueryNode)
```

Usage example:

```typescript
// --- Online Flow Execution ---
async function runOnline(memoryFromOffline: any) {
  // Add the user's question to the memory from the offline stage
  memoryFromOffline.question = 'Why do people like cats?'

  console.log(`\nStarting online RAG flow for question: "${memoryFromOffline.question}"`)
  await OnlineFlow.run(memoryFromOffline)
  console.log('Online RAG complete.')
  return memoryFromOffline
}

// --- Combined Example ---
async function runFullRAG() {
  // Mock external functions for example
  globalThis.getEmbedding = async (text: string) => Array(5).fill(Math.random())
  globalThis.createIndex = (embeds: number[][]) => ({
    search: (q: number[], k: number) => [[Math.floor(Math.random() * embeds.length)]],
  }) // Mock index
  globalThis.searchIndex = (index: any, q: number[], k: number) => index.search(q, k)
  globalThis.callLLM = async (prompt: string) => `Mock LLM answer for: ${prompt.split('\n')[1]}`

  const memoryAfterOffline = await runOffline()
  const finalMemory = await runOnline(memoryAfterOffline)

  console.log('\n--- Full RAG Result ---')
  console.log('Final Answer:', finalMemory.answer)
}

runFullRAG().catch(console.error)
```




================================================
File: docs/design_pattern/mapreduce.md
================================================
# Map Reduce

MapReduce is a design pattern suitable when you need to process multiple pieces of data (e.g., files, records) independently and then combine the results.

<div align="center">
  <img src="https://github.com/zvictor/brainyflow/raw/main/.github/media/mapreduce.png?raw=true" width="400"/>
</div>

In BrainyFlow, this pattern is typically implemented using:

1.  **Map Phase:** A "Mapper" node triggers multiple instances of a "Processor" node, one for each piece of data. It uses `trigger` with `forkingData` to pass the specific data item to each processor via its local memory. Using `ParallelFlow` here allows processors to run concurrently.
2.  **Reduce Phase:** A "Reducer" node runs after the map phase. It reads the individual results (accumulated in the global memory by the processor nodes) and aggregates them into a final output.

### Example: Document Summarization

```typescript
import { Flow, Memory, Node, ParallelFlow } from 'brainyflow'

// Assume callLLM is defined elsewhere
declare function callLLM(prompt: string): Promise<string>

// 1. Mapper Node: Triggers processing for each file
class TriggerSummariesNode extends Node {
  async prep(memory: Memory): Promise<[string, string][]> {
    // Get file data from global memory
    const filesDict = memory.files ?? {}
    return Object.entries(filesDict) // [["file1.txt", "content1"], ...]
  }

  async exec(files: [string, string][]): Promise<number> {
    // No main computation needed here, just return the count for info
    return files.length
  }

  async post(memory: Memory, filesToProcess: [string, string][], fileCount: number): Promise<void> {
    console.log(`Mapper: Triggering summary for ${fileCount} files.`)
    // Initialize results array and counter in global memory
    // Note: Using an array might still have race conditions if indices aren't guaranteed
    // in ParallelFlow. An object map { index: summary } might be safer.
    memory.file_summaries = new Array(fileCount).fill(null)
    memory.remaining_summaries = fileCount // Add counter
    // Trigger a 'summarize_file' action for each file
    filesToProcess.forEach(([filename, content], index) => {
      this.trigger('summarize_file', { filename, content, index })
    })
    // NOTE: 'combine_summaries' is now triggered by SummarizeFileNode when the counter reaches zero.
  }
}

// 2. Processor Node: Summarizes a single file
class SummarizeFileNode extends Node {
  async prep(memory: Memory): Promise<{ filename: string; content: string; index: number }> {
    // Read specific file data from local memory (passed via forkingData)
    return { filename: memory.filename, content: memory.content, index: memory.index }
  }

  async exec(fileData: { filename: string; content: string; index: number }): Promise<string> {
    // Summarize the content
    console.log(`Processor: Summarizing ${fileData.filename} (Index ${fileData.index})`)
    return await callLLM(`Summarize this:\n${fileData.content}`)
  }

  async post(
    memory: Memory,
    prepRes: { index: number; filename: string },
    summary: string,
  ): Promise<void> {
    // Store individual summary in global memory at the correct index
    // Note: Direct array index assignment might cause issues with ParallelFlow if order matters
    // A safer approach might be to push {filename, summary} and sort later, or use an object.
    memory.file_summaries[prepRes.index] = { filename: prepRes.filename, summary }
    console.log(`Processor: Finished ${prepRes.filename} (Index ${prepRes.index})`)
    // Decrement counter and trigger combine if this is the last summary
    memory.remaining_summaries--
    if (memory.remaining_summaries === 0) {
      console.log('Processor: All summaries collected, triggering combine.')
      this.trigger('combine_summaries')
    }
  }
}

// 3. Reducer Node: Combines individual summaries
class CombineSummariesNode extends Node {
  async prep(memory: Memory): Promise<any[]> {
    // Read the array of individual summaries
    return memory.file_summaries ?? []
  }

  async exec(summaries: { filename: string; summary: string }[]): Promise<string> {
    console.log(`Reducer: Combining ${summaries.length} summaries.`)
    if (!summaries || summaries.length === 0) {
      return 'No summaries to combine.'
    }
    // Format summaries for the final prompt
    const combinedText = summaries.map((s) => `${s.filename}:\n${s.summary}`).join('\n\n---\n\n')
    return await callLLM(`Combine these summaries into one final summary:\n${combinedText}`)
  }

  async post(memory: Memory, prepRes: any, finalSummary: string): Promise<void> {
    // Store the final combined summary
    memory.final_summary = finalSummary
    console.log('Reducer: Final summary generated.')
  }
}

// --- Flow Definition ---
const triggerNode = new TriggerSummariesNode()
const processorNode = new SummarizeFileNode()
const reducerNode = new CombineSummariesNode()

// Define transitions
triggerNode.on('summarize_file', processorNode) // Map step
triggerNode.on('combine_summaries', reducerNode) // Reduce step (runs after all triggers are processed by the flow runner)

// Use ParallelFlow for potentially faster summarization
const mapReduceFlow = new ParallelFlow(triggerNode)
// Alternatively, if strict sequential processing is acceptable or required (e.g., to avoid
// the complexity of the counter mechanism), you can use a standard Flow:
// const mapReduceFlow = new Flow(triggerNode);
// This ensures the 'combine_summaries' step (triggered by the last processor)
// only runs after all 'summarize_file' steps are complete.

// --- Execution ---
async function main() {
  const memory = {
    files: {
      'file1.txt': 'Alice was beginning to get very tired of sitting by her sister...',
      'file2.txt': 'The quick brown fox jumps over the lazy dog.',
      'file3.txt': 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.',
    },
  }
  await mapReduceFlow.run(memory)
  console.log('\n--- MapReduce Complete ---')
  console.log('Individual Summaries:', memory.file_summaries)
  console.log('\nFinal Summary:\n', memory.final_summary)
}

main().catch(console.error)
```




================================================
File: docs/design_pattern/structure.md
================================================
# Structured Output

In many use cases, you may want the LLM to output a specific structure, such as a list or a dictionary with predefined keys.

There are several approaches to achieve a structured output:

- **Prompting** the LLM to strictly return a defined structure.
- Using LLMs that natively support **schema enforcement**.
- **Post-processing** the LLM's response to extract structured content.

In practice, **Prompting** is simple and reliable for modern LLMs.

### Example Use Cases

- Extracting Key Information

```yaml
product:
  name: Widget Pro
  price: 199.99
  description: |
    A high-quality widget designed for professionals.
    Recommended for advanced users.
```

- Summarizing Documents into Bullet Points

```yaml
summary:
  - This product is easy to use.
  - It is cost-effective.
  - Suitable for all skill levels.
```

- Generating Configuration Files

```yaml
server:
  host: 127.0.0.1
  port: 8080
  ssl: true
```

## Prompt Engineering

When prompting the LLM to produce **structured** output:

1. **Wrap** the structure in code fences (e.g., `yaml`).
2. **Validate** that all required fields exist (and let `Node` handles retry).

### Example Text Summarization

````typescript
import { Memory, Node } from 'brainyflow'

// Assuming callLLM and a YAML parser are available
declare function callLLM(prompt: string): Promise<string>
declare function parseYaml(text: string): any

class SummarizeNode extends Node {
  async prep(memory: Memory): Promise<string> {
    // Assuming the text to summarize is in memory.text
    return memory.text ?? ''
  }

  async exec(textToSummarize: string): Promise<any> {
    if (!textToSummarize) return { summary: ['No text provided'] }

    const prompt = `
Please summarize the following text as YAML, with exactly 3 bullet points:

${textToSummarize}

Now, output ONLY the YAML structure:
\`\`\`yaml
summary:
  - bullet 1
  - bullet 2
  - bullet 3
\`\`\``

    const response = await callLLM(prompt)
    let structuredResult: any
    try {
      const yamlStr = response.split(/```(?:yaml)?/)[1]?.trim()
      if (!yamlStr) throw new Error('No YAML block found')
      structuredResult = parseYaml(yamlStr)

      // Basic validation
      if (!structuredResult?.summary || !Array.isArray(structuredResult.summary)) {
        throw new Error('Invalid YAML structure: missing or non-array summary')
      }
    } catch (e: any) {
      console.error('Failed to parse structured output:', e.message)
      // Handle error, maybe return a default structure or re-throw
      // Returning the raw response might be an option too
      return { summary: [`Error parsing summary: ${e.message}`] }
    }

    return structuredResult // e.g., { summary: ['Point 1', 'Point 2', 'Point 3'] }
  }

  async post(memory: Memory, prepRes: any, execRes: any): Promise<void> {
    // Store the structured result in memory
    memory.structured_summary = execRes
    console.log('Stored structured summary:', execRes)
    // No trigger needed if this is the end of the flow/branch
  }
}
````

{% hint style="info" %}
Besides using `assert` statements, another popular way to validate schemas is [Pydantic](https://github.com/pydantic/pydantic)
{% endhint %}

### Why YAML instead of JSON?

Current LLMs struggle with escaping. YAML is easier with strings since they don't always need quotes.

**In JSON**

```json
{
  "dialogue": "Alice said: \"Hello Bob.\\nHow are you?\\nI am good.\""
}
```

- Every double quote inside the string must be escaped with `\"`.
- Each newline in the dialogue must be represented as `\n`.

**In YAML**

```yaml
dialogue: |
  Alice said: "Hello Bob.
  How are you?
  I am good."
```

- No need to escape interior quotes—just place the entire text under a block literal (`|`).
- Newlines are naturally preserved without needing `\n`.




================================================
File: docs/design_pattern/multi_agent.md
================================================
# Multi-Agent Systems Pattern

The multi-agent pattern enables complex behaviors by coordinating multiple specialized agents. Each agent focuses on a specific capability while communicating through shared memory and queues.

{% hint style="success" %}
Most of time, you don't need Multi-Agents. Start with a simple solution first.
{% endhint %}

## Example: Agent Communication through Message Queue

Here's a simple example showing how to implement agent communication using `asyncio.Queue`.
The agent listens for messages, processes them, and continues listening:

```typescript
import { Flow, Memory, Node } from 'brainyflow'

// Agent node that processes messages from a queue
class AgentNode extends Node {
  // We'll store the queue in global memory for simplicity here,
  // though in real apps, dependency injection might be better.
  async prep(memory: Memory): Promise<string> {
    const messageQueue = memory.messageQueue as AsyncQueue<string>
    if (!messageQueue) throw new Error('Message queue not found in memory')

    const message = await messageQueue.get() // Wait for a message
    console.log(`Agent received: ${message}`)
    return message // Pass message to exec (optional)
  }

  async exec(message: string): Promise<void> {
    // Process the message (optional)
    // console.log(`Agent processing: ${message}`);
  }

  async post(memory: Memory): Promise<void> {
    // Trigger self to listen for the next message
    this.trigger('continue')
  }
}

// --- Flow Setup ---
const agent = new AgentNode()
agent.on('continue', agent) // Loop back to self

const agentFlow = new Flow(agent)

// --- Message Sender ---
async function sendSystemMessages(messageQueue: AsyncQueue<string>) {
  let counter = 0
  const messages = [
    'System status: all systems operational',
    'Memory usage: normal',
    'Network connectivity: stable',
    'Processing load: optimal',
  ]

  while (true) {
    // In a real app, you'd have a termination condition
    const message = `${messages[counter % messages.length]} | timestamp_${counter}`
    await messageQueue.put(message)
    counter++
    await new Promise((resolve) => setTimeout(resolve, 1000)) // Wait 1s
  }
}

// --- Main Execution ---
async function main() {
  const messageQueue = new AsyncQueue<string>()
  // Pass the queue via the shared memory
  const data = { messageQueue }

  console.log('Starting agent listener and message sender...')
  // Run the agent flow and the message sender concurrently
  // Note: This will run indefinitely without a termination mechanism
  await Promise.all([agentFlow.run(data), sendSystemMessages(messageQueue)])
}

class AsyncQueue<T> {
  private queue: T[] = []
  private waiting: ((value: T) => void)[] = []

  async get(): Promise<T> {
    if (this.queue.length > 0) {
      return this.queue.shift()!
    }
    return new Promise((resolve) => {
      this.waiting.push(resolve)
    })
  }

  async put(item: T): Promise<void> {
    if (this.waiting.length > 0) {
      const resolve = this.waiting.shift()!
      resolve(item)
    } else {
      this.queue.push(item)
    }
  }
}

main().catch(console.error)
```

The output:

```
Agent received: System status: all systems operational | timestamp_0
Agent received: Memory usage: normal | timestamp_1
Agent received: Network connectivity: stable | timestamp_2
Agent received: Processing load: optimal | timestamp_3
```

## Example: Word Guessing Game (Taboo)

Two agents collaborate in a word guessing game:

- **Hinter**: Provides clues without using forbidden words
- **Guesser**: Attempts to guess the target word based on hints

```typescript
import { Memory, Node } from 'brainyflow'
import { callLLM } from '../utils/callLLM'

class Hinter extends Node {
  async prep(memory: Memory): Promise<any> {
    if (memory.guesserQueue) {
      try {
        const guess = await memory.guesserQueue.get()
        if (guess === 'GAME_OVER') {
          return null
        }
        return { guess }
      } catch (e) {
        // Queue empty
      }
    }
    return {}
  }

  async exec(prepRes: any): Promise<string | null> {
    if (prepRes === null) return null

    const prompt = `
Given target word: ${memory.targetWord}
Forbidden words: ${memory.forbiddenWords}
Last wrong guess: ${prepRes.guess || ''}
Generate a creative hint that helps guess the target word without using forbidden words.
Reply only with the hint text.
`
    return await callLLM(prompt)
  }

  async post(memory: Memory, data: any, hint: string | null): Promise<void> {
    if (hint === null) {
      this.trigger('end')
      return
    }
    await memory.hinterQueue.put(hint)
    this.trigger('continue')
  }
}

class Guesser extends Node {
  async prep(memory: Memory): Promise<any> {
    const hint = await memory.guesserQueue.get()
    return {
      hint,
      pastGuesses: memory.pastGuesses || [],
    }
  }

  async exec(prepRes: any): Promise<string> {
    const prompt = `
Given hint: ${prepRes.hint}
Past wrong guesses: ${prepRes.pastGuesses}
Make a new guess for the target word.
Reply only with the guessed word.
`
    return await callLLM(prompt)
  }

  async post(memory: Memory, prepRes: any, guess: string): Promise<void> {
    if (guess.toLowerCase() === memory.targetWord.toLowerCase()) {
      console.log('Game Over - Correct guess!')
      await memory.hinterQueue.put('GAME_OVER')
      this.trigger('end')
      return
    }

    // Update past guesses
    memory.pastGuesses = [...(memory.pastGuesses || []), guess]
    await memory.hinterQueue.put(guess)
    this.trigger('continue')
  }
}
```

## Running the Game

```typescript
// --- Main Execution ---
async function main() {
  // Set up game state in initial memory
  const data = {
    targetWord: 'nostalgia',
    forbiddenWords: ['memory', 'past', 'remember', 'feeling', 'longing'],
    hinterQueue: new AsyncQueue<string>(),
    guesserQueue: new AsyncQueue<string>(),
    pastGuesses: [],
  }

  console.log('Game starting!')
  console.log(`Target word: ${data.targetWord}`)
  console.log(`Forbidden words: ${data.forbiddenWords}`)

  // Initialize by sending empty guess to hinter queue
  await data.hinterQueue.put('')

  // Create nodes
  const hinterNode = new Hinter()
  const guesserNode = new Guesser()

  // Define transitions for looping and ending
  hinterNode.on('continue', hinterNode)
  guesserNode.on('continue', guesserNode)

  // Create separate flows for each agent
  const hinterFlow = new Flow(hinterNode)
  const guesserFlow = new Flow(guesserNode)

  // Run both agent flows concurrently using the same memory object
  console.log('Running agents...')
  await Promise.all([hinterFlow.run(data), guesserFlow.run(data)])
  console.log('\nGame finished normally.')
  console.log('Final memory state:', data)
}

// Assuming AsyncQueue and callLLM are defined elsewhere
// class AsyncQueue<T> { ... }
// async function callLLM(prompt: string): Promise<string> { return 'mock'; }

main().catch(console.error)
```

When you run this code, the multi-agent system works as follows:

1. The Hinter agent creates clues (avoiding forbidden words)
2. The Guesser agent makes guesses based on the hints
3. Both agents operate independently but communicate via queues
4. The game continues until the correct word is guessed

The output will look something like:

```
Game starting!
Target word: nostalgia
Forbidden words: ['memory', 'past', 'remember', 'feeling', 'longing']

Hinter: Here's your hint - Thinking about childhood summers
Guesser: I guess it's - happiness

Hinter: Here's your hint - The warm sensation when seeing old photos
Guesser: I guess it's - nostalgia

Game Over - Correct guess!
```

## Benefits of Multi-Agent Systems

This pattern demonstrates several key advantages:

1. **Specialization**: Each agent can focus on a specific task
2. **Independence**: Agents can operate on different schedules or priorities
3. **Coordination**: Agents can collaborate through shared memory and queues
4. **Flexibility**: Easy to add new agents or modify existing ones
5. **Scalability**: The system can grow to include many specialized agents




================================================
File: docs/utility_function/index.md
================================================
---
machine-display: true
---

# Utility Functions

BrainyFlow does not provide built-in utilities. Instead, we offer examples that you can implement yourself. This approach gives you more flexibility and control over your project's dependencies and functionality.

## Available Utility Function Examples

1. [LLM Wrapper](./llm.md): Interact with Language Models
2. [Web Search](./websearch.md): Perform web searches
3. [Chunking](./chunking.md): Split large texts into manageable chunks
4. [Embedding](./embedding.md): Generate vector embeddings for text
5. [Vector Databases](./vector.md): Store and query vector embeddings
6. [Text-to-Speech](./text_to_speech.md): Convert text to speech

## Why Not Built-in?

We believe it's a bad practice to include vendor-specific APIs in a general framework for several reasons:

1. **API Volatility**: Frequent changes in external APIs lead to heavy maintenance for hardcoded APIs.
2. **Flexibility**: You may want to switch vendors, use fine-tuned models, or run them locally.
3. **Optimizations**: Prompt caching, batching, and streaming are easier to implement without vendor lock-in.

## Implementing Utility Functions

When implementing utility functions for your BrainyFlow project:

1. Create a separate file for each utility function in the `utils/` directory.
2. Include a simple test or example usage in each file.
3. Document the input/output and purpose of each utility function.

Example structure:

```
my_project/
├── utils/
│   ├── callLlm.ts
│   ├── searchWeb.ts
│   └── embedText.ts
└── ...
```

By following this approach, you can easily maintain and update your utility functions as needed, without being constrained by the framework's built-in utilities.




================================================
File: docs/guides/best_practices.md
================================================
# BrainyFlow Best Practices

## Node Design

1.  **Keep Nodes Focused**: Each node should perform a single, well-defined task.
2.  **Idempotent Execution**: If using retries (`maxRetries > 1`), design the `exec()` method to be idempotent (produce the same result for the same input) as it might be called multiple times.
3.  **Clear Lifecycle**: Use `prep` to read/prepare data, `exec` for computation (no memory access), and `post` to write results and trigger successors.
4.  **Graceful Degradation**: Implement `execFallback` in `Node` subclasses to handle errors gracefully after all retries are exhausted, potentially returning a default value instead of throwing an error.

## Memory (State) Management

1.  **Schema Design**: Define clear interfaces (in TypeScript) or conventions (in Python) for your `GlobalStore` and `LocalStore` structures.
2.  **Global vs. Local**: Use the `GlobalStore` (accessed via `memory.prop = value`) for state shared across the entire flow. Use the `LocalStore` (populated via `forkingData` in `trigger`) for context specific to a particular execution branch.
3.  **Minimize Global State**: Prefer passing data locally via `forkingData` when possible to keep the global state clean and reduce potential conflicts, especially in parallel flows.
4.  **Read Transparently**: Always read via the `memory` proxy (e.g., `memory.value`); it handles the local-then-global lookup.

## Flow Design

1.  **Visualization First**: Sketch your flow diagram (e.g., using Mermaid) before coding to clarify logic and transitions.
2.  **Modularity**: Break complex processes into smaller, potentially nested, sub-flows (`Flow` extends `BaseNode`).
3.  **Explicit Transitions**: Clearly define transitions using descriptive action names (`node.on('action', nextNode)`). Consider default paths (`node.next(defaultNode)`).
4.  **Error Paths**: Define explicit transitions for error conditions (e.g., `node.on('error', errorHandlerNode)`) or handle errors within `execFallback`.
5.  **Cycle Management**: Use the `maxVisits` option in the `Flow` constructor to prevent infinite loops.
6.  **Parallelism**: Choose `ParallelFlow` for independent branches that can run concurrently; use `Flow` (sequential) otherwise or if order matters.
7.  **Test Incrementally**: Test individual nodes (`node.run()`) and sub-flows before integrating them.

## Project Structure

A well-organized project structure enhances maintainability and collaboration:

{% tabs %}
{% tab title="TypeScript (simple)" %}

```
my_project/
├── src/
│   ├── main.ts
│   ├── nodes.ts
│   ├── flow.ts
│   └── utils/
│       ├── callLLM.ts
│       └── searchWeb.ts
├── package.json
└── docs/
    └── design.md
```

{% endtab %}
{% tab title="TypeScript (complex)" %}

```
my_complex_project/
├── src/                      # Source code
│   ├── index.ts              # Entry point
│   ├── nodes/                # Node implementations
│   │   ├── index.ts          # Exports all nodes
│   │   ├── inputNodes.ts
│   │   ├── processingNodes.ts
│   │   └── outputNodes.ts
│   ├── flows/                # Flow definitions
│   │   ├── index.ts          # Exports all flows
│   │   └── mainFlow.ts
│   ├── utils/                # Utility functions
│   │   ├── index.ts          # Exports all utilities
│   │   ├── llm.ts
│   │   ├── database.ts
│   │   └── webSearch.ts
│   ├── types/                # Type definitions
│   │   ├── index.ts          # Exports all types
│   │   ├── node.types.ts
│   │   └── flow.types.ts
│   └── config/               # Configuration
│       └── settings.ts
├── dist/                     # Compiled JavaScript
├── tests/                    # Test cases
│   ├── nodes.test.ts
│   └── flows.test.ts
├── package.json              # Dependencies and scripts
└── docs/                     # Documentation
    ├── design.md             # High-level design
    └── api.md                # API documentation
```

{% endtab %}
{% endtabs %}

- **`docs/design.md`**: Contains project documentation for each step designed in [agentic coding](./agentic_coding.md). This should be _high-level_ and _no-code_.
- **`utils/`**: Contains all utility functions.
  - It's recommended to dedicate one file to each API call, for example `call_llm.py` or `search_web.ts`.
  - Each file should also include a `main()` function to try that API call
- **`nodes.py`** or **`nodes.ts`**: Contains all the node definitions.
- **`flow.py`** or **`flow.ts`**: Implements functions that create flows by importing node definitions and connecting them.
- **`main.py`** or **`main.ts`**: Serves as the project's entry point.




================================================
File: docs/guides/agentic_coding.md
================================================
# Agentic Coding with BrainyFlow

<details>

<summary>Notes For AI Assistants</summary>

{% hint style="warning" %}
**AI only**: If you are a human, skip this section.
{% endhint %}

This is your implementation playbook and most important guideline. Strictly follow the instructions listed in this document.

Use the design document as your single source of truth. Never proceed to code without explicit human approval of the design.

1.  start with a small and simple solution
2.  design at a high level (`docs/design.md`) before implementation and do not start coding until the design is approved
3.  once approved, start coding and iterate on the design based on feedback
4.  do not stop coding until the implementation is working as intended and fully complaint with the design document

</details>

In the context of Human-AI Co-Design, agentic coding involves humans providing high-level guidance while AI agents handle implementation details:
It represents a powerful approach to software development where humans are freed up to focus solely in strategic decisions.
This guide will help you create effective design documents that enable successful BrainyFlow implementations.

## The AI Implementation Brief

```mermaid
flowchart TD
    A[Human Request] --> B{AI Asks Questions}
    B --> C[AI Generates Structured Design Draft]
    C --> D{Human Validates/Edits}
    D -->|Approved| E[AI Implements]
    D -->|Needs Changes| B
    E --> F[Continuous Co-Refinement]
```

- **AI-Driven Structuring:** Convert vague requests into technical specifications through dialogue
- **Essentialism:** Only capture requirements that directly impact implementation
- **Living Documentation:** Design evolves organically through implementation insights

Before writing any code, create a comprehensive AI Implementation Brief at `docs/design.md`. This document serves as the foundation for human-AI collaboration and should contain all the essential sections listed below.

### 1. Requirements Definition

Clearly articulate what you're building and why:

- **Problem Statement**: Define the problem being solved in 1-2 sentences
- **User Needs**: Describe who will use this and what they need
- **Success Criteria**: List measurable outcomes that define success
- **Constraints**: Note any technical or business limitations

Example:

```
We need a document processing system that extracts key information from legal contracts,
summarizes them, and stores the results for easy retrieval. This will help our legal
team review contracts 70% faster.
```

### 2. Flow Design

Outline the high-level architecture using BrainyFlow's nested directed graph abstraction:

- **Flow Diagram**: Create a mermaid diagram showing node connections
- **Processing Stages**: Describe each major stage in the flow
- **Decision Points**: Identify where branching logic occurs
- **Data Flow**: Explain how information moves through the system

Example:

```mermaid
graph TD
    A[DocumentLoader] --> B[TextExtractor]
    B --> C[EntityExtractor]
    C --> D[ValidationNode]
    D -->|Valid| E[SummaryGenerator]
    D -->|Invalid| C
    E --> F[DatabaseStorage]
```

### 3. Utility Functions

List all external utilities needed:

- **Function Name**: Clear, descriptive name
- **Purpose**: What the function does
- **Inputs/Outputs**: Expected parameters and return values
- **External Dependencies**: Any APIs or libraries required

Example:

```
extract_entities(text: str) -> dict:
- Purpose: Uses NER to identify entities in text
- Input: Document text string
- Output: Dictionary of entity lists by type
- Dependencies: spaCy NLP library with legal model
```

### 4. Node Design

For each node in your flow, define:

- **Purpose**: One-line description of what the node does
- **Shared Store Access**: What data it reads from and writes to the shared store
- **Lifecycle Implementation**: How `prep`, `exec`, and `post` will be implemented
- **Action Returns**: What actions the node might return to direct flow
- **Error Handling**: How failures will be managed

Example:

```
EntityExtractorNode:
- Purpose: Identifies parties, dates, and monetary values in contract text
- Reads: document_text from shared store
- Writes: entities dictionary to shared store
- Actions: Returns "valid" if entities found, "retry" if processing failed
- Error Handling: Will retry up to 3 times with exponential backoff
```

### 5. Shared Store Schema

Define the structure of your shared store. Using interfaces (TypeScript) or type hints (Python) is highly recommended.

- **Key Namespaces**: Major sections of your shared store (often represented as nested objects or distinct keys).
- **Data Types**: Expected types for each key.
- **Data Flow**: How data evolves through processing (which nodes read/write which keys).

Example:

```typescript
// Define interfaces for the shared store structure
interface InputStore {
  document_path: string
}

interface ProcessingStore {
  document_text: string
  entities: {
    parties: any[]
    dates: any[]
    amounts: any[]
  }
  validation_status: string
}

interface OutputStore {
  summary: string
  storage_id: string
}

// Combine interfaces for the complete global store (if using nested structure conceptually)
interface GlobalStore extends InputStore, ProcessingStore, OutputStore {}

// Or define a flat global store interface (more common in BrainyFlow usage)
interface FlatGlobalStore {
  document_path?: string
  document_text?: string
  entities?: {
    parties: any[]
    dates: any[]
    amounts: any[]
  }
  validation_status?: string
  summary?: string
  storage_id?: string
}

// Conceptual structure (using the flat interface)
const memoryConceptual: FlatGlobalStore = {
  document_path: 'path/to/file.pdf',
  document_text: '',
  entities: {
    parties: [],
    dates: [],
    amounts: [],
  },
  validation_status: '',
  summary: '',
  storage_id: '',
}

// Note: In BrainyFlow, you'd typically pass an object conforming to
// FlatGlobalStore (or a relevant subset) to flow.run() and access
// properties directly, e.g., memory.document_text = "...", const entities = memory.entities;
```

## Best Practices for Your Design Document

1. **Start Simple**: Begin with the minimal viable solution
2. **Be Explicit**: Clearly define all components and their interactions
3. **Visualize Flows**: Use diagrams to illustrate complex relationships
4. **Define Boundaries**: Clarify what's in and out of scope
5. **Consider Edge Cases**: Note how the system handles failures
6. **Be Concise**: Keep it short and to the point; Do not add any other section to the design document other than the ones listed above

## Implementation Process

After completing your design document:

1. **Review with Stakeholders**: Ensure the design meets requirements. If you are the AI agent, ask the user to verify the design document and confirm it fits their needs.
2. **Refine as Needed**: Iterate on the design based on feedback
3. **Hand Off to AI**: Provide the design document to your AI assistant - If you are the AI agent, start working on the solution based on the design document
4. **Collaborative Implementation**: Work with AI to implement the design
5. **Test and Validate**: Verify the implementation against success criteria

Once the AI assistant starts working on the implementation it's common for it to get lazy midway through the implementation.
Whenever it claims to be done, you can give it an extra push with the following prompt:

```markdown
Review the design document and the whole implemented code, then answer the following questions:

- are all stubs and placeholders gone?
- have all requirements and features been implemented?
- have you successfully run the application and all tests?
- what should be implemented next?

If the answer to any question is "no", or you can name something to be implemented next, then you are not done yet.
Go back to working and finish it! Do not stop until you are done.
```

<details>

<summary>Notes For AI Assistants</summary>

{% hint style="warning" %}
**AI only**: If you are a human, skip this section.
{% endhint %}

Be sure you **always** strictly follow these instructions:

1. **Be Patient**: Do not start the implementation until the design document is approved.
2. **Be Brainy**: Import `brainyflow` and use its archictecture to implement the design document! Adhere to the proposed flow, node and shared store design.
3. **Focus on What Matters**: Do not fix types or edge cases until you are certain the core implementation is working as intended.
4. **Be Adaptive**: Continuously refine the design document based on feedback and changes in the implementation, but keep it clear and concise.
5. **Be Compliant**: You must always write code that is compliant with the design document and the system's architecture. Remember to stick to the flow, node and shared store design:
   - prep(memory)
     Read and preprocess data from the memory object.
   - exec(prep_res)
     Execute compute logic. Receives result from prep. Cannot access memory.
   - post(memory, prep_res, exec_res)
     Postprocess results, write data back to the memory object (usually global store), and trigger next actions.
6. **Be Resilient**: You are not done until the implementation is working as intended.
7. **Be Accountable**: Before finishing the implementation, ask yourself the following questions:
   - are all stubs and placeholders gone?
   - have all requirements and features been implemented?
   - have I successfully run the application and all tests?
   - what should be implemented next?
     If the answer to any question is "no", or you can name something to be implemented next, then you are not done yet.
     Go back to working and finish it!

</details>

## Conclusion: Precision Through Structure

This approach ensures all BrainyFlow solutions maintain:

- **Human Focus:** Strategic requirements and validation

- **AI Precision:** Structured implementation targets

- **System Integrity:** Clear component boundaries

By enforcing these four pillars through adaptive dialogue rather than rigid templates, we achieve flexible yet reliable AI system development. The design document becomes a living contract between human intent and AI execution.

You provide your AI assistant with the clear direction needed to implement an effective BrainyFlow solution while maintaining human oversight of the critical design decisions.

Remember: The quality of your design document directly impacts the quality of the implementation. Invest time in creating a comprehensive brief to ensure successful outcomes.




================================================
File: docs/guides/throttling.md
================================================
# Rate Limiting and Throttling

Effective rate limiting is crucial when working with external APIs and services. This guide covers patterns for implementing throttling in BrainyFlow applications.

This is particularly important when:

1. Calling external APIs with rate limits
2. Managing expensive operations (like LLM calls)
3. Preventing system overload from too many parallel requests

## Concurrency Control Patterns

These patterns limit the number of concurrent operations within a node.

```typescript
// Requires: npm install p-limit
import { Memory, Node } from 'brainyflow' // Assuming imports
import pLimit from 'p-limit'

class LimitedParallelNodeTs extends Node {
  private limit: ReturnType<typeof pLimit>

  constructor(concurrency: number = 3) {
    super()
    if (concurrency <= 0) {
      throw new Error('Concurrency limit must be positive')
    }
    this.limit = pLimit(concurrency)
    console.log(`Node initialized with concurrency limit: ${concurrency}`)
  }

  // Prep is usually needed to get 'items' from memory
  async prep(memory: Memory): Promise<any[]> {
    // Example: Fetch items from memory
    const items = memory.items_to_process || []
    console.log(`Prep: Found ${items.length} items to process.`)
    return items // Assuming items are in memory.items_to_process
  }

  async exec(items: any[]): Promise<any[]> {
    if (!items || items.length === 0) {
      console.log('Exec: No items to process.')
      return []
    }

    console.log(`Exec: Starting processing of ${items.length} items with limit...`)
    // Map each item to a limited async task
    const tasks = items.map((item) =>
      this.limit(async () => {
        console.log(` Starting processing item: ${item}`)
        const result = await this.processOneItem(item)
        console.log(` Finished processing item: ${item} -> ${result}`)
        return result
      }),
    )

    // Wait for all limited tasks to complete
    const results = await Promise.all(tasks)
    console.log('Exec: All items processed.')
    return results
  }

  async processOneItem(item: any): Promise<any> {
    /** Placeholder: Subclasses must implement this method. */
    // Example implementation:
    await new Promise((resolve) => setTimeout(resolve, 500)) // Simulate async work
    return `Processed_${item}`
    // throw new Error("processOneItem must be implemented by subclasses");
  }

  // Post is needed to store results and trigger next step
  async post(memory: Memory, prepRes: any[], execRes: any[]): Promise<void> {
    console.log(`Post: Storing ${execRes.length} results.`)
    memory.processed_results = execRes // Store results
    this.trigger('default') // Trigger next node
  }
}
```

## Rate Limiting with Window Limits

```typescript
import { RateLimiter } from 'limiter'

// 30 calls per minute
const limiter = new RateLimiter({ tokensPerInterval: 30, interval: 'minute' })

async function callApi() {
  await limiter.removeTokens(1)
  // Your API call here
}
```

## Throttler Utility

```typescript
import pRetry from 'p-retry'

async function callApiWithRetry() {
  return pRetry(
    async () => {
      // Your API call here
    },
    {
      retries: 5,
      minTimeout: 4000,
      maxTimeout: 10000,
    },
  )
}
```

## Advanced Throttling Patterns

### 1. Token Bucket Rate Limiter

```typescript
import { TokenBucket } from 'limiter'

// 10 requests per minute
const limiter = new TokenBucket({
  bucketSize: 10,
  tokensPerInterval: 10,
  interval: 'minute',
})

async function callApi() {
  await limiter.removeTokens(1)
  // Your API call here
}
```

### 2. Sliding Window Rate Limiter

```typescript
class SlidingWindowRateLimiter {
  private requests: number[] = []

  constructor(
    private maxRequests: number,
    private windowSize: number,
  ) {}

  allowRequest(): boolean {
    const now = Date.now()
    // Remove expired requests
    this.requests = this.requests.filter((time) => now - time < this.windowSize * 1000)
    // Check if we can allow another request
    return this.requests.length < this.maxRequests
  }

  timeToNextRequest(): number {
    const now = Date.now()
    this.requests = this.requests.filter((time) => now - time < this.windowSize * 1000)
    if (this.requests.length < this.maxRequests) return 0
    const oldest = this.requests[0]
    return Math.ceil((oldest + this.windowSize * 1000 - now) / 1000)
  }
}

// Usage:
const limiter = new SlidingWindowRateLimiter(100, 60) // 100 requests per 60 seconds

async function callApi() {
  if (!limiter.allowRequest()) {
    await new Promise((resolve) => setTimeout(resolve, limiter.timeToNextRequest() * 1000))
  }
  // Your API call here
  return 'API response'
}
```

## Best Practices

1. **Monitor API Responses**: Watch for 429 (Too Many Requests) responses and adjust your rate limiting accordingly
2. **Implement Retry Logic**: When hitting rate limits, implement exponential backoff for retries
3. **Distribute Load**: If possible, spread requests across multiple API keys or endpoints
4. **Cache Responses**: Cache frequent identical requests to reduce API calls
5. **Batch Requests**: Combine multiple requests into single API calls when possible

## Integration with BrainyFlow

### Throttled LLM Node

```typescript
import { Memory, Node, NodeError } from 'brainyflow'
import { RateLimiter } from 'limiter'

class ThrottledLLMNode extends Node {
  private limiter: RateLimiter

  constructor(
    private maxRetries = 3,
    callsPerMinute = 30,
  ) {
    super({ maxRetries })
    this.limiter = new RateLimiter({ tokensPerInterval: callsPerMinute, interval: 'minute' })
  }

  async exec(prompt: string): Promise<string> {
    // Wait for token before proceeding
    await this.limiter.removeTokens(1)
    return await callLLM(prompt)
  }

  async execFallback(prompt: string, error: NodeError): Promise<string> {
    // Handle rate limit errors specially
    if (error.message.toLowerCase().includes('rate limit')) {
      // Wait longer before retrying
      await new Promise((resolve) => setTimeout(resolve, 60000))
      return this.exec(prompt)
    }
    // For other errors, fall back to a simple response
    return "I'm having trouble processing your request right now."
  }
}
```

## Linking to Related Concepts

For batch processing patterns, see [Flow](../core_abstraction/flow.md).




================================================
File: docs/guides/testing.md
================================================
# Testing and Debugging BrainyFlow Applications

Effective testing and debugging are essential for building reliable applications. This guide covers strategies for testing and debugging complex flows, and monitoring applications in production.

## Testing Approaches

BrainyFlow supports multiple testing approaches to ensure your applications work correctly:

### Unit Testing (Nodes)

Individual nodes can be tested in isolation to verify their behavior:

```typescript
import { describe, expect, it, vi } from 'vitest'
import { SummarizeNode } from './SummarizeNode' // Your Node implementation
import { callLLM } from './utils/callLLM' // Your LLM utility

// Mock the LLM utility
vi.mock('./utils/callLLM', () => ({
  callLLM: vi.fn().mockResolvedValue('Short summary.'),
}))

describe('SummarizeNode', () => {
  it('should summarize text correctly', async () => {
    // Create the node instance
    const summarizeNode = new SummarizeNode()

    // Create initial global memory state
    const memory = { text: 'This is a long text that needs to be summarized.' }

    // Run the node's lifecycle (prep -> exec -> post)
    await summarizeNode.run(memory) // Pass memory object

    // Verify the LLM call
    expect(callLLM).toHaveBeenCalledTimes(1)
    const callArgs = vi.mocked(callLLM).mock.calls[0][0] // Get the first argument of the first call
    expect(callArgs.toLowerCase()).toContain('summarize') // Check if prompt contains 'summarize'

    // Verify the result was stored correctly in the global memory object
    expect(memory.summary).toBe('Short summary.') // Access memory object
  })
})
```

### Integration Testing (Flows)

Test complete flows to verify that nodes work together correctly:

````typescript
import { afterEach, beforeEach, describe, expect, it, vi } from 'vitest'
import { createQaFlow } from './qaFlow' // Your function that creates the Flow
import { callLLM } from './utils/callLLM' // Your LLM utility

// Mock the LLM utility
vi.mock('./utils/callLLM', () => ({
  callLLM: vi.fn(),
}))

describe('Question Answering Flow', () => {
  beforeEach(() => {
    // Clear any previous mock calls before each test
    vi.clearAllMocks()
  })

  it('should generate an answer using the flow', async () => {
    // Configure mock to return different values based on the prompt
    vi.mocked(callLLM).mockImplementation((prompt: string) => {
      // Simulate different stages of a potential QA flow (e.g., search vs. answer)
      if (prompt.toLowerCase().includes('search')) {
        return Promise.resolve('Paris is the capital of France.')
      } else if (prompt.toLowerCase().includes('answer')) {
        return Promise.resolve('The capital of France is Paris.')
      }
      return Promise.resolve('Unexpected prompt')
    })

    // Create the flow
    const qaFlow = createQaFlow()

    // Create initial memory state
    const memory = { question: 'What is the capital of France?' }

    // Run the flow
    await qaFlow.run(memory) // Pass memory object

    // Verify the final answer
    expect(memory.answer).toBe('The capital of France is Paris.') // Access memory object

    // Verify the LLM was called the expected number of times
    expect(callLLM).toHaveBeenCalledTimes(2)

    // Verify the calls were made with appropriate prompts
    const calls = vi.mocked(callLLM).mock.calls
    const retrieveCall = calls.some(
      (call) => typeof call === 'string' && call.toLowerCase().includes('retrieve'),
    )
    const generateCall = calls.some(
      (call) => typeof call === 'string' && call.toLowerCase().includes('generate'),
    )

    expect(retrieveCall).toBe(true)
    expect(generateCall).toBe(true)
  })
})

// Example testing a MapReduce flow (Trigger, Processor, Reducer)
describe('MapReduce Flow Test', () => {
  // Mock the nodes used in the MapReduce example
class TriggerNode extends Node<Memory, any, ['process_item','reduce']> {
  async post(memory: Memory, prepRes: any, execRes: any): Promise<void> {
      const items = memory.items || []
      memory.results = [] // Initialize results
      items.forEach((item: any, index: number) => {
        this.trigger('process_item', { item, index })
      })
      this.trigger('reduce')
    }
  }
  const ProcessorNode = class extends Node {
     async prep(memory: Memory): Promise<any> { return { item: memory.item, index: memory.index }; }
     async exec(prepRes: { item: any, index: number }): Promise<string> { return `Processed ${prepRes.item}`; }
     async post(memory: Memory, prepRes: { item: any, index: number }, execRes: string): Promise<void> {
         if (!memory.results) memory.results = [];
         // Store result at the correct index if possible, or just push
         memory.results[prepRes.index] = execRes;
     }
  }
  const ReducerNode = class extends Node {
     async prep(memory: Memory): Promise<any[]> { return memory.results || []; }
     async exec(results: any[]): Promise<string> { return `Combined: ${results.join(', ')}`; }
     async post(memory: Memory, prepRes: any, execRes: string): Promise<void> { memory.final_result = execRes; }
  }

  it('should process items via map and reduce steps', async () => {
    // Instantiate nodes
    const trigger = new TriggerNode()
    const processor = new ProcessorNode()
    const reducer = new ReducerNode()

    // Connect nodes
    trigger.on('process_item', processor)
    trigger.on('reduce', reducer) // This action is triggered after all 'process_item'

    // Use ParallelFlow for the map phase
    const mapReduceFlow = new ParallelFlow(trigger)

    // Initial memory
    const memory = { items: ['A', 'B', 'C'] }

    // Run the flow
    await mapReduceFlow.run(memory)

    // Verify final result in memory
    expect(memory.results).toEqual(['Processed A', 'Processed B', 'Processed C'])
    expect(memory.final_result).toBe('Combined: Processed A, Processed B, Processed C')
  })
})

## Testing Approaches

### Unit Testing Individual Nodes

1. **Isolate Dependencies**: Mock external services and LLM calls
2. **Test Each Lifecycle Method**: Verify `prep`, `exec`, and `post` individually
3. **Test Error Handling**: Ensure `exec_fallback` works as expected
4. **Verify Memory Updates**: Check if memory is modified correctly
5. **Test Triggers**: Ensure the right actions are triggered

### Integration Testing Flows

1. **Mock External Services**: Keep tests deterministic by mocking APIs
2. **Verify End-to-End Behavior**: Test the entire flow from start to finish
3. **Test Branching Logic**: Ensure different paths work correctly
4. **Check Final Memory State**: Verify that the memory contains expected results
5. **Test Error Handling**: Make sure flows handle errors gracefully

### Testing Strategies

#### Testing LLM-Based Nodes

For nodes that call LLMs, you can use these approaches:

1.  **Canned Responses**: Prepare fixed responses for specific prompts.
2.  **Prompt Verification**: Check if prompts contain expected information.
3.  **Response Validation**: Test if the node correctly handles various LLM responses.

```typescript
import { describe, expect, it, vi } from 'vitest'
import { callLLM } from './utils/callLLM' // Your LLM utility

// import { MyLlmNode } from './MyLlmNode'; // Your Node implementation
// import { Memory } from 'brainyflow'; // Assuming Memory is imported if needed

// Mock the LLM utility module
vi.mock('./utils/callLLM', () => ({
  callLLM: vi.fn(), // Create a mock function
}))

describe('Testing LLM Nodes', () => {
  it('should use canned responses based on prompt', async () => {
    // Configure the mock implementation
    vi.mocked(callLLM).mockImplementation(async (prompt: string): Promise<string> => {
      if (prompt.toLowerCase().includes('summarize')) {
        return 'This is a summary.'
      } else if (prompt.toLowerCase().includes('extract')) {
        return JSON.stringify({ key: 'value' }) // Return JSON string
      } else {
        return 'Default response'
      }
    })

    // const node = new MyLlmNode();
    // const memory = { input: 'some text to summarize' }; // Initial memory state

    // await node.run(memory); // Run the node

    // Add assertions here
    // expect(callLLM).toHaveBeenCalled();
    // expect(memory.summary).toBe('This is a summary.');
  })
})
```

#### Testing Retry Logic

To test retry behavior:

1.  **Simulate Transient Failures**: Make the mock function fail a few times before succeeding.
2.  **Check Retry Count**: Verify that retries happened the expected number of times (e.g., by checking `node.cur_retry` inside the mock or tracking calls).
3.  **Test Backoff**: If using `wait`, mock `asyncio.sleep` (Python) or `setTimeout` (TypeScript) to verify delays without actually waiting.

```typescript
import { beforeEach, describe, expect, it, vi } from 'vitest'

// import { Node, Memory } from 'brainyflow'; // Assuming imports
// import { someExternalCall } from './utils/externalCall'; // The function called by exec

// Mock the external call module
vi.mock('./utils/externalCall', () => ({
  someExternalCall: vi.fn(),
}))

// Mock setTimeout used for 'wait' (if applicable)
vi.useFakeTimers()

// Example Node (conceptual)
// class NodeWithRetry extends Node<any, any, [], any, string> {
//   constructor() {
//     super({ maxRetries: 3, wait: 100 }); // Retry up to 3 times, wait 100ms
//   }
//   async exec(prepRes: any): Promise<string> {
//     // This method calls the function we will mock
//     return await someExternalCall(prepRes);
//   }
// }

describe('Retry Logic Testing', () => {
  let callCountRetry = 0

  beforeEach(() => {
    callCountRetry = 0 // Reset counter
    vi.clearAllMocks() // Clear mock history
    vi.clearAllTimers() // Clear pending timers
  })

  it('should retry exec on failure and succeed eventually', async () => {
    // Configure the mock to fail twice, then succeed
    vi.mocked(someExternalCall).mockImplementation(async () => {
      callCountRetry++
      console.log(`Mock called (Attempt ${callCountRetry})`) // For debugging test
      if (callCountRetry <= 2) {
        throw new Error('Temporary network failure')
      }
      return 'Success on third try'
    })

    // const node = new NodeWithRetry();
    // const memory = {}; // Initial memory

    // await node.run(memory); // Run the node

    // Advance timers to simulate waiting (if wait > 0)
    // vi.advanceTimersByTime(100); // Advance by wait time
    // await Promise.resolve(); // Allow promises to settle after timer advance
    // vi.advanceTimersByTime(100); // Advance for second wait
    // await Promise.resolve();

    // Assertions
    // expect(callCountRetry).toBe(3); // Called 3 times
    // expect(memory.result).toBe('Success on third try'); // Check final result
    // expect(vi.getTimerCount()).toBe(0); // Ensure all timers were cleared/run
  })
})
```

## Test Fixtures and Helpers

Creating helper functions can make tests more readable and maintainable.

```typescript
import { expect } from 'vitest'

// import { Node, Memory } from 'brainyflow'; // Assuming imports

// Define a type for your standard test memory if desired
interface TestMemory {
  input?: string
  config?: { setting: string }
  output?: any
  status?: string
  [key: string]: any // Allow other properties
}

export function createDefaultTestMemory(): TestMemory {
  /** Creates a standard object for test memory. */
  return { input: 'test data', config: { setting: 'value' } }
}

export async function runNodeWithMemory(
  node: Node,
  initialMemory?: TestMemory,
): Promise<TestMemory> {
  /** Runs a node with provided or default initial memory. */
  const memory = initialMemory ?? createDefaultTestMemory()
  // Assumes node.run modifies the object in place
  await node.run(memory)
  return memory
}

export function assertMemoryContains(memory: TestMemory, expectedData: Partial<TestMemory>): void {
  /** Asserts that the memory object contains the expected key-value pairs. */
  for (const key in expectedData) {
    expect(memory).toHaveProperty(key)
    expect(memory[key]).toEqual(expectedData[key])
  }
}

// Example usage in a test
/*
import { MyProcessingNode } from './MyProcessingNode';
import { runNodeWithMemory, assertMemoryContains } from './testHelpers';

it('should produce correct output in memory', async () => {
    const node = new MyProcessingNode();
    const finalMemory = await runNodeWithMemory(node);
    assertMemoryContains(finalMemory, { output: "processed data", status: "completed" });
});
*/
```

## Common Testing Patterns

### 1. Input Validation Testing

Test that nodes properly handle invalid or unexpected inputs.

```typescript
import { describe, expect, it } from 'vitest'

// import { MyNodeThatValidates } from './MyNodeThatValidates'; // Your node
// import { Memory } from 'brainyflow'; // Assuming imports

describe('Input Validation', () => {
  const invalidInputs = [null, undefined, '', {}, [], { wrongKey: 1 }]

  it.each(invalidInputs)('should handle invalid input: %s', async (invalidInput) => {
    /** Tests if the node handles various invalid inputs gracefully. */
    // const node = new MyNodeThatValidates(); // Node that should validate memory.input_data
    const memory: Record<string, any> = { input_data: invalidInput } // Pass invalid data

    // Expect the node to run without unhandled exceptions
    // Use try/catch if specific errors are expected, otherwise just run
    await node.run(memory)

    // Example assertions: Check for an error flag or a specific state
    expect(memory.error_message || memory.status).toBeDefined() // Check if either is set
    expect(memory.status === 'validation_failed' || memory.error_message).toBeTruthy()
    // Or assert that a default value was set
    // expect(memory.output).toBe('default_value');
  })
})
```

### 2. Flow Path Testing

Test that flows follow the expected paths based on node triggers.

```typescript
import { describe, expect, it } from 'vitest'

// import { Node, Flow, Memory, BaseNode } from 'brainyflow'; // Assuming imports

describe('Flow Path Testing', () => {
  it('should follow the correct path based on triggers', async () => {
    /** Tests if the flow executes nodes in the expected sequence. */
    const visitedNodesLog: string[] = []

    // Define simple tracking nodes
    class SimpleTrackingNode extends Node<any, any, ['next_step', 'finish']> {
      private nodeName: string
      private triggerAction: 'next_step' | 'finish' | 'default'

      constructor(name: string, triggerAction: 'next_step' | 'finish' | 'default' = 'default') {
        super()
        this.nodeName = name
        this.triggerAction = triggerAction
      }

      async exec(prepRes: any): Promise<string> {
        // No real work, just track visit
        visitedNodesLog.push(this.nodeName)
        return `Processed by ${this.nodeName}` // Return something for post
      }

      async post(memory: Memory, prepRes: any, execRes: string): Promise<void> {
        // Trigger the specified action
        this.trigger(this.triggerAction)
      }
    }

    // Create nodes for a path: A -> B -> C (where B triggers 'finish')
    const nodeA = new SimpleTrackingNode('A', 'next_step')
    const nodeB = new SimpleTrackingNode('B', 'finish')
    const nodeC = new SimpleTrackingNode('C') // This node shouldn't be reached

    // Connect nodes based on actions
    nodeA.on('next_step', nodeB)
    nodeB.on('finish', nodeC) // Connect C, but B will trigger 'finish'

    // Create and run the flow
    const flow = new Flow(nodeA)
    await flow.run({}) // Pass empty memory

    // Verify the execution path
    expect(visitedNodesLog).toEqual(['A', 'B'])
  })
})
```

## Best Practices

### Testing Best Practices

1. **Test Each Node Individually**: Verify that each node performs its specific task correctly
2. **Test Flows as Integration Tests**: Ensure nodes work together as expected
3. **Mock External Dependencies**: Use mocks for LLMs, APIs, and databases to ensure consistent testing
4. **Test Error Handling**: Explicitly test how your application handles failures
5. **Automate Tests**: Include BrainyFlow tests in your CI/CD pipeline

### Debugging Best Practices

1. **Start Simple**: Begin with a minimal flow and add complexity incrementally
2. **Visualize Your Flow**: Generate flow diagrams to understand the structure
3. **Isolate Issues**: Test individual nodes to narrow down problems
4. **Check Shared Store**: Verify that data is correctly passed between nodes
5. **Monitor Actions**: Ensure nodes are returning the expected actions

### Monitoring Best Practices

1. **Monitor Node Performance**: Track execution time for each node
2. **Watch for Bottlenecks**: Identify nodes that take longer than expected
3. **Track Error Rates**: Monitor how often nodes and flows fail
4. **Set Up Alerts**: Configure alerts for critical failures
5. **Log Judiciously**: Log important events without overwhelming storage
6. **Implement Distributed Tracing**: Use tracing for complex, distributed applications

By applying these testing techniques, you can ensure your BrainyFlow applications are reliable and maintainable.


